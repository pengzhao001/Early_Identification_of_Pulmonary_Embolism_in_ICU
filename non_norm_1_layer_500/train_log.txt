Training iter #50:   Batch Loss = 1.589014, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 1.1631147861480713, Accuracy = 0.6304348111152649
Training iter #100:   Batch Loss = 1.872036, Accuracy = 0.3799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 1.1548253297805786, Accuracy = 0.6304348111152649
Training iter #200:   Batch Loss = 1.455123, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 1.1399435997009277, Accuracy = 0.6304348111152649
Training iter #300:   Batch Loss = 1.710247, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 1.1254011392593384, Accuracy = 0.6304348111152649
Training iter #400:   Batch Loss = 1.637683, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 1.109592318534851, Accuracy = 0.6304348111152649
Training iter #500:   Batch Loss = 0.808058, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 1.090778112411499, Accuracy = 0.6304348111152649
Training iter #600:   Batch Loss = 1.569841, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 1.066463828086853, Accuracy = 0.6304348111152649
Training iter #700:   Batch Loss = 1.386394, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 1.0261099338531494, Accuracy = 0.6304348111152649
Training iter #800:   Batch Loss = 1.320314, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.94624263048172, Accuracy = 0.6304348111152649
Training iter #900:   Batch Loss = 1.324142, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 0.7994799017906189, Accuracy = 0.6304348111152649
Training iter #1000:   Batch Loss = 0.822049, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7136892080307007, Accuracy = 0.6086956262588501
Training iter #1100:   Batch Loss = 0.741601, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.8076230883598328, Accuracy = 0.42391303181648254
Training iter #1200:   Batch Loss = 0.767784, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.9049142599105835, Accuracy = 0.3804347813129425
Training iter #1300:   Batch Loss = 1.036869, Accuracy = 0.23999999463558197
PERFORMANCE ON TEST SET: Batch Loss = 0.9362826347351074, Accuracy = 0.3804347813129425
Training iter #1400:   Batch Loss = 0.805056, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.95073401927948, Accuracy = 0.3695652186870575
Training iter #1500:   Batch Loss = 0.848654, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.9564804434776306, Accuracy = 0.3695652186870575
Training iter #1600:   Batch Loss = 0.821473, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.9532709121704102, Accuracy = 0.3695652186870575
Training iter #1700:   Batch Loss = 0.668638, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.9364330172538757, Accuracy = 0.3695652186870575
Training iter #1800:   Batch Loss = 0.808844, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.8782492876052856, Accuracy = 0.3804347813129425
Training iter #1900:   Batch Loss = 0.719964, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.8175902366638184, Accuracy = 0.3695652186870575
Training iter #2000:   Batch Loss = 0.750394, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7945627570152283, Accuracy = 0.3913043439388275
Training iter #2100:   Batch Loss = 0.835321, Accuracy = 0.3400000035762787
PERFORMANCE ON TEST SET: Batch Loss = 0.7841802835464478, Accuracy = 0.3804347813129425
Training iter #2200:   Batch Loss = 0.738215, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7792777419090271, Accuracy = 0.41304346919059753
Training iter #2300:   Batch Loss = 0.742974, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7762569189071655, Accuracy = 0.41304346919059753
Training iter #2400:   Batch Loss = 0.742672, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7733729481697083, Accuracy = 0.41304346919059753
Training iter #2500:   Batch Loss = 0.713308, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7711881995201111, Accuracy = 0.41304346919059753
Training iter #2600:   Batch Loss = 0.730961, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7691131830215454, Accuracy = 0.4021739065647125
Training iter #2700:   Batch Loss = 0.721417, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7667951583862305, Accuracy = 0.42391303181648254
Training iter #2800:   Batch Loss = 0.732889, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7652918100357056, Accuracy = 0.42391303181648254
Training iter #2900:   Batch Loss = 0.794930, Accuracy = 0.30000001192092896
PERFORMANCE ON TEST SET: Batch Loss = 0.763155460357666, Accuracy = 0.42391303181648254
Training iter #3000:   Batch Loss = 0.738424, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7615708708763123, Accuracy = 0.42391303181648254
Training iter #3100:   Batch Loss = 0.733945, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7603265643119812, Accuracy = 0.42391303181648254
Training iter #3200:   Batch Loss = 0.747491, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7584028244018555, Accuracy = 0.43478259444236755
Training iter #3300:   Batch Loss = 0.721470, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7573654055595398, Accuracy = 0.43478259444236755
Training iter #3400:   Batch Loss = 0.729927, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7561258673667908, Accuracy = 0.43478259444236755
Training iter #3500:   Batch Loss = 0.727949, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.754647433757782, Accuracy = 0.44565218687057495
Training iter #3600:   Batch Loss = 0.734041, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7541484832763672, Accuracy = 0.44565218687057495
Training iter #3700:   Batch Loss = 0.780504, Accuracy = 0.3400000035762787
PERFORMANCE ON TEST SET: Batch Loss = 0.7527559995651245, Accuracy = 0.44565218687057495
Training iter #3800:   Batch Loss = 0.733667, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7520689964294434, Accuracy = 0.45652174949645996
Training iter #3900:   Batch Loss = 0.731406, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7518972158432007, Accuracy = 0.44565218687057495
Training iter #4000:   Batch Loss = 0.750002, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.75054532289505, Accuracy = 0.45652174949645996
Training iter #4100:   Batch Loss = 0.719392, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7506505846977234, Accuracy = 0.45652174949645996
Training iter #4200:   Batch Loss = 0.735713, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7501147389411926, Accuracy = 0.45652174949645996
Training iter #4300:   Batch Loss = 0.719272, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7494037747383118, Accuracy = 0.45652174949645996
Training iter #4400:   Batch Loss = 0.735110, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7502745985984802, Accuracy = 0.44565218687057495
Training iter #4500:   Batch Loss = 0.764335, Accuracy = 0.3799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 0.7495473623275757, Accuracy = 0.45652174949645996
Training iter #4600:   Batch Loss = 0.720778, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7493096590042114, Accuracy = 0.45652174949645996
Training iter #4700:   Batch Loss = 0.732984, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7500592470169067, Accuracy = 0.43478259444236755
Training iter #4800:   Batch Loss = 0.731020, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7477768659591675, Accuracy = 0.45652174949645996
Training iter #4900:   Batch Loss = 0.713195, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7483364343643188, Accuracy = 0.45652174949645996
Training iter #5000:   Batch Loss = 0.745232, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7477399706840515, Accuracy = 0.45652174949645996
Training iter #5100:   Batch Loss = 0.715327, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7463914155960083, Accuracy = 0.45652174949645996
Training iter #5200:   Batch Loss = 0.738437, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7470300197601318, Accuracy = 0.45652174949645996
Training iter #5300:   Batch Loss = 0.764955, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 0.7454684972763062, Accuracy = 0.45652174949645996
Training iter #5400:   Batch Loss = 0.722526, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7444148659706116, Accuracy = 0.45652174949645996
Training iter #5500:   Batch Loss = 0.725006, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7446660399436951, Accuracy = 0.45652174949645996
Training iter #5600:   Batch Loss = 0.723486, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7420122027397156, Accuracy = 0.45652174949645996
Training iter #5700:   Batch Loss = 0.718333, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.742511510848999, Accuracy = 0.45652174949645996
Training iter #5800:   Batch Loss = 0.744503, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7416796088218689, Accuracy = 0.45652174949645996
Training iter #5900:   Batch Loss = 0.713835, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7406057119369507, Accuracy = 0.47826087474823
Training iter #6000:   Batch Loss = 0.739260, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7418949007987976, Accuracy = 0.45652174949645996
Training iter #6100:   Batch Loss = 0.755585, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.740279495716095, Accuracy = 0.47826087474823
Training iter #6200:   Batch Loss = 0.717680, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.739746630191803, Accuracy = 0.47826087474823
Training iter #6300:   Batch Loss = 0.716515, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7405195236206055, Accuracy = 0.46739131212234497
Training iter #6400:   Batch Loss = 0.722268, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7371876239776611, Accuracy = 0.489130437374115
Training iter #6500:   Batch Loss = 0.716838, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7383418679237366, Accuracy = 0.46739131212234497
Training iter #6600:   Batch Loss = 0.741912, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7373028993606567, Accuracy = 0.47826087474823
Training iter #6700:   Batch Loss = 0.715451, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.736143946647644, Accuracy = 0.47826087474823
Training iter #6800:   Batch Loss = 0.730402, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7380968332290649, Accuracy = 0.46739131212234497
Training iter #6900:   Batch Loss = 0.748504, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7360904216766357, Accuracy = 0.47826087474823
Training iter #7000:   Batch Loss = 0.719839, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7357350587844849, Accuracy = 0.47826087474823
Training iter #7100:   Batch Loss = 0.715379, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7376695871353149, Accuracy = 0.46739131212234497
Training iter #7200:   Batch Loss = 0.730241, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7336769700050354, Accuracy = 0.46739131212234497
Training iter #7300:   Batch Loss = 0.713124, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7346336841583252, Accuracy = 0.47826087474823
Training iter #7400:   Batch Loss = 0.742746, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7335218787193298, Accuracy = 0.46739131212234497
Training iter #7500:   Batch Loss = 0.707044, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7324385046958923, Accuracy = 0.46739131212234497
Training iter #7600:   Batch Loss = 0.723705, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7345144748687744, Accuracy = 0.47826087474823
Training iter #7700:   Batch Loss = 0.748121, Accuracy = 0.3799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 0.7330936193466187, Accuracy = 0.46739131212234497
Training iter #7800:   Batch Loss = 0.715657, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7334039807319641, Accuracy = 0.47826087474823
Training iter #7900:   Batch Loss = 0.707569, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.736163854598999, Accuracy = 0.46739131212234497
Training iter #8000:   Batch Loss = 0.723872, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7318124175071716, Accuracy = 0.46739131212234497
Training iter #8100:   Batch Loss = 0.713726, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7329106330871582, Accuracy = 0.47826087474823
Training iter #8200:   Batch Loss = 0.736296, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.731547474861145, Accuracy = 0.46739131212234497
Training iter #8300:   Batch Loss = 0.708800, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7300061583518982, Accuracy = 0.46739131212234497
Training iter #8400:   Batch Loss = 0.721847, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7327514290809631, Accuracy = 0.47826087474823
Training iter #8500:   Batch Loss = 0.753000, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 0.731280505657196, Accuracy = 0.46739131212234497
Training iter #8600:   Batch Loss = 0.710753, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7320539951324463, Accuracy = 0.46739131212234497
Training iter #8700:   Batch Loss = 0.708154, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7348939180374146, Accuracy = 0.45652174949645996
Training iter #8800:   Batch Loss = 0.723307, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7302199006080627, Accuracy = 0.45652174949645996
Training iter #8900:   Batch Loss = 0.709454, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7315218448638916, Accuracy = 0.46739131212234497
Training iter #9000:   Batch Loss = 0.733128, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7296229004859924, Accuracy = 0.45652174949645996
Training iter #9100:   Batch Loss = 0.710272, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7283121347427368, Accuracy = 0.46739131212234497
Training iter #9200:   Batch Loss = 0.719666, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7312844395637512, Accuracy = 0.46739131212234497
Training iter #9300:   Batch Loss = 0.750873, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 0.7295420169830322, Accuracy = 0.45652174949645996
Training iter #9400:   Batch Loss = 0.704078, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.729938805103302, Accuracy = 0.45652174949645996
Training iter #9500:   Batch Loss = 0.705568, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7337033748626709, Accuracy = 0.45652174949645996
Training iter #9600:   Batch Loss = 0.716265, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7287433743476868, Accuracy = 0.45652174949645996
Training iter #9700:   Batch Loss = 0.710814, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7304162383079529, Accuracy = 0.45652174949645996
Training iter #9800:   Batch Loss = 0.734467, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7278940677642822, Accuracy = 0.45652174949645996
Training iter #9900:   Batch Loss = 0.705341, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.726352334022522, Accuracy = 0.47826087474823
Training iter #10000:   Batch Loss = 0.714618, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7291412353515625, Accuracy = 0.45652174949645996
Training iter #10100:   Batch Loss = 0.742407, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7273342609405518, Accuracy = 0.45652174949645996
Training iter #10200:   Batch Loss = 0.697414, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7280945181846619, Accuracy = 0.45652174949645996
Training iter #10300:   Batch Loss = 0.702854, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7315066456794739, Accuracy = 0.45652174949645996
Training iter #10400:   Batch Loss = 0.710668, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7261497378349304, Accuracy = 0.45652174949645996
Training iter #10500:   Batch Loss = 0.709773, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7280265688896179, Accuracy = 0.45652174949645996
Training iter #10600:   Batch Loss = 0.725693, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7250036597251892, Accuracy = 0.46739131212234497
Training iter #10700:   Batch Loss = 0.703658, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7229781746864319, Accuracy = 0.47826087474823
Training iter #10800:   Batch Loss = 0.708130, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7261963486671448, Accuracy = 0.45652174949645996
Training iter #10900:   Batch Loss = 0.734819, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7232775092124939, Accuracy = 0.46739131212234497
Training iter #11000:   Batch Loss = 0.698972, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7239828705787659, Accuracy = 0.45652174949645996
Training iter #11100:   Batch Loss = 0.700743, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7267611622810364, Accuracy = 0.44565218687057495
Training iter #11200:   Batch Loss = 0.710407, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7220773696899414, Accuracy = 0.489130437374115
Training iter #11300:   Batch Loss = 0.714162, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7251584529876709, Accuracy = 0.45652174949645996
Training iter #11400:   Batch Loss = 0.709793, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.722733736038208, Accuracy = 0.489130437374115
Training iter #11500:   Batch Loss = 0.713261, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7218260765075684, Accuracy = 0.47826087474823
Training iter #11600:   Batch Loss = 0.700945, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7264114022254944, Accuracy = 0.47826087474823
Training iter #11700:   Batch Loss = 0.730780, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7246683835983276, Accuracy = 0.489130437374115
Training iter #11800:   Batch Loss = 0.686170, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7259168028831482, Accuracy = 0.47826087474823
Training iter #11900:   Batch Loss = 0.692586, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7294821739196777, Accuracy = 0.46739131212234497
Training iter #12000:   Batch Loss = 0.706519, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7243828177452087, Accuracy = 0.44565218687057495
Training iter #12100:   Batch Loss = 0.704700, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7269407510757446, Accuracy = 0.45652174949645996
Training iter #12200:   Batch Loss = 0.706596, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7243517637252808, Accuracy = 0.43478259444236755
Training iter #12300:   Batch Loss = 0.720295, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.724362313747406, Accuracy = 0.43478259444236755
Training iter #12400:   Batch Loss = 0.687377, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7299902439117432, Accuracy = 0.44565218687057495
Training iter #12500:   Batch Loss = 0.716932, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7279761433601379, Accuracy = 0.42391303181648254
Training iter #12600:   Batch Loss = 0.682073, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7305934429168701, Accuracy = 0.43478259444236755
Training iter #12700:   Batch Loss = 0.695055, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7331517934799194, Accuracy = 0.4021739065647125
Training iter #12800:   Batch Loss = 0.719191, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7236871719360352, Accuracy = 0.47826087474823
Training iter #12900:   Batch Loss = 0.694632, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7284837961196899, Accuracy = 0.44565218687057495
Training iter #13000:   Batch Loss = 0.711602, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7244121432304382, Accuracy = 0.46739131212234497
Training iter #13100:   Batch Loss = 0.712637, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7263267636299133, Accuracy = 0.47826087474823
Training iter #13200:   Batch Loss = 0.684104, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.73649001121521, Accuracy = 0.43478259444236755
Training iter #13300:   Batch Loss = 0.714915, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7318406701087952, Accuracy = 0.41304346919059753
Training iter #13400:   Batch Loss = 0.679734, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7340876460075378, Accuracy = 0.42391303181648254
Training iter #13500:   Batch Loss = 0.694537, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7356443405151367, Accuracy = 0.41304346919059753
Training iter #13600:   Batch Loss = 0.717345, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7255950570106506, Accuracy = 0.45652174949645996
Training iter #13700:   Batch Loss = 0.690297, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7286407947540283, Accuracy = 0.44565218687057495
Training iter #13800:   Batch Loss = 0.696590, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.723563551902771, Accuracy = 0.489130437374115
Training iter #13900:   Batch Loss = 0.709895, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7236248850822449, Accuracy = 0.489130437374115
Training iter #14000:   Batch Loss = 0.684952, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7299346327781677, Accuracy = 0.43478259444236755
Training iter #14100:   Batch Loss = 0.712027, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7289441227912903, Accuracy = 0.45652174949645996
Training iter #14200:   Batch Loss = 0.687294, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7340862154960632, Accuracy = 0.41304346919059753
Training iter #14300:   Batch Loss = 0.701917, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.737842857837677, Accuracy = 0.42391303181648254
Training iter #14400:   Batch Loss = 0.710477, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7303056120872498, Accuracy = 0.43478259444236755
Training iter #14500:   Batch Loss = 0.692801, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7345761060714722, Accuracy = 0.42391303181648254
Training iter #14600:   Batch Loss = 0.712637, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7265002727508545, Accuracy = 0.489130437374115
Training iter #14700:   Batch Loss = 0.704603, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7248707413673401, Accuracy = 0.5
Training iter #14800:   Batch Loss = 0.689773, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7284493446350098, Accuracy = 0.45652174949645996
Training iter #14900:   Batch Loss = 0.706547, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7267165184020996, Accuracy = 0.47826087474823
Training iter #15000:   Batch Loss = 0.691728, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7315472960472107, Accuracy = 0.42391303181648254
Training iter #15100:   Batch Loss = 0.693467, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7367473244667053, Accuracy = 0.42391303181648254
Training iter #15200:   Batch Loss = 0.708618, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7311770915985107, Accuracy = 0.44565218687057495
Training iter #15300:   Batch Loss = 0.692404, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7368205189704895, Accuracy = 0.43478259444236755
Training iter #15400:   Batch Loss = 0.711417, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7293229103088379, Accuracy = 0.45652174949645996
Training iter #15500:   Batch Loss = 0.703912, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7267740964889526, Accuracy = 0.489130437374115
Training iter #15600:   Batch Loss = 0.694580, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7296395897865295, Accuracy = 0.44565218687057495
Training iter #15700:   Batch Loss = 0.700787, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7267873287200928, Accuracy = 0.47826087474823
Training iter #15800:   Batch Loss = 0.692131, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7309136986732483, Accuracy = 0.44565218687057495
Training iter #15900:   Batch Loss = 0.691934, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7350971698760986, Accuracy = 0.42391303181648254
Training iter #16000:   Batch Loss = 0.713332, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7306211590766907, Accuracy = 0.44565218687057495
Training iter #16100:   Batch Loss = 0.695814, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7366212606430054, Accuracy = 0.42391303181648254
Training iter #16200:   Batch Loss = 0.713985, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7301656603813171, Accuracy = 0.44565218687057495
Training iter #16300:   Batch Loss = 0.698150, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7288055419921875, Accuracy = 0.45652174949645996
Training iter #16400:   Batch Loss = 0.692777, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7314503788948059, Accuracy = 0.45652174949645996
Training iter #16500:   Batch Loss = 0.702128, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7259730100631714, Accuracy = 0.489130437374115
Training iter #16600:   Batch Loss = 0.690275, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7299614548683167, Accuracy = 0.44565218687057495
Training iter #16700:   Batch Loss = 0.695619, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7325143814086914, Accuracy = 0.43478259444236755
Training iter #16800:   Batch Loss = 0.708921, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7292906641960144, Accuracy = 0.47826087474823
Training iter #16900:   Batch Loss = 0.694051, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7357692122459412, Accuracy = 0.42391303181648254
Training iter #17000:   Batch Loss = 0.716977, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7289424538612366, Accuracy = 0.46739131212234497
Training iter #17100:   Batch Loss = 0.687513, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7277641892433167, Accuracy = 0.47826087474823
Training iter #17200:   Batch Loss = 0.690376, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7311680912971497, Accuracy = 0.44565218687057495
Training iter #17300:   Batch Loss = 0.707113, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7253957986831665, Accuracy = 0.489130437374115
Training iter #17400:   Batch Loss = 0.688678, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7308529019355774, Accuracy = 0.45652174949645996
Training iter #17500:   Batch Loss = 0.694837, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7331891655921936, Accuracy = 0.45652174949645996
Training iter #17600:   Batch Loss = 0.712628, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7307339906692505, Accuracy = 0.45652174949645996
Training iter #17700:   Batch Loss = 0.689105, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.736273467540741, Accuracy = 0.42391303181648254
Training iter #17800:   Batch Loss = 0.710750, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7304799556732178, Accuracy = 0.45652174949645996
Training iter #17900:   Batch Loss = 0.679323, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.729251503944397, Accuracy = 0.44565218687057495
Training iter #18000:   Batch Loss = 0.691902, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7317444682121277, Accuracy = 0.44565218687057495
Training iter #18100:   Batch Loss = 0.701849, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7247490286827087, Accuracy = 0.489130437374115
Training iter #18200:   Batch Loss = 0.689547, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7298397421836853, Accuracy = 0.45652174949645996
Training iter #18300:   Batch Loss = 0.693226, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7323527336120605, Accuracy = 0.45652174949645996
Training iter #18400:   Batch Loss = 0.712779, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7295493483543396, Accuracy = 0.46739131212234497
Training iter #18500:   Batch Loss = 0.681730, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.735624372959137, Accuracy = 0.41304346919059753
Training iter #18600:   Batch Loss = 0.707722, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7302366495132446, Accuracy = 0.46739131212234497
Training iter #18700:   Batch Loss = 0.685118, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7274637818336487, Accuracy = 0.47826087474823
Training iter #18800:   Batch Loss = 0.683875, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7304346561431885, Accuracy = 0.45652174949645996
Training iter #18900:   Batch Loss = 0.698219, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7249355912208557, Accuracy = 0.489130437374115
Training iter #19000:   Batch Loss = 0.686104, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7307004332542419, Accuracy = 0.46739131212234497
Training iter #19100:   Batch Loss = 0.702057, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7332104444503784, Accuracy = 0.44565218687057495
Training iter #19200:   Batch Loss = 0.719796, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7313600778579712, Accuracy = 0.46739131212234497
Training iter #19300:   Batch Loss = 0.675555, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7379624843597412, Accuracy = 0.41304346919059753
Training iter #19400:   Batch Loss = 0.701394, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7311832904815674, Accuracy = 0.46739131212234497
Training iter #19500:   Batch Loss = 0.671355, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.727426290512085, Accuracy = 0.489130437374115
Training iter #19600:   Batch Loss = 0.674088, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7297009229660034, Accuracy = 0.47826087474823
Training iter #19700:   Batch Loss = 0.703238, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7235593199729919, Accuracy = 0.489130437374115
Training iter #19800:   Batch Loss = 0.681626, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7299976348876953, Accuracy = 0.47826087474823
Training iter #19900:   Batch Loss = 0.694754, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7325358390808105, Accuracy = 0.45652174949645996
Training iter #20000:   Batch Loss = 0.710593, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7309054136276245, Accuracy = 0.47826087474823
Training iter #20100:   Batch Loss = 0.671011, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7381250858306885, Accuracy = 0.41304346919059753
Training iter #20200:   Batch Loss = 0.704572, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7304765582084656, Accuracy = 0.489130437374115
Training iter #20300:   Batch Loss = 0.675976, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7273613214492798, Accuracy = 0.489130437374115
Training iter #20400:   Batch Loss = 0.671810, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.731762707233429, Accuracy = 0.46739131212234497
Training iter #20500:   Batch Loss = 0.715291, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7241220474243164, Accuracy = 0.489130437374115
Training iter #20600:   Batch Loss = 0.673026, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.729290783405304, Accuracy = 0.47826087474823
Training iter #20700:   Batch Loss = 0.688287, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7308309078216553, Accuracy = 0.47826087474823
Training iter #20800:   Batch Loss = 0.706536, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7301743030548096, Accuracy = 0.47826087474823
Training iter #20900:   Batch Loss = 0.652227, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7381101250648499, Accuracy = 0.44565218687057495
Training iter #21000:   Batch Loss = 0.711975, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7327384948730469, Accuracy = 0.45652174949645996
Training iter #21100:   Batch Loss = 0.663213, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7308769822120667, Accuracy = 0.47826087474823
Training iter #21200:   Batch Loss = 0.657998, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7346941828727722, Accuracy = 0.45652174949645996
Training iter #21300:   Batch Loss = 0.711984, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7244492173194885, Accuracy = 0.489130437374115
Training iter #21400:   Batch Loss = 0.677847, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7283309102058411, Accuracy = 0.489130437374115
Training iter #21500:   Batch Loss = 0.673426, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7282138466835022, Accuracy = 0.489130437374115
Training iter #21600:   Batch Loss = 0.705370, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7276754379272461, Accuracy = 0.489130437374115
Training iter #21700:   Batch Loss = 0.654496, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7376912832260132, Accuracy = 0.46739131212234497
Training iter #21800:   Batch Loss = 0.727090, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7332425713539124, Accuracy = 0.45652174949645996
Training iter #21900:   Batch Loss = 0.648391, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7333829998970032, Accuracy = 0.45652174949645996
Training iter #22000:   Batch Loss = 0.665838, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7364435195922852, Accuracy = 0.46739131212234497
Training iter #22100:   Batch Loss = 0.706110, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7251647710800171, Accuracy = 0.489130437374115
Training iter #22200:   Batch Loss = 0.674212, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7284934520721436, Accuracy = 0.489130437374115
Training iter #22300:   Batch Loss = 0.671115, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7271174192428589, Accuracy = 0.489130437374115
Training iter #22400:   Batch Loss = 0.707263, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7274634838104248, Accuracy = 0.489130437374115
Training iter #22500:   Batch Loss = 0.646920, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7372145652770996, Accuracy = 0.46739131212234497
Training iter #22600:   Batch Loss = 0.719455, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7334443926811218, Accuracy = 0.47826087474823
Training iter #22700:   Batch Loss = 0.636770, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7334729433059692, Accuracy = 0.47826087474823
Training iter #22800:   Batch Loss = 0.665009, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7383438944816589, Accuracy = 0.46739131212234497
Training iter #22900:   Batch Loss = 0.705398, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7264909744262695, Accuracy = 0.489130437374115
Training iter #23000:   Batch Loss = 0.667126, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7301535606384277, Accuracy = 0.489130437374115
Training iter #23100:   Batch Loss = 0.675212, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7265938520431519, Accuracy = 0.489130437374115
Training iter #23200:   Batch Loss = 0.703150, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7261465787887573, Accuracy = 0.489130437374115
Training iter #23300:   Batch Loss = 0.634837, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7352250218391418, Accuracy = 0.46739131212234497
Training iter #23400:   Batch Loss = 0.712415, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.733028769493103, Accuracy = 0.47826087474823
Training iter #23500:   Batch Loss = 0.640285, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7357693314552307, Accuracy = 0.45652174949645996
Training iter #23600:   Batch Loss = 0.656166, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7407273054122925, Accuracy = 0.45652174949645996
Training iter #23700:   Batch Loss = 0.711436, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7268343567848206, Accuracy = 0.489130437374115
Training iter #23800:   Batch Loss = 0.665474, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7300219535827637, Accuracy = 0.489130437374115
Training iter #23900:   Batch Loss = 0.668933, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7257624268531799, Accuracy = 0.489130437374115
Training iter #24000:   Batch Loss = 0.703469, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7251104116439819, Accuracy = 0.489130437374115
Training iter #24100:   Batch Loss = 0.634299, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7353150844573975, Accuracy = 0.47826087474823
Training iter #24200:   Batch Loss = 0.709168, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7318191528320312, Accuracy = 0.489130437374115
Training iter #24300:   Batch Loss = 0.640341, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7352665066719055, Accuracy = 0.47826087474823
Training iter #24400:   Batch Loss = 0.652606, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7390111684799194, Accuracy = 0.45652174949645996
Training iter #24500:   Batch Loss = 0.715857, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7263472676277161, Accuracy = 0.489130437374115
Training iter #24600:   Batch Loss = 0.669219, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7310457229614258, Accuracy = 0.5
Training iter #24700:   Batch Loss = 0.658936, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7256407737731934, Accuracy = 0.489130437374115
Training iter #24800:   Batch Loss = 0.702538, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.725564181804657, Accuracy = 0.489130437374115
Training iter #24900:   Batch Loss = 0.634425, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7368786334991455, Accuracy = 0.46739131212234497
Training iter #25000:   Batch Loss = 0.709304, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7331182956695557, Accuracy = 0.47826087474823
Training iter #25100:   Batch Loss = 0.634871, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.734950840473175, Accuracy = 0.489130437374115
Training iter #25200:   Batch Loss = 0.661007, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7380246520042419, Accuracy = 0.47826087474823
Training iter #25300:   Batch Loss = 0.707919, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7254048585891724, Accuracy = 0.489130437374115
Training iter #25400:   Batch Loss = 0.667334, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7286707162857056, Accuracy = 0.489130437374115
Training iter #25500:   Batch Loss = 0.664447, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7243878245353699, Accuracy = 0.489130437374115
Training iter #25600:   Batch Loss = 0.702768, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7258514761924744, Accuracy = 0.489130437374115
Training iter #25700:   Batch Loss = 0.624218, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7382969260215759, Accuracy = 0.46739131212234497
Training iter #25800:   Batch Loss = 0.691643, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7337778806686401, Accuracy = 0.489130437374115
Training iter #25900:   Batch Loss = 0.630970, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7361520528793335, Accuracy = 0.47826087474823
Training iter #26000:   Batch Loss = 0.664251, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7362074255943298, Accuracy = 0.46739131212234497
Training iter #26100:   Batch Loss = 0.723662, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7237824201583862, Accuracy = 0.489130437374115
Training iter #26200:   Batch Loss = 0.666075, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7269573211669922, Accuracy = 0.5
Training iter #26300:   Batch Loss = 0.676324, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7230228185653687, Accuracy = 0.489130437374115
Training iter #26400:   Batch Loss = 0.692419, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7255308628082275, Accuracy = 0.5
Training iter #26500:   Batch Loss = 0.631581, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7393579483032227, Accuracy = 0.46739131212234497
Training iter #26600:   Batch Loss = 0.701990, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.732978880405426, Accuracy = 0.510869562625885
Training iter #26700:   Batch Loss = 0.634901, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7354435324668884, Accuracy = 0.46739131212234497
Training iter #26800:   Batch Loss = 0.666421, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7354231476783752, Accuracy = 0.46739131212234497
Training iter #26900:   Batch Loss = 0.721623, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7228224277496338, Accuracy = 0.510869562625885
Training iter #27000:   Batch Loss = 0.654180, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7274667620658875, Accuracy = 0.5
Training iter #27100:   Batch Loss = 0.666812, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7240309119224548, Accuracy = 0.489130437374115
Training iter #27200:   Batch Loss = 0.681460, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7264295220375061, Accuracy = 0.5
Training iter #27300:   Batch Loss = 0.640683, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7380306124687195, Accuracy = 0.46739131212234497
Training iter #27400:   Batch Loss = 0.695626, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7316794395446777, Accuracy = 0.510869562625885
Training iter #27500:   Batch Loss = 0.647733, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7342876195907593, Accuracy = 0.510869562625885
Training iter #27600:   Batch Loss = 0.674407, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7339503765106201, Accuracy = 0.46739131212234497
Training iter #27700:   Batch Loss = 0.703499, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7232642769813538, Accuracy = 0.5
Training iter #27800:   Batch Loss = 0.663162, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7289647459983826, Accuracy = 0.510869562625885
Training iter #27900:   Batch Loss = 0.679577, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7227052450180054, Accuracy = 0.510869562625885
Training iter #28000:   Batch Loss = 0.673623, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7246916890144348, Accuracy = 0.510869562625885
Training iter #28100:   Batch Loss = 0.649051, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7338602542877197, Accuracy = 0.47826087474823
Training iter #28200:   Batch Loss = 0.689979, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7285876870155334, Accuracy = 0.5
Training iter #28300:   Batch Loss = 0.651123, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7339593172073364, Accuracy = 0.47826087474823
Training iter #28400:   Batch Loss = 0.666650, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.736409604549408, Accuracy = 0.46739131212234497
Training iter #28500:   Batch Loss = 0.702592, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7246138453483582, Accuracy = 0.510869562625885
Training iter #28600:   Batch Loss = 0.657260, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.729962170124054, Accuracy = 0.52173912525177
Training iter #28700:   Batch Loss = 0.670915, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7227039337158203, Accuracy = 0.52173912525177
Training iter #28800:   Batch Loss = 0.666452, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7235202193260193, Accuracy = 0.52173912525177
Training iter #28900:   Batch Loss = 0.658044, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7323940396308899, Accuracy = 0.489130437374115
Training iter #29000:   Batch Loss = 0.682680, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7283592820167542, Accuracy = 0.52173912525177
Training iter #29100:   Batch Loss = 0.647452, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7348660230636597, Accuracy = 0.46739131212234497
Training iter #29200:   Batch Loss = 0.671525, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7355142831802368, Accuracy = 0.46739131212234497
Training iter #29300:   Batch Loss = 0.698723, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.723924994468689, Accuracy = 0.52173912525177
Training iter #29400:   Batch Loss = 0.668356, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.728607714176178, Accuracy = 0.52173912525177
Training iter #29500:   Batch Loss = 0.668811, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7220523357391357, Accuracy = 0.532608687877655
Training iter #29600:   Batch Loss = 0.653752, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7247616052627563, Accuracy = 0.52173912525177
Training iter #29700:   Batch Loss = 0.656885, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7335600852966309, Accuracy = 0.489130437374115
Training iter #29800:   Batch Loss = 0.689457, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7264014482498169, Accuracy = 0.510869562625885
Training iter #29900:   Batch Loss = 0.649198, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7339922785758972, Accuracy = 0.46739131212234497
Training iter #30000:   Batch Loss = 0.679484, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7336175441741943, Accuracy = 0.46739131212234497
Training iter #30100:   Batch Loss = 0.685953, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7250997424125671, Accuracy = 0.52173912525177
Training iter #30200:   Batch Loss = 0.665799, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7312338352203369, Accuracy = 0.52173912525177
Training iter #30300:   Batch Loss = 0.672231, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7231365442276001, Accuracy = 0.532608687877655
Training iter #30400:   Batch Loss = 0.641018, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7251733541488647, Accuracy = 0.52173912525177
Training iter #30500:   Batch Loss = 0.655097, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7338559627532959, Accuracy = 0.489130437374115
Training iter #30600:   Batch Loss = 0.697400, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7255154252052307, Accuracy = 0.510869562625885
Training iter #30700:   Batch Loss = 0.650393, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7342419028282166, Accuracy = 0.46739131212234497
Training iter #30800:   Batch Loss = 0.674980, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7321627736091614, Accuracy = 0.46739131212234497
Training iter #30900:   Batch Loss = 0.693415, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7243717312812805, Accuracy = 0.532608687877655
Training iter #31000:   Batch Loss = 0.654577, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7295916080474854, Accuracy = 0.510869562625885
Training iter #31100:   Batch Loss = 0.667971, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7243412733078003, Accuracy = 0.532608687877655
Training iter #31200:   Batch Loss = 0.634624, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7280457615852356, Accuracy = 0.5
Training iter #31300:   Batch Loss = 0.661936, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7370755076408386, Accuracy = 0.46739131212234497
Training iter #31400:   Batch Loss = 0.693358, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7262889742851257, Accuracy = 0.52173912525177
Training iter #31500:   Batch Loss = 0.652889, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7350051999092102, Accuracy = 0.45652174949645996
Training iter #31600:   Batch Loss = 0.675555, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7326502203941345, Accuracy = 0.45652174949645996
Training iter #31700:   Batch Loss = 0.701527, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7236865162849426, Accuracy = 0.54347825050354
Training iter #31800:   Batch Loss = 0.637451, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7277438044548035, Accuracy = 0.510869562625885
Training iter #31900:   Batch Loss = 0.658936, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7247530817985535, Accuracy = 0.54347825050354
Training iter #32000:   Batch Loss = 0.642570, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7286954522132874, Accuracy = 0.52173912525177
Training iter #32100:   Batch Loss = 0.647616, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.740363597869873, Accuracy = 0.47826087474823
Training iter #32200:   Batch Loss = 0.682692, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7308446764945984, Accuracy = 0.52173912525177
Training iter #32300:   Batch Loss = 0.645004, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7401486039161682, Accuracy = 0.46739131212234497
Training iter #32400:   Batch Loss = 0.691854, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7338844537734985, Accuracy = 0.489130437374115
Training iter #32500:   Batch Loss = 0.709932, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7239036560058594, Accuracy = 0.532608687877655
Training iter #32600:   Batch Loss = 0.625809, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7274675369262695, Accuracy = 0.52173912525177
Training iter #32700:   Batch Loss = 0.663342, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.723628044128418, Accuracy = 0.554347813129425
Training iter #32800:   Batch Loss = 0.620493, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7266198992729187, Accuracy = 0.532608687877655
Training iter #32900:   Batch Loss = 0.627904, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7360920310020447, Accuracy = 0.489130437374115
Training iter #33000:   Batch Loss = 0.697278, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7265567779541016, Accuracy = 0.54347825050354
Training iter #33100:   Batch Loss = 0.649337, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7393577098846436, Accuracy = 0.46739131212234497
Training iter #33200:   Batch Loss = 0.677922, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.736568808555603, Accuracy = 0.47826087474823
Training iter #33300:   Batch Loss = 0.697140, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7271736860275269, Accuracy = 0.532608687877655
Training iter #33400:   Batch Loss = 0.623350, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7316599488258362, Accuracy = 0.510869562625885
Training iter #33500:   Batch Loss = 0.662239, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7252669930458069, Accuracy = 0.532608687877655
Training iter #33600:   Batch Loss = 0.630576, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7266138195991516, Accuracy = 0.554347813129425
Training iter #33700:   Batch Loss = 0.635917, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7367150187492371, Accuracy = 0.46739131212234497
Training iter #33800:   Batch Loss = 0.717956, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7246871590614319, Accuracy = 0.554347813129425
Training iter #33900:   Batch Loss = 0.642321, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7333722710609436, Accuracy = 0.5
Training iter #34000:   Batch Loss = 0.664241, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7322952151298523, Accuracy = 0.510869562625885
Training iter #34100:   Batch Loss = 0.689696, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7275359034538269, Accuracy = 0.54347825050354
Training iter #34200:   Batch Loss = 0.602461, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7353377938270569, Accuracy = 0.47826087474823
Training iter #34300:   Batch Loss = 0.696050, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7291098833084106, Accuracy = 0.54347825050354
Training iter #34400:   Batch Loss = 0.620362, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7291239500045776, Accuracy = 0.54347825050354
Training iter #34500:   Batch Loss = 0.615544, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.735526442527771, Accuracy = 0.47826087474823
Training iter #34600:   Batch Loss = 0.725033, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7262285351753235, Accuracy = 0.532608687877655
Training iter #34700:   Batch Loss = 0.636779, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7395860552787781, Accuracy = 0.46739131212234497
Training iter #34800:   Batch Loss = 0.656740, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7313686609268188, Accuracy = 0.52173912525177
Training iter #34900:   Batch Loss = 0.690485, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7225404381752014, Accuracy = 0.554347813129425
Training iter #35000:   Batch Loss = 0.599461, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7277098894119263, Accuracy = 0.532608687877655
Training iter #35100:   Batch Loss = 0.705573, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7274404168128967, Accuracy = 0.54347825050354
Training iter #35200:   Batch Loss = 0.604294, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7349494099617004, Accuracy = 0.510869562625885
Training iter #35300:   Batch Loss = 0.635239, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7468625903129578, Accuracy = 0.44565218687057495
Training iter #35400:   Batch Loss = 0.707278, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7310076951980591, Accuracy = 0.52173912525177
Training iter #35500:   Batch Loss = 0.649171, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7357910871505737, Accuracy = 0.489130437374115
Training iter #35600:   Batch Loss = 0.637681, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7290083765983582, Accuracy = 0.532608687877655
Training iter #35700:   Batch Loss = 0.686628, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7247568964958191, Accuracy = 0.554347813129425
Training iter #35800:   Batch Loss = 0.598513, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7342333793640137, Accuracy = 0.47826087474823
Training iter #35900:   Batch Loss = 0.714160, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7293563485145569, Accuracy = 0.532608687877655
Training iter #36000:   Batch Loss = 0.598647, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7304573655128479, Accuracy = 0.54347825050354
Training iter #36100:   Batch Loss = 0.637284, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7357780337333679, Accuracy = 0.5
Training iter #36200:   Batch Loss = 0.731405, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7311394214630127, Accuracy = 0.54347825050354
Training iter #36300:   Batch Loss = 0.651006, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7483355402946472, Accuracy = 0.46739131212234497
Training iter #36400:   Batch Loss = 0.663053, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7420694828033447, Accuracy = 0.47826087474823
Training iter #36500:   Batch Loss = 0.689741, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7329857349395752, Accuracy = 0.532608687877655
Training iter #36600:   Batch Loss = 0.591820, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7367129325866699, Accuracy = 0.510869562625885
Training iter #36700:   Batch Loss = 0.695956, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7303373217582703, Accuracy = 0.54347825050354
Training iter #36800:   Batch Loss = 0.611749, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7326881289482117, Accuracy = 0.52173912525177
Training iter #36900:   Batch Loss = 0.623564, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7392053604125977, Accuracy = 0.45652174949645996
Training iter #37000:   Batch Loss = 0.729463, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.727265477180481, Accuracy = 0.54347825050354
Training iter #37100:   Batch Loss = 0.649144, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7353722453117371, Accuracy = 0.46739131212234497
Training iter #37200:   Batch Loss = 0.644546, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7296479344367981, Accuracy = 0.532608687877655
Training iter #37300:   Batch Loss = 0.681485, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7261558771133423, Accuracy = 0.54347825050354
Training iter #37400:   Batch Loss = 0.586767, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7368848919868469, Accuracy = 0.46739131212234497
Training iter #37500:   Batch Loss = 0.700705, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7315584421157837, Accuracy = 0.52173912525177
Training iter #37600:   Batch Loss = 0.610618, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7353962063789368, Accuracy = 0.510869562625885
Training iter #37700:   Batch Loss = 0.617368, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7408981919288635, Accuracy = 0.45652174949645996
Training iter #37800:   Batch Loss = 0.730471, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7264432311058044, Accuracy = 0.532608687877655
Training iter #37900:   Batch Loss = 0.658881, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7368078231811523, Accuracy = 0.5
Training iter #38000:   Batch Loss = 0.635722, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7300766110420227, Accuracy = 0.532608687877655
Training iter #38100:   Batch Loss = 0.678441, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.728243887424469, Accuracy = 0.532608687877655
Training iter #38200:   Batch Loss = 0.590387, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7399492859840393, Accuracy = 0.45652174949645996
Training iter #38300:   Batch Loss = 0.695989, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7334685921669006, Accuracy = 0.52173912525177
Training iter #38400:   Batch Loss = 0.609105, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7342899441719055, Accuracy = 0.5
Training iter #38500:   Batch Loss = 0.636131, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7379394769668579, Accuracy = 0.46739131212234497
Training iter #38600:   Batch Loss = 0.711207, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7275689840316772, Accuracy = 0.532608687877655
Training iter #38700:   Batch Loss = 0.659003, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7338986992835999, Accuracy = 0.532608687877655
Training iter #38800:   Batch Loss = 0.636397, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7303453087806702, Accuracy = 0.532608687877655
Training iter #38900:   Batch Loss = 0.675590, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7318486571311951, Accuracy = 0.54347825050354
Training iter #39000:   Batch Loss = 0.585607, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7453427910804749, Accuracy = 0.46739131212234497
Training iter #39100:   Batch Loss = 0.682955, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7335602045059204, Accuracy = 0.54347825050354
Training iter #39200:   Batch Loss = 0.614324, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7326032519340515, Accuracy = 0.532608687877655
Training iter #39300:   Batch Loss = 0.635493, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7344159483909607, Accuracy = 0.47826087474823
Training iter #39400:   Batch Loss = 0.725063, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7278589010238647, Accuracy = 0.54347825050354
Training iter #39500:   Batch Loss = 0.645440, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7368695735931396, Accuracy = 0.489130437374115
Training iter #39600:   Batch Loss = 0.659391, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7329556941986084, Accuracy = 0.532608687877655
Training iter #39700:   Batch Loss = 0.653939, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.731871485710144, Accuracy = 0.54347825050354
Training iter #39800:   Batch Loss = 0.605361, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7427676916122437, Accuracy = 0.45652174949645996
Training iter #39900:   Batch Loss = 0.698823, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7348911166191101, Accuracy = 0.532608687877655
Training iter #40000:   Batch Loss = 0.606939, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7418639659881592, Accuracy = 0.46739131212234497
Training iter #40100:   Batch Loss = 0.637067, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7383344173431396, Accuracy = 0.47826087474823
Training iter #40200:   Batch Loss = 0.732734, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.724729597568512, Accuracy = 0.5869565010070801
Training iter #40300:   Batch Loss = 0.642604, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7281870245933533, Accuracy = 0.554347813129425
Training iter #40400:   Batch Loss = 0.623729, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.728524923324585, Accuracy = 0.54347825050354
Training iter #40500:   Batch Loss = 0.650683, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7332338690757751, Accuracy = 0.5
Training iter #40600:   Batch Loss = 0.619097, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7498730421066284, Accuracy = 0.46739131212234497
Training iter #40700:   Batch Loss = 0.694616, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7368985414505005, Accuracy = 0.52173912525177
Training iter #40800:   Batch Loss = 0.637040, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7354452013969421, Accuracy = 0.54347825050354
Training iter #40900:   Batch Loss = 0.644866, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7350924611091614, Accuracy = 0.54347825050354
Training iter #41000:   Batch Loss = 0.693673, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7292824387550354, Accuracy = 0.52173912525177
Training iter #41100:   Batch Loss = 0.647479, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7395612001419067, Accuracy = 0.489130437374115
Training iter #41200:   Batch Loss = 0.663765, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7329221367835999, Accuracy = 0.54347825050354
Training iter #41300:   Batch Loss = 0.634057, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.732548713684082, Accuracy = 0.54347825050354
Training iter #41400:   Batch Loss = 0.627286, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7395073771476746, Accuracy = 0.489130437374115
Training iter #41500:   Batch Loss = 0.686496, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7314239144325256, Accuracy = 0.52173912525177
Training iter #41600:   Batch Loss = 0.645000, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7350245118141174, Accuracy = 0.54347825050354
Training iter #41700:   Batch Loss = 0.639465, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.739699125289917, Accuracy = 0.47826087474823
Training iter #41800:   Batch Loss = 0.689706, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7326698303222656, Accuracy = 0.532608687877655
Training iter #41900:   Batch Loss = 0.634608, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7405850291252136, Accuracy = 0.47826087474823
Training iter #42000:   Batch Loss = 0.639228, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7328235507011414, Accuracy = 0.532608687877655
Training iter #42100:   Batch Loss = 0.628934, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.732153058052063, Accuracy = 0.532608687877655
Training iter #42200:   Batch Loss = 0.636049, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7422076463699341, Accuracy = 0.46739131212234497
Training iter #42300:   Batch Loss = 0.671783, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7371281385421753, Accuracy = 0.52173912525177
Training iter #42400:   Batch Loss = 0.638530, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7430382370948792, Accuracy = 0.46739131212234497
Training iter #42500:   Batch Loss = 0.647586, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7439165711402893, Accuracy = 0.46739131212234497
Training iter #42600:   Batch Loss = 0.679264, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7322207093238831, Accuracy = 0.532608687877655
Training iter #42700:   Batch Loss = 0.644682, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7371923327445984, Accuracy = 0.510869562625885
Training iter #42800:   Batch Loss = 0.633137, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7306865453720093, Accuracy = 0.532608687877655
Training iter #42900:   Batch Loss = 0.613976, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7339185476303101, Accuracy = 0.54347825050354
Training iter #43000:   Batch Loss = 0.635863, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7460469603538513, Accuracy = 0.46739131212234497
Training iter #43100:   Batch Loss = 0.680338, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7348307967185974, Accuracy = 0.54347825050354
Training iter #43200:   Batch Loss = 0.651149, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.740502655506134, Accuracy = 0.46739131212234497
Training iter #43300:   Batch Loss = 0.657896, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7433667778968811, Accuracy = 0.47826087474823
Training iter #43400:   Batch Loss = 0.660294, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7353830933570862, Accuracy = 0.532608687877655
Training iter #43500:   Batch Loss = 0.639307, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7410901188850403, Accuracy = 0.489130437374115
Training iter #43600:   Batch Loss = 0.640012, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7305454611778259, Accuracy = 0.554347813129425
Training iter #43700:   Batch Loss = 0.606110, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7305782437324524, Accuracy = 0.54347825050354
Training iter #43800:   Batch Loss = 0.626497, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7436195015907288, Accuracy = 0.489130437374115
Training iter #43900:   Batch Loss = 0.690108, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7370654344558716, Accuracy = 0.532608687877655
Training iter #44000:   Batch Loss = 0.638116, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7475478053092957, Accuracy = 0.46739131212234497
Training iter #44100:   Batch Loss = 0.656527, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7424845695495605, Accuracy = 0.47826087474823
Training iter #44200:   Batch Loss = 0.667624, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7315660715103149, Accuracy = 0.54347825050354
Training iter #44300:   Batch Loss = 0.622646, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7365354299545288, Accuracy = 0.510869562625885
Training iter #44400:   Batch Loss = 0.642677, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7329974174499512, Accuracy = 0.54347825050354
Training iter #44500:   Batch Loss = 0.605045, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7359856367111206, Accuracy = 0.52173912525177
Training iter #44600:   Batch Loss = 0.642169, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7403011918067932, Accuracy = 0.47826087474823
Training iter #44700:   Batch Loss = 0.702418, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7291529774665833, Accuracy = 0.54347825050354
Training iter #44800:   Batch Loss = 0.672920, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7372788190841675, Accuracy = 0.5
Training iter #44900:   Batch Loss = 0.651879, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7437546849250793, Accuracy = 0.46739131212234497
Training iter #45000:   Batch Loss = 0.688142, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7417845129966736, Accuracy = 0.47826087474823
Training iter #45100:   Batch Loss = 0.620054, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.751288115978241, Accuracy = 0.46739131212234497
Training iter #45200:   Batch Loss = 0.666152, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7399911880493164, Accuracy = 0.52173912525177
Training iter #45300:   Batch Loss = 0.608145, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.733456015586853, Accuracy = 0.52173912525177
Training iter #45400:   Batch Loss = 0.612898, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7360954284667969, Accuracy = 0.54347825050354
Training iter #45500:   Batch Loss = 0.702421, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7310900688171387, Accuracy = 0.554347813129425
Training iter #45600:   Batch Loss = 0.637829, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7411561012268066, Accuracy = 0.5
Training iter #45700:   Batch Loss = 0.671851, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7435100674629211, Accuracy = 0.489130437374115
Training iter #45800:   Batch Loss = 0.687248, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7325626611709595, Accuracy = 0.5
Training iter #45900:   Batch Loss = 0.615945, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7396525144577026, Accuracy = 0.47826087474823
Training iter #46000:   Batch Loss = 0.648460, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7345939874649048, Accuracy = 0.52173912525177
Training iter #46100:   Batch Loss = 0.602428, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7353232502937317, Accuracy = 0.54347825050354
Training iter #46200:   Batch Loss = 0.619786, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7447879910469055, Accuracy = 0.489130437374115
Training iter #46300:   Batch Loss = 0.700404, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.738651692867279, Accuracy = 0.52173912525177
Training iter #46400:   Batch Loss = 0.625706, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.748845100402832, Accuracy = 0.45652174949645996
Training iter #46500:   Batch Loss = 0.673006, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7393984198570251, Accuracy = 0.510869562625885
Training iter #46600:   Batch Loss = 0.687877, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7295472025871277, Accuracy = 0.532608687877655
Training iter #46700:   Batch Loss = 0.599565, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7352442145347595, Accuracy = 0.54347825050354
Training iter #46800:   Batch Loss = 0.634844, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.737287163734436, Accuracy = 0.52173912525177
Training iter #46900:   Batch Loss = 0.599290, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7467334866523743, Accuracy = 0.5
Training iter #47000:   Batch Loss = 0.629048, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7627055644989014, Accuracy = 0.46739131212234497
Training iter #47100:   Batch Loss = 0.710128, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7436081767082214, Accuracy = 0.510869562625885
Training iter #47200:   Batch Loss = 0.638774, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7457177639007568, Accuracy = 0.489130437374115
Training iter #47300:   Batch Loss = 0.633942, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7440820932388306, Accuracy = 0.510869562625885
Training iter #47400:   Batch Loss = 0.682644, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7394924759864807, Accuracy = 0.532608687877655
Training iter #47500:   Batch Loss = 0.591650, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7450302243232727, Accuracy = 0.47826087474823
Training iter #47600:   Batch Loss = 0.686036, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7378631234169006, Accuracy = 0.54347825050354
Training iter #47700:   Batch Loss = 0.606842, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7372170090675354, Accuracy = 0.52173912525177
Training iter #47800:   Batch Loss = 0.597277, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7445210218429565, Accuracy = 0.47826087474823
Training iter #47900:   Batch Loss = 0.714790, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7362421154975891, Accuracy = 0.532608687877655
Training iter #48000:   Batch Loss = 0.632681, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7452083230018616, Accuracy = 0.489130437374115
Training iter #48100:   Batch Loss = 0.623285, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7398998737335205, Accuracy = 0.5
Training iter #48200:   Batch Loss = 0.672190, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7316683530807495, Accuracy = 0.54347825050354
Training iter #48300:   Batch Loss = 0.587302, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7407568097114563, Accuracy = 0.5
Training iter #48400:   Batch Loss = 0.716580, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7354370355606079, Accuracy = 0.52173912525177
Training iter #48500:   Batch Loss = 0.576615, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.736409068107605, Accuracy = 0.510869562625885
Training iter #48600:   Batch Loss = 0.612730, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7407274842262268, Accuracy = 0.5
Training iter #48700:   Batch Loss = 0.726000, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7323930859565735, Accuracy = 0.54347825050354
Training iter #48800:   Batch Loss = 0.654472, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7421687245368958, Accuracy = 0.489130437374115
Training iter #48900:   Batch Loss = 0.618180, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7430140376091003, Accuracy = 0.489130437374115
Training iter #49000:   Batch Loss = 0.676031, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7393134832382202, Accuracy = 0.489130437374115
Training iter #49100:   Batch Loss = 0.585658, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7470471858978271, Accuracy = 0.47826087474823
Training iter #49200:   Batch Loss = 0.695996, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7380567193031311, Accuracy = 0.52173912525177
Training iter #49300:   Batch Loss = 0.590929, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7376031279563904, Accuracy = 0.52173912525177
Training iter #49400:   Batch Loss = 0.605546, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7449320554733276, Accuracy = 0.46739131212234497
Training iter #49500:   Batch Loss = 0.730815, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7372850775718689, Accuracy = 0.532608687877655
Training iter #49600:   Batch Loss = 0.643464, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7488855123519897, Accuracy = 0.489130437374115
Training iter #49700:   Batch Loss = 0.627673, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7428067922592163, Accuracy = 0.489130437374115
Training iter #49800:   Batch Loss = 0.669657, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7364080548286438, Accuracy = 0.54347825050354
Training iter #49900:   Batch Loss = 0.563508, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7463057041168213, Accuracy = 0.47826087474823
Training iter #50000:   Batch Loss = 0.701190, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7408230304718018, Accuracy = 0.5
Training iter #50100:   Batch Loss = 0.589190, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7411938309669495, Accuracy = 0.5
Training iter #50200:   Batch Loss = 0.601019, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7436909675598145, Accuracy = 0.489130437374115
Training iter #50300:   Batch Loss = 0.748444, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7335082292556763, Accuracy = 0.554347813129425
Training iter #50400:   Batch Loss = 0.651710, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.742290735244751, Accuracy = 0.47826087474823
Training iter #50500:   Batch Loss = 0.616587, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7429317831993103, Accuracy = 0.47826087474823
Training iter #50600:   Batch Loss = 0.666527, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7419775724411011, Accuracy = 0.489130437374115
Training iter #50700:   Batch Loss = 0.582204, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.753090500831604, Accuracy = 0.45652174949645996
Training iter #50800:   Batch Loss = 0.698186, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7400103807449341, Accuracy = 0.52173912525177
Training iter #50900:   Batch Loss = 0.612118, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7390241622924805, Accuracy = 0.52173912525177
Training iter #51000:   Batch Loss = 0.601148, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7452375292778015, Accuracy = 0.489130437374115
Training iter #51100:   Batch Loss = 0.733121, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7375569939613342, Accuracy = 0.532608687877655
Training iter #51200:   Batch Loss = 0.630045, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7464509606361389, Accuracy = 0.489130437374115
Training iter #51300:   Batch Loss = 0.598535, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7378612160682678, Accuracy = 0.532608687877655
Training iter #51400:   Batch Loss = 0.670508, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7363599538803101, Accuracy = 0.54347825050354
Training iter #51500:   Batch Loss = 0.564650, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7472864389419556, Accuracy = 0.45652174949645996
Training iter #51600:   Batch Loss = 0.699320, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7488020062446594, Accuracy = 0.46739131212234497
Training iter #51700:   Batch Loss = 0.578564, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7570754289627075, Accuracy = 0.46739131212234497
Training iter #51800:   Batch Loss = 0.624421, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7533149719238281, Accuracy = 0.44565218687057495
Training iter #51900:   Batch Loss = 0.725746, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7312498688697815, Accuracy = 0.5760869383811951
Training iter #52000:   Batch Loss = 0.665508, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7320693731307983, Accuracy = 0.532608687877655
Training iter #52100:   Batch Loss = 0.597130, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7325248122215271, Accuracy = 0.532608687877655
Training iter #52200:   Batch Loss = 0.668530, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7393238544464111, Accuracy = 0.54347825050354
Training iter #52300:   Batch Loss = 0.576999, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.76176917552948, Accuracy = 0.47826087474823
Training iter #52400:   Batch Loss = 0.681666, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.749679446220398, Accuracy = 0.47826087474823
Training iter #52500:   Batch Loss = 0.594207, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7445787787437439, Accuracy = 0.489130437374115
Training iter #52600:   Batch Loss = 0.625185, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7411379814147949, Accuracy = 0.47826087474823
Training iter #52700:   Batch Loss = 0.726104, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7350776791572571, Accuracy = 0.554347813129425
Training iter #52800:   Batch Loss = 0.647740, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7404083013534546, Accuracy = 0.489130437374115
Training iter #52900:   Batch Loss = 0.630937, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7407852411270142, Accuracy = 0.5
Training iter #53000:   Batch Loss = 0.635565, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7433724403381348, Accuracy = 0.489130437374115
Training iter #53100:   Batch Loss = 0.597788, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7549474835395813, Accuracy = 0.489130437374115
Training iter #53200:   Batch Loss = 0.697093, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7402524352073669, Accuracy = 0.5
Training iter #53300:   Batch Loss = 0.626791, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7389822602272034, Accuracy = 0.52173912525177
Training iter #53400:   Batch Loss = 0.627572, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7420437932014465, Accuracy = 0.47826087474823
Training iter #53500:   Batch Loss = 0.718619, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.738581120967865, Accuracy = 0.532608687877655
Training iter #53600:   Batch Loss = 0.618573, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.750842809677124, Accuracy = 0.47826087474823
Training iter #53700:   Batch Loss = 0.639487, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7453658580780029, Accuracy = 0.489130437374115
Training iter #53800:   Batch Loss = 0.632668, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7415177822113037, Accuracy = 0.5
Training iter #53900:   Batch Loss = 0.600810, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7481295466423035, Accuracy = 0.47826087474823
Training iter #54000:   Batch Loss = 0.686197, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7403770089149475, Accuracy = 0.532608687877655
Training iter #54100:   Batch Loss = 0.647635, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7443143725395203, Accuracy = 0.489130437374115
Training iter #54200:   Batch Loss = 0.622411, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7512466907501221, Accuracy = 0.46739131212234497
Training iter #54300:   Batch Loss = 0.675512, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7449108362197876, Accuracy = 0.510869562625885
Training iter #54400:   Batch Loss = 0.637490, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7502412796020508, Accuracy = 0.47826087474823
Training iter #54500:   Batch Loss = 0.626150, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.738926112651825, Accuracy = 0.54347825050354
Training iter #54600:   Batch Loss = 0.627213, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7340983152389526, Accuracy = 0.54347825050354
Training iter #54700:   Batch Loss = 0.610151, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7505207657814026, Accuracy = 0.46739131212234497
Training iter #54800:   Batch Loss = 0.674811, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7478206753730774, Accuracy = 0.47826087474823
Training iter #54900:   Batch Loss = 0.611003, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7528978586196899, Accuracy = 0.46739131212234497
Training iter #55000:   Batch Loss = 0.623390, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7506195902824402, Accuracy = 0.47826087474823
Training iter #55100:   Batch Loss = 0.687675, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7372477054595947, Accuracy = 0.54347825050354
Training iter #55200:   Batch Loss = 0.627594, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7411460280418396, Accuracy = 0.5
Training iter #55300:   Batch Loss = 0.603541, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7383006811141968, Accuracy = 0.54347825050354
Training iter #55400:   Batch Loss = 0.611746, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7413406372070312, Accuracy = 0.5
Training iter #55500:   Batch Loss = 0.638849, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7560218572616577, Accuracy = 0.489130437374115
Training iter #55600:   Batch Loss = 0.665638, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7458924651145935, Accuracy = 0.489130437374115
Training iter #55700:   Batch Loss = 0.637100, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7471349835395813, Accuracy = 0.5
Training iter #55800:   Batch Loss = 0.634846, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7485492825508118, Accuracy = 0.5
Training iter #55900:   Batch Loss = 0.668799, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7420863509178162, Accuracy = 0.52173912525177
Training iter #56000:   Batch Loss = 0.644121, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7479666471481323, Accuracy = 0.489130437374115
Training iter #56100:   Batch Loss = 0.619005, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7415810823440552, Accuracy = 0.52173912525177
Training iter #56200:   Batch Loss = 0.597796, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7420831322669983, Accuracy = 0.5
Training iter #56300:   Batch Loss = 0.622790, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7541443109512329, Accuracy = 0.47826087474823
Training iter #56400:   Batch Loss = 0.674702, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7456594109535217, Accuracy = 0.5
Training iter #56500:   Batch Loss = 0.629244, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.752028226852417, Accuracy = 0.46739131212234497
Training iter #56600:   Batch Loss = 0.649791, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7496849894523621, Accuracy = 0.46739131212234497
Training iter #56700:   Batch Loss = 0.648683, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7408282160758972, Accuracy = 0.532608687877655
Training iter #56800:   Batch Loss = 0.627297, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7459205389022827, Accuracy = 0.489130437374115
Training iter #56900:   Batch Loss = 0.618676, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7398934960365295, Accuracy = 0.532608687877655
Training iter #57000:   Batch Loss = 0.587419, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7420575618743896, Accuracy = 0.5
Training iter #57100:   Batch Loss = 0.623000, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7553311586380005, Accuracy = 0.46739131212234497
Training iter #57200:   Batch Loss = 0.684096, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7442055940628052, Accuracy = 0.510869562625885
Training iter #57300:   Batch Loss = 0.653060, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7518170475959778, Accuracy = 0.46739131212234497
Training iter #57400:   Batch Loss = 0.645854, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7504105567932129, Accuracy = 0.47826087474823
Training iter #57500:   Batch Loss = 0.655490, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7420637011528015, Accuracy = 0.510869562625885
Training iter #57600:   Batch Loss = 0.613234, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7456576228141785, Accuracy = 0.5
Training iter #57700:   Batch Loss = 0.626106, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7402186393737793, Accuracy = 0.54347825050354
Training iter #57800:   Batch Loss = 0.595199, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7425679564476013, Accuracy = 0.5
Training iter #57900:   Batch Loss = 0.626174, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.753208339214325, Accuracy = 0.489130437374115
Training iter #58000:   Batch Loss = 0.695136, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7441856265068054, Accuracy = 0.510869562625885
Training iter #58100:   Batch Loss = 0.649044, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7534753084182739, Accuracy = 0.46739131212234497
Training iter #58200:   Batch Loss = 0.650779, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7532581090927124, Accuracy = 0.46739131212234497
Training iter #58300:   Batch Loss = 0.675411, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7433121800422668, Accuracy = 0.5
Training iter #58400:   Batch Loss = 0.604573, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7470720410346985, Accuracy = 0.489130437374115
Training iter #58500:   Batch Loss = 0.632967, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7418378591537476, Accuracy = 0.52173912525177
Training iter #58600:   Batch Loss = 0.606127, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.742379367351532, Accuracy = 0.5
Training iter #58700:   Batch Loss = 0.608208, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7514628171920776, Accuracy = 0.47826087474823
Training iter #58800:   Batch Loss = 0.683695, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7456857562065125, Accuracy = 0.5
Training iter #58900:   Batch Loss = 0.638701, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7555809617042542, Accuracy = 0.47826087474823
Training iter #59000:   Batch Loss = 0.666528, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7524204850196838, Accuracy = 0.46739131212234497
Training iter #59100:   Batch Loss = 0.681241, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7424372434616089, Accuracy = 0.5
Training iter #59200:   Batch Loss = 0.592955, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7466455101966858, Accuracy = 0.489130437374115
Training iter #59300:   Batch Loss = 0.631885, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7435570955276489, Accuracy = 0.5
Training iter #59400:   Batch Loss = 0.577868, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7443826198577881, Accuracy = 0.5
Training iter #59500:   Batch Loss = 0.597092, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7510437965393066, Accuracy = 0.489130437374115
Training iter #59600:   Batch Loss = 0.704744, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7433443665504456, Accuracy = 0.5
Training iter #59700:   Batch Loss = 0.659007, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7545664310455322, Accuracy = 0.47826087474823
Training iter #59800:   Batch Loss = 0.650321, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7538506984710693, Accuracy = 0.47826087474823
Training iter #59900:   Batch Loss = 0.670488, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7413491010665894, Accuracy = 0.5
Training iter #60000:   Batch Loss = 0.589453, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7431088089942932, Accuracy = 0.489130437374115
Training iter #60100:   Batch Loss = 0.633474, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7393938302993774, Accuracy = 0.532608687877655
Training iter #60200:   Batch Loss = 0.597820, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7417933344841003, Accuracy = 0.489130437374115
Training iter #60300:   Batch Loss = 0.613931, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7548683285713196, Accuracy = 0.489130437374115
Training iter #60400:   Batch Loss = 0.709460, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7452546954154968, Accuracy = 0.5
Training iter #60500:   Batch Loss = 0.617484, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.750967264175415, Accuracy = 0.47826087474823
Training iter #60600:   Batch Loss = 0.620802, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7479355931282043, Accuracy = 0.5
Training iter #60700:   Batch Loss = 0.663293, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7436925172805786, Accuracy = 0.5
Training iter #60800:   Batch Loss = 0.564503, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7511957287788391, Accuracy = 0.47826087474823
Training iter #60900:   Batch Loss = 0.685227, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7486001253128052, Accuracy = 0.489130437374115
Training iter #61000:   Batch Loss = 0.586973, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7488987445831299, Accuracy = 0.47826087474823
Training iter #61100:   Batch Loss = 0.586832, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7520930767059326, Accuracy = 0.489130437374115
Training iter #61200:   Batch Loss = 0.723780, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7413740158081055, Accuracy = 0.52173912525177
Training iter #61300:   Batch Loss = 0.644637, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7475773692131042, Accuracy = 0.47826087474823
Training iter #61400:   Batch Loss = 0.599586, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7483882904052734, Accuracy = 0.489130437374115
Training iter #61500:   Batch Loss = 0.666388, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7443822026252747, Accuracy = 0.5
Training iter #61600:   Batch Loss = 0.586933, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7530795931816101, Accuracy = 0.489130437374115
Training iter #61700:   Batch Loss = 0.714588, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7436695098876953, Accuracy = 0.5
Training iter #61800:   Batch Loss = 0.575904, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7421361207962036, Accuracy = 0.5
Training iter #61900:   Batch Loss = 0.602261, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7462693452835083, Accuracy = 0.5
Training iter #62000:   Batch Loss = 0.721942, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7406802177429199, Accuracy = 0.5
Training iter #62100:   Batch Loss = 0.646774, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.749772846698761, Accuracy = 0.510869562625885
Training iter #62200:   Batch Loss = 0.612935, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7469794750213623, Accuracy = 0.5
Training iter #62300:   Batch Loss = 0.667095, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7411371469497681, Accuracy = 0.5
Training iter #62400:   Batch Loss = 0.573520, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7478715181350708, Accuracy = 0.46739131212234497
Training iter #62500:   Batch Loss = 0.684834, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7446372509002686, Accuracy = 0.510869562625885
Training iter #62600:   Batch Loss = 0.566114, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7457319498062134, Accuracy = 0.5
Training iter #62700:   Batch Loss = 0.596194, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7513708472251892, Accuracy = 0.489130437374115
Training iter #62800:   Batch Loss = 0.728190, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7414954900741577, Accuracy = 0.5
Training iter #62900:   Batch Loss = 0.648786, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7479971647262573, Accuracy = 0.489130437374115
Training iter #63000:   Batch Loss = 0.603206, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7456034421920776, Accuracy = 0.5
Training iter #63100:   Batch Loss = 0.657568, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7429013848304749, Accuracy = 0.47826087474823
Training iter #63200:   Batch Loss = 0.560214, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7519313097000122, Accuracy = 0.510869562625885
Training iter #63300:   Batch Loss = 0.696654, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7452664375305176, Accuracy = 0.47826087474823
Training iter #63400:   Batch Loss = 0.586828, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7436321377754211, Accuracy = 0.5
Training iter #63500:   Batch Loss = 0.591739, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7447990775108337, Accuracy = 0.5
Training iter #63600:   Batch Loss = 0.745246, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7379193902015686, Accuracy = 0.532608687877655
Training iter #63700:   Batch Loss = 0.646842, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7445569038391113, Accuracy = 0.5
Training iter #63800:   Batch Loss = 0.613885, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7431427240371704, Accuracy = 0.5
Training iter #63900:   Batch Loss = 0.658173, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7406202554702759, Accuracy = 0.47826087474823
Training iter #64000:   Batch Loss = 0.560895, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7517305016517639, Accuracy = 0.510869562625885
Training iter #64100:   Batch Loss = 0.690902, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7446635365486145, Accuracy = 0.5
Training iter #64200:   Batch Loss = 0.592421, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7449818253517151, Accuracy = 0.489130437374115
Training iter #64300:   Batch Loss = 0.587703, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7478083372116089, Accuracy = 0.5
Training iter #64400:   Batch Loss = 0.731290, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7404080629348755, Accuracy = 0.52173912525177
Training iter #64500:   Batch Loss = 0.641781, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7473018765449524, Accuracy = 0.5
Training iter #64600:   Batch Loss = 0.592723, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7432199716567993, Accuracy = 0.47826087474823
Training iter #64700:   Batch Loss = 0.648069, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7405379414558411, Accuracy = 0.47826087474823
Training iter #64800:   Batch Loss = 0.558894, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7525419592857361, Accuracy = 0.52173912525177
Training iter #64900:   Batch Loss = 0.677572, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.74253249168396, Accuracy = 0.489130437374115
Training iter #65000:   Batch Loss = 0.589638, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7408180832862854, Accuracy = 0.47826087474823
Training iter #65100:   Batch Loss = 0.599648, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7439562678337097, Accuracy = 0.5
Training iter #65200:   Batch Loss = 0.705546, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7378443479537964, Accuracy = 0.5
Training iter #65300:   Batch Loss = 0.636049, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7422285079956055, Accuracy = 0.532608687877655
Training iter #65400:   Batch Loss = 0.600739, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7398059368133545, Accuracy = 0.47826087474823
Training iter #65500:   Batch Loss = 0.649165, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7387018799781799, Accuracy = 0.47826087474823
Training iter #65600:   Batch Loss = 0.560522, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7511187791824341, Accuracy = 0.52173912525177
Training iter #65700:   Batch Loss = 0.668808, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7393569946289062, Accuracy = 0.5
Training iter #65800:   Batch Loss = 0.613494, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.736757218837738, Accuracy = 0.47826087474823
Training iter #65900:   Batch Loss = 0.611487, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7402770519256592, Accuracy = 0.510869562625885
Training iter #66000:   Batch Loss = 0.713788, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7353774905204773, Accuracy = 0.489130437374115
Training iter #66100:   Batch Loss = 0.635722, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7421635985374451, Accuracy = 0.52173912525177
Training iter #66200:   Batch Loss = 0.629417, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7371086478233337, Accuracy = 0.489130437374115
Training iter #66300:   Batch Loss = 0.618548, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7344141602516174, Accuracy = 0.510869562625885
Training iter #66400:   Batch Loss = 0.575527, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7433040738105774, Accuracy = 0.532608687877655
Training iter #66500:   Batch Loss = 0.686920, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7381291389465332, Accuracy = 0.47826087474823
Training iter #66600:   Batch Loss = 0.618730, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7373687028884888, Accuracy = 0.54347825050354
Training iter #66700:   Batch Loss = 0.604285, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7401279211044312, Accuracy = 0.532608687877655
Training iter #66800:   Batch Loss = 0.704964, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7358244061470032, Accuracy = 0.510869562625885
Training iter #66900:   Batch Loss = 0.623417, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7375311255455017, Accuracy = 0.532608687877655
Training iter #67000:   Batch Loss = 0.601580, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7356391549110413, Accuracy = 0.47826087474823
Training iter #67100:   Batch Loss = 0.623919, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7316499352455139, Accuracy = 0.5
Training iter #67200:   Batch Loss = 0.595637, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.743633508682251, Accuracy = 0.510869562625885
Training iter #67300:   Batch Loss = 0.676711, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7415909767150879, Accuracy = 0.5
Training iter #67400:   Batch Loss = 0.633589, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7454847693443298, Accuracy = 0.489130437374115
Training iter #67500:   Batch Loss = 0.606410, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7484209537506104, Accuracy = 0.489130437374115
Training iter #67600:   Batch Loss = 0.668017, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7434431314468384, Accuracy = 0.510869562625885
Training iter #67700:   Batch Loss = 0.626800, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7505888342857361, Accuracy = 0.52173912525177
Training iter #67800:   Batch Loss = 0.625984, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7437807321548462, Accuracy = 0.47826087474823
Training iter #67900:   Batch Loss = 0.613440, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7395836710929871, Accuracy = 0.489130437374115
Training iter #68000:   Batch Loss = 0.613649, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7465986013412476, Accuracy = 0.532608687877655
Training iter #68100:   Batch Loss = 0.664178, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7385779619216919, Accuracy = 0.47826087474823
Training iter #68200:   Batch Loss = 0.637982, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.737597644329071, Accuracy = 0.52173912525177
Training iter #68300:   Batch Loss = 0.607649, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7410781383514404, Accuracy = 0.5
Training iter #68400:   Batch Loss = 0.670795, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7322214841842651, Accuracy = 0.510869562625885
Training iter #68500:   Batch Loss = 0.626654, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7332478165626526, Accuracy = 0.554347813129425
Training iter #68600:   Batch Loss = 0.598850, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7337758541107178, Accuracy = 0.554347813129425
Training iter #68700:   Batch Loss = 0.610609, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7371291518211365, Accuracy = 0.5652173757553101
Training iter #68800:   Batch Loss = 0.617622, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7465810775756836, Accuracy = 0.52173912525177
Training iter #68900:   Batch Loss = 0.652468, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.747484564781189, Accuracy = 0.52173912525177
Training iter #69000:   Batch Loss = 0.623730, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7553461194038391, Accuracy = 0.52173912525177
Training iter #69100:   Batch Loss = 0.630102, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7518858909606934, Accuracy = 0.532608687877655
Training iter #69200:   Batch Loss = 0.659806, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7398610711097717, Accuracy = 0.510869562625885
Training iter #69300:   Batch Loss = 0.638265, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.739086389541626, Accuracy = 0.5
Training iter #69400:   Batch Loss = 0.601261, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7328178882598877, Accuracy = 0.52173912525177
Training iter #69500:   Batch Loss = 0.579403, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7141726613044739, Accuracy = 0.5652173757553101
Training iter #69600:   Batch Loss = 0.654994, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7246861457824707, Accuracy = 0.5652173757553101
Training iter #69700:   Batch Loss = 0.694789, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7496282458305359, Accuracy = 0.5978260636329651
Training iter #69800:   Batch Loss = 0.679893, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7501377463340759, Accuracy = 0.54347825050354
Training iter #69900:   Batch Loss = 0.657101, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7566072940826416, Accuracy = 0.532608687877655
Training iter #70000:   Batch Loss = 0.667069, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7634541988372803, Accuracy = 0.46739131212234497
Training iter #70100:   Batch Loss = 0.659677, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7821235656738281, Accuracy = 0.46739131212234497
Training iter #70200:   Batch Loss = 0.676134, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.772782027721405, Accuracy = 0.45652174949645996
Training iter #70300:   Batch Loss = 0.576617, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7653911113739014, Accuracy = 0.46739131212234497
Training iter #70400:   Batch Loss = 0.654253, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7640196084976196, Accuracy = 0.489130437374115
Training iter #70500:   Batch Loss = 0.680385, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7564803957939148, Accuracy = 0.52173912525177
Training iter #70600:   Batch Loss = 0.674599, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.759719729423523, Accuracy = 0.532608687877655
Training iter #70700:   Batch Loss = 0.639989, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7607520818710327, Accuracy = 0.532608687877655
Training iter #70800:   Batch Loss = 0.675187, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7587254047393799, Accuracy = 0.52173912525177
Training iter #70900:   Batch Loss = 0.650273, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7682193517684937, Accuracy = 0.44565218687057495
Training iter #71000:   Batch Loss = 0.632638, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7630236148834229, Accuracy = 0.489130437374115
Training iter #71100:   Batch Loss = 0.575351, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7626053094863892, Accuracy = 0.47826087474823
Training iter #71200:   Batch Loss = 0.674250, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7680509090423584, Accuracy = 0.45652174949645996
Training iter #71300:   Batch Loss = 0.695556, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7559206485748291, Accuracy = 0.52173912525177
Training iter #71400:   Batch Loss = 0.649500, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7643494606018066, Accuracy = 0.47826087474823
Training iter #71500:   Batch Loss = 0.646504, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7668589353561401, Accuracy = 0.47826087474823
Training iter #71600:   Batch Loss = 0.694391, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7594585418701172, Accuracy = 0.5
Training iter #71700:   Batch Loss = 0.634661, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7638363838195801, Accuracy = 0.5
Training iter #71800:   Batch Loss = 0.629016, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7564118504524231, Accuracy = 0.532608687877655
Training iter #71900:   Batch Loss = 0.596621, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7566037178039551, Accuracy = 0.532608687877655
Training iter #72000:   Batch Loss = 0.640575, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7648040056228638, Accuracy = 0.5
Training iter #72100:   Batch Loss = 0.687370, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7637559771537781, Accuracy = 0.5
Training iter #72200:   Batch Loss = 0.622178, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7757353782653809, Accuracy = 0.45652174949645996
Training iter #72300:   Batch Loss = 0.661306, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7744587659835815, Accuracy = 0.46739131212234497
Training iter #72400:   Batch Loss = 0.692871, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7652879953384399, Accuracy = 0.510869562625885
Training iter #72500:   Batch Loss = 0.621550, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7678816318511963, Accuracy = 0.489130437374115
Training iter #72600:   Batch Loss = 0.624084, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7608765959739685, Accuracy = 0.510869562625885
Training iter #72700:   Batch Loss = 0.573911, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7594777345657349, Accuracy = 0.52173912525177
Training iter #72800:   Batch Loss = 0.633782, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7645857334136963, Accuracy = 0.5
Training iter #72900:   Batch Loss = 0.705203, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7609909176826477, Accuracy = 0.510869562625885
Training iter #73000:   Batch Loss = 0.632979, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7722404599189758, Accuracy = 0.45652174949645996
Training iter #73100:   Batch Loss = 0.632335, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.774876594543457, Accuracy = 0.45652174949645996
Training iter #73200:   Batch Loss = 0.678841, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7661144137382507, Accuracy = 0.489130437374115
Training iter #73300:   Batch Loss = 0.617359, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7672111392021179, Accuracy = 0.47826087474823
Training iter #73400:   Batch Loss = 0.621367, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7580999135971069, Accuracy = 0.532608687877655
Training iter #73500:   Batch Loss = 0.603952, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7572972178459167, Accuracy = 0.532608687877655
Training iter #73600:   Batch Loss = 0.614439, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.764830470085144, Accuracy = 0.47826087474823
Training iter #73700:   Batch Loss = 0.714035, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7604184150695801, Accuracy = 0.489130437374115
Training iter #73800:   Batch Loss = 0.611109, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7716633677482605, Accuracy = 0.46739131212234497
Training iter #73900:   Batch Loss = 0.628274, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7692455649375916, Accuracy = 0.45652174949645996
Training iter #74000:   Batch Loss = 0.670894, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7600480318069458, Accuracy = 0.5
Training iter #74100:   Batch Loss = 0.595443, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7625098824501038, Accuracy = 0.489130437374115
Training iter #74200:   Batch Loss = 0.666939, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7564345598220825, Accuracy = 0.510869562625885
Training iter #74300:   Batch Loss = 0.594633, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7569584250450134, Accuracy = 0.489130437374115
Training iter #74400:   Batch Loss = 0.588066, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7651821374893188, Accuracy = 0.47826087474823
Training iter #74500:   Batch Loss = 0.702568, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7587057948112488, Accuracy = 0.489130437374115
Training iter #74600:   Batch Loss = 0.638993, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7695108652114868, Accuracy = 0.47826087474823
Training iter #74700:   Batch Loss = 0.597215, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7686688899993896, Accuracy = 0.45652174949645996
Training iter #74800:   Batch Loss = 0.679376, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.760306179523468, Accuracy = 0.489130437374115
Training iter #74900:   Batch Loss = 0.612620, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.763339638710022, Accuracy = 0.47826087474823
Training iter #75000:   Batch Loss = 0.680255, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.755687952041626, Accuracy = 0.532608687877655
Training iter #75100:   Batch Loss = 0.566413, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.757051944732666, Accuracy = 0.5
Training iter #75200:   Batch Loss = 0.607429, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7641136646270752, Accuracy = 0.46739131212234497
Training iter #75300:   Batch Loss = 0.704008, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7587005496025085, Accuracy = 0.489130437374115
Training iter #75400:   Batch Loss = 0.629531, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7693793773651123, Accuracy = 0.47826087474823
Training iter #75500:   Batch Loss = 0.590962, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.765849232673645, Accuracy = 0.46739131212234497
Training iter #75600:   Batch Loss = 0.675536, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7600884437561035, Accuracy = 0.489130437374115
Training iter #75700:   Batch Loss = 0.601335, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7653734087944031, Accuracy = 0.46739131212234497
Training iter #75800:   Batch Loss = 0.680750, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.757737934589386, Accuracy = 0.489130437374115
Training iter #75900:   Batch Loss = 0.559124, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7572824954986572, Accuracy = 0.489130437374115
Training iter #76000:   Batch Loss = 0.599114, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7634270191192627, Accuracy = 0.46739131212234497
Training iter #76100:   Batch Loss = 0.717832, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7574270963668823, Accuracy = 0.5
Training iter #76200:   Batch Loss = 0.630430, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7676377296447754, Accuracy = 0.46739131212234497
Training iter #76300:   Batch Loss = 0.609825, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7616955637931824, Accuracy = 0.47826087474823
Training iter #76400:   Batch Loss = 0.669615, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7566758990287781, Accuracy = 0.5
Training iter #76500:   Batch Loss = 0.582347, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7639966011047363, Accuracy = 0.46739131212234497
Training iter #76600:   Batch Loss = 0.690238, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7570740580558777, Accuracy = 0.5
Training iter #76700:   Batch Loss = 0.573781, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7547730207443237, Accuracy = 0.489130437374115
Training iter #76800:   Batch Loss = 0.596496, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.757167398929596, Accuracy = 0.489130437374115
Training iter #76900:   Batch Loss = 0.746900, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7508006691932678, Accuracy = 0.532608687877655
Training iter #77000:   Batch Loss = 0.655165, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7618110775947571, Accuracy = 0.44565218687057495
Training iter #77100:   Batch Loss = 0.599828, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7654227018356323, Accuracy = 0.44565218687057495
Training iter #77200:   Batch Loss = 0.664943, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7645432949066162, Accuracy = 0.44565218687057495
Training iter #77300:   Batch Loss = 0.607313, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7736254930496216, Accuracy = 0.47826087474823
Training iter #77400:   Batch Loss = 0.683425, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7596786618232727, Accuracy = 0.510869562625885
Training iter #77500:   Batch Loss = 0.586903, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7577137351036072, Accuracy = 0.489130437374115
Training iter #77600:   Batch Loss = 0.589545, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7597450017929077, Accuracy = 0.5
Training iter #77700:   Batch Loss = 0.737225, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7567906379699707, Accuracy = 0.52173912525177
Training iter #77800:   Batch Loss = 0.629038, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7690956592559814, Accuracy = 0.47826087474823
Training iter #77900:   Batch Loss = 0.576905, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.765885055065155, Accuracy = 0.47826087474823
Training iter #78000:   Batch Loss = 0.648761, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7641926407814026, Accuracy = 0.47826087474823
Training iter #78100:   Batch Loss = 0.592498, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7752763628959656, Accuracy = 0.489130437374115
Training iter #78200:   Batch Loss = 0.668841, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7616178393363953, Accuracy = 0.489130437374115
Training iter #78300:   Batch Loss = 0.573969, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7590839862823486, Accuracy = 0.489130437374115
Training iter #78400:   Batch Loss = 0.602836, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7605342268943787, Accuracy = 0.489130437374115
Training iter #78500:   Batch Loss = 0.701756, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7545157670974731, Accuracy = 0.52173912525177
Training iter #78600:   Batch Loss = 0.636708, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7627999782562256, Accuracy = 0.46739131212234497
Training iter #78700:   Batch Loss = 0.589371, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7623565793037415, Accuracy = 0.47826087474823
Training iter #78800:   Batch Loss = 0.649424, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7631082534790039, Accuracy = 0.46739131212234497
Training iter #78900:   Batch Loss = 0.589995, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7720534205436707, Accuracy = 0.47826087474823
Training iter #79000:   Batch Loss = 0.650565, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7599347233772278, Accuracy = 0.489130437374115
Training iter #79100:   Batch Loss = 0.622845, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7605858445167542, Accuracy = 0.489130437374115
Training iter #79200:   Batch Loss = 0.605662, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7638261318206787, Accuracy = 0.46739131212234497
Training iter #79300:   Batch Loss = 0.705043, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7570763230323792, Accuracy = 0.47826087474823
Training iter #79400:   Batch Loss = 0.626575, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7625895738601685, Accuracy = 0.46739131212234497
Training iter #79500:   Batch Loss = 0.598376, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.757585346698761, Accuracy = 0.489130437374115
Training iter #79600:   Batch Loss = 0.627007, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7576955556869507, Accuracy = 0.489130437374115
Training iter #79700:   Batch Loss = 0.603714, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7733577489852905, Accuracy = 0.489130437374115
Training iter #79800:   Batch Loss = 0.673365, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7648360133171082, Accuracy = 0.46739131212234497
Training iter #79900:   Batch Loss = 0.593702, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7648668885231018, Accuracy = 0.46739131212234497
Training iter #80000:   Batch Loss = 0.602593, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7621464133262634, Accuracy = 0.47826087474823
Training iter #80100:   Batch Loss = 0.710876, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.755333662033081, Accuracy = 0.52173912525177
Training iter #80200:   Batch Loss = 0.638867, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7618187665939331, Accuracy = 0.47826087474823
Training iter #80300:   Batch Loss = 0.618358, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7575474977493286, Accuracy = 0.5
Training iter #80400:   Batch Loss = 0.621575, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7565151453018188, Accuracy = 0.489130437374115
Training iter #80500:   Batch Loss = 0.627668, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7654985785484314, Accuracy = 0.47826087474823
Training iter #80600:   Batch Loss = 0.666119, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7592521905899048, Accuracy = 0.489130437374115
Training iter #80700:   Batch Loss = 0.637040, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7650923728942871, Accuracy = 0.46739131212234497
Training iter #80800:   Batch Loss = 0.610442, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7692421078681946, Accuracy = 0.47826087474823
Training iter #80900:   Batch Loss = 0.666413, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7591989636421204, Accuracy = 0.489130437374115
Training iter #81000:   Batch Loss = 0.658737, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7628045082092285, Accuracy = 0.47826087474823
Optimization Finished!
FINAL RESULT: Batch Loss = 0.7628045082092285, Accuracy = 0.47826087474823
