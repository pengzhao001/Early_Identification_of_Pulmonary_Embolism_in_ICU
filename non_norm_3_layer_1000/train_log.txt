Training iter #50:   Batch Loss = 0.787423, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7130400538444519, Accuracy = 0.6304348111152649
Training iter #100:   Batch Loss = 0.785128, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7185834646224976, Accuracy = 0.6304348111152649
Training iter #200:   Batch Loss = 0.746478, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7865121364593506, Accuracy = 0.3913043439388275
Training iter #300:   Batch Loss = 0.761414, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7817221879959106, Accuracy = 0.3695652186870575
Training iter #400:   Batch Loss = 0.733212, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7676061391830444, Accuracy = 0.3695652186870575
Training iter #500:   Batch Loss = 0.745573, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7525683641433716, Accuracy = 0.41304346919059753
Training iter #600:   Batch Loss = 0.745733, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7343582510948181, Accuracy = 0.6304348111152649
Training iter #700:   Batch Loss = 0.757863, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7317062616348267, Accuracy = 0.6304348111152649
Training iter #800:   Batch Loss = 0.731337, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7293863892555237, Accuracy = 0.6304348111152649
Training iter #900:   Batch Loss = 0.737723, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7271912097930908, Accuracy = 0.6304348111152649
Training iter #1000:   Batch Loss = 0.728916, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7349424958229065, Accuracy = 0.532608687877655
Training iter #1100:   Batch Loss = 0.737209, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7398730516433716, Accuracy = 0.41304346919059753
Training iter #1200:   Batch Loss = 0.743862, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7523770332336426, Accuracy = 0.43478259444236755
Training iter #1300:   Batch Loss = 0.732487, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.756633996963501, Accuracy = 0.43478259444236755
Training iter #1400:   Batch Loss = 0.734552, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7454017996788025, Accuracy = 0.43478259444236755
Training iter #1500:   Batch Loss = 0.733443, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7479698657989502, Accuracy = 0.42391303181648254
Training iter #1600:   Batch Loss = 0.739272, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7456752061843872, Accuracy = 0.41304346919059753
Training iter #1700:   Batch Loss = 0.725735, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7431967854499817, Accuracy = 0.45652174949645996
Training iter #1800:   Batch Loss = 0.724697, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7509994506835938, Accuracy = 0.41304346919059753
Training iter #1900:   Batch Loss = 0.729324, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7566959261894226, Accuracy = 0.41304346919059753
Training iter #2000:   Batch Loss = 0.734845, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7724094390869141, Accuracy = 0.42391303181648254
Training iter #2100:   Batch Loss = 0.731178, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.771964967250824, Accuracy = 0.43478259444236755
Training iter #2200:   Batch Loss = 0.716483, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7519049644470215, Accuracy = 0.45652174949645996
Training iter #2300:   Batch Loss = 0.734491, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7513852715492249, Accuracy = 0.44565218687057495
Training iter #2400:   Batch Loss = 0.742465, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7495744824409485, Accuracy = 0.44565218687057495
Training iter #2500:   Batch Loss = 0.732443, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7492238879203796, Accuracy = 0.44565218687057495
Training iter #2600:   Batch Loss = 0.716278, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7585440278053284, Accuracy = 0.42391303181648254
Training iter #2700:   Batch Loss = 0.728687, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7638211250305176, Accuracy = 0.41304346919059753
Training iter #2800:   Batch Loss = 0.729937, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7738558053970337, Accuracy = 0.3913043439388275
Training iter #2900:   Batch Loss = 0.732949, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7717782855033875, Accuracy = 0.41304346919059753
Training iter #3000:   Batch Loss = 0.714705, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7623682022094727, Accuracy = 0.45652174949645996
Training iter #3100:   Batch Loss = 0.722320, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7649966478347778, Accuracy = 0.47826087474823
Training iter #3200:   Batch Loss = 0.742210, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7621976733207703, Accuracy = 0.46739131212234497
Training iter #3300:   Batch Loss = 0.717333, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7584097385406494, Accuracy = 0.5
Training iter #3400:   Batch Loss = 0.711145, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7668725252151489, Accuracy = 0.46739131212234497
Training iter #3500:   Batch Loss = 0.712790, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7678824663162231, Accuracy = 0.489130437374115
Training iter #3600:   Batch Loss = 0.727858, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7810901999473572, Accuracy = 0.46739131212234497
Training iter #3700:   Batch Loss = 0.750353, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7791177034378052, Accuracy = 0.5
Training iter #3800:   Batch Loss = 0.706750, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7739297747612, Accuracy = 0.47826087474823
Training iter #3900:   Batch Loss = 0.717201, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7795835733413696, Accuracy = 0.46739131212234497
Training iter #4000:   Batch Loss = 0.742930, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.771625816822052, Accuracy = 0.47826087474823
Training iter #4100:   Batch Loss = 0.699669, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7607392072677612, Accuracy = 0.489130437374115
Training iter #4200:   Batch Loss = 0.698243, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7730937004089355, Accuracy = 0.47826087474823
Training iter #4300:   Batch Loss = 0.696688, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7737024426460266, Accuracy = 0.46739131212234497
Training iter #4400:   Batch Loss = 0.724247, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7839938998222351, Accuracy = 0.43478259444236755
Training iter #4500:   Batch Loss = 0.759756, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7751925587654114, Accuracy = 0.46739131212234497
Training iter #4600:   Batch Loss = 0.678757, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7611333727836609, Accuracy = 0.5
Training iter #4700:   Batch Loss = 0.715544, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7798848748207092, Accuracy = 0.46739131212234497
Training iter #4800:   Batch Loss = 0.761897, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7804450988769531, Accuracy = 0.46739131212234497
Training iter #4900:   Batch Loss = 0.692939, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7697829604148865, Accuracy = 0.489130437374115
Training iter #5000:   Batch Loss = 0.699245, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7853434085845947, Accuracy = 0.47826087474823
Training iter #5100:   Batch Loss = 0.703258, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7767310738563538, Accuracy = 0.489130437374115
Training iter #5200:   Batch Loss = 0.708931, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7860339879989624, Accuracy = 0.46739131212234497
Training iter #5300:   Batch Loss = 0.777093, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7877944111824036, Accuracy = 0.46739131212234497
Training iter #5400:   Batch Loss = 0.645777, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7721151113510132, Accuracy = 0.489130437374115
Training iter #5500:   Batch Loss = 0.709685, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.781198263168335, Accuracy = 0.45652174949645996
Training iter #5600:   Batch Loss = 0.743133, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7730705738067627, Accuracy = 0.489130437374115
Training iter #5700:   Batch Loss = 0.684351, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7680648565292358, Accuracy = 0.510869562625885
Training iter #5800:   Batch Loss = 0.687064, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7947505116462708, Accuracy = 0.46739131212234497
Training iter #5900:   Batch Loss = 0.684161, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7833113074302673, Accuracy = 0.489130437374115
Training iter #6000:   Batch Loss = 0.713571, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7870489954948425, Accuracy = 0.47826087474823
Training iter #6100:   Batch Loss = 0.776182, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.792199432849884, Accuracy = 0.47826087474823
Training iter #6200:   Batch Loss = 0.630305, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7750500440597534, Accuracy = 0.489130437374115
Training iter #6300:   Batch Loss = 0.712966, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7744596600532532, Accuracy = 0.489130437374115
Training iter #6400:   Batch Loss = 0.750213, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7669762372970581, Accuracy = 0.510869562625885
Training iter #6500:   Batch Loss = 0.682184, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7639281749725342, Accuracy = 0.510869562625885
Training iter #6600:   Batch Loss = 0.688995, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.790845513343811, Accuracy = 0.46739131212234497
Training iter #6700:   Batch Loss = 0.683411, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7800693511962891, Accuracy = 0.489130437374115
Training iter #6800:   Batch Loss = 0.700537, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7804909944534302, Accuracy = 0.489130437374115
Training iter #6900:   Batch Loss = 0.767347, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.784650444984436, Accuracy = 0.47826087474823
Training iter #7000:   Batch Loss = 0.633440, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7731257081031799, Accuracy = 0.489130437374115
Training iter #7100:   Batch Loss = 0.710157, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7770816087722778, Accuracy = 0.489130437374115
Training iter #7200:   Batch Loss = 0.732253, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.765798807144165, Accuracy = 0.510869562625885
Training iter #7300:   Batch Loss = 0.687675, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7620092630386353, Accuracy = 0.52173912525177
Training iter #7400:   Batch Loss = 0.695674, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7931527495384216, Accuracy = 0.44565218687057495
Training iter #7500:   Batch Loss = 0.675732, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7759005427360535, Accuracy = 0.489130437374115
Training iter #7600:   Batch Loss = 0.699697, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7755377888679504, Accuracy = 0.489130437374115
Training iter #7700:   Batch Loss = 0.790664, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7794556617736816, Accuracy = 0.489130437374115
Training iter #7800:   Batch Loss = 0.640348, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7758090496063232, Accuracy = 0.489130437374115
Training iter #7900:   Batch Loss = 0.701455, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7813788056373596, Accuracy = 0.489130437374115
Training iter #8000:   Batch Loss = 0.714794, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7678812742233276, Accuracy = 0.510869562625885
Training iter #8100:   Batch Loss = 0.697594, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7575221657752991, Accuracy = 0.52173912525177
Training iter #8200:   Batch Loss = 0.697042, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7860366702079773, Accuracy = 0.47826087474823
Training iter #8300:   Batch Loss = 0.670894, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7756600379943848, Accuracy = 0.5
Training iter #8400:   Batch Loss = 0.672730, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7788085341453552, Accuracy = 0.489130437374115
Training iter #8500:   Batch Loss = 0.784669, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7761752605438232, Accuracy = 0.510869562625885
Training iter #8600:   Batch Loss = 0.639420, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7722783088684082, Accuracy = 0.510869562625885
Training iter #8700:   Batch Loss = 0.707897, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7820707559585571, Accuracy = 0.5
Training iter #8800:   Batch Loss = 0.717512, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7733919620513916, Accuracy = 0.510869562625885
Training iter #8900:   Batch Loss = 0.693526, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7681516408920288, Accuracy = 0.52173912525177
Training iter #9000:   Batch Loss = 0.713655, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7930455803871155, Accuracy = 0.46739131212234497
Training iter #9100:   Batch Loss = 0.642919, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.780676007270813, Accuracy = 0.5
Training iter #9200:   Batch Loss = 0.660130, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7814311385154724, Accuracy = 0.5
Training iter #9300:   Batch Loss = 0.783084, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7771607637405396, Accuracy = 0.510869562625885
Training iter #9400:   Batch Loss = 0.626496, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7691801190376282, Accuracy = 0.52173912525177
Training iter #9500:   Batch Loss = 0.699558, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7833890318870544, Accuracy = 0.510869562625885
Training iter #9600:   Batch Loss = 0.704057, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.781488299369812, Accuracy = 0.5
Training iter #9700:   Batch Loss = 0.684576, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7790201902389526, Accuracy = 0.52173912525177
Training iter #9800:   Batch Loss = 0.726830, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7894266843795776, Accuracy = 0.46739131212234497
Training iter #9900:   Batch Loss = 0.637126, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.764994204044342, Accuracy = 0.532608687877655
Training iter #10000:   Batch Loss = 0.662543, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7761411666870117, Accuracy = 0.510869562625885
Training iter #10100:   Batch Loss = 0.784611, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.782398521900177, Accuracy = 0.5
Training iter #10200:   Batch Loss = 0.630467, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7803142070770264, Accuracy = 0.510869562625885
Training iter #10300:   Batch Loss = 0.691049, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7860047817230225, Accuracy = 0.5
Training iter #10400:   Batch Loss = 0.700749, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7751369476318359, Accuracy = 0.52173912525177
Training iter #10500:   Batch Loss = 0.674251, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7768888473510742, Accuracy = 0.52173912525177
Training iter #10600:   Batch Loss = 0.737755, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7905341982841492, Accuracy = 0.45652174949645996
Training iter #10700:   Batch Loss = 0.629671, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7643538117408752, Accuracy = 0.532608687877655
Training iter #10800:   Batch Loss = 0.661690, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7759649753570557, Accuracy = 0.52173912525177
Training iter #10900:   Batch Loss = 0.776129, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7768406271934509, Accuracy = 0.52173912525177
Training iter #11000:   Batch Loss = 0.638010, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7787516117095947, Accuracy = 0.5
Training iter #11100:   Batch Loss = 0.665742, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7801594734191895, Accuracy = 0.510869562625885
Training iter #11200:   Batch Loss = 0.712247, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7670117020606995, Accuracy = 0.52173912525177
Training iter #11300:   Batch Loss = 0.678940, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7790999412536621, Accuracy = 0.510869562625885
Training iter #11400:   Batch Loss = 0.720278, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7947371006011963, Accuracy = 0.46739131212234497
Training iter #11500:   Batch Loss = 0.630618, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7613562941551208, Accuracy = 0.54347825050354
Training iter #11600:   Batch Loss = 0.649188, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7758615016937256, Accuracy = 0.489130437374115
Training iter #11700:   Batch Loss = 0.766156, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7703298330307007, Accuracy = 0.52173912525177
Training iter #11800:   Batch Loss = 0.647407, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7707506418228149, Accuracy = 0.510869562625885
Training iter #11900:   Batch Loss = 0.659341, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7774074077606201, Accuracy = 0.510869562625885
Training iter #12000:   Batch Loss = 0.714041, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7632349729537964, Accuracy = 0.532608687877655
Training iter #12100:   Batch Loss = 0.698797, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.774350643157959, Accuracy = 0.510869562625885
Training iter #12200:   Batch Loss = 0.721344, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7960100173950195, Accuracy = 0.45652174949645996
Training iter #12300:   Batch Loss = 0.646077, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7599392533302307, Accuracy = 0.532608687877655
Training iter #12400:   Batch Loss = 0.672676, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7646468281745911, Accuracy = 0.532608687877655
Training iter #12500:   Batch Loss = 0.763835, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7711113095283508, Accuracy = 0.489130437374115
Training iter #12600:   Batch Loss = 0.659796, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7615051865577698, Accuracy = 0.54347825050354
Training iter #12700:   Batch Loss = 0.656401, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7708410024642944, Accuracy = 0.532608687877655
Training iter #12800:   Batch Loss = 0.693556, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7691634297370911, Accuracy = 0.52173912525177
Training iter #12900:   Batch Loss = 0.694513, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7730746865272522, Accuracy = 0.52173912525177
Training iter #13000:   Batch Loss = 0.700092, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7816612124443054, Accuracy = 0.510869562625885
Training iter #13100:   Batch Loss = 0.633399, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7608110904693604, Accuracy = 0.54347825050354
Training iter #13200:   Batch Loss = 0.678397, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7848096489906311, Accuracy = 0.47826087474823
Training iter #13300:   Batch Loss = 0.766039, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7717140316963196, Accuracy = 0.489130437374115
Training iter #13400:   Batch Loss = 0.658418, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7528747916221619, Accuracy = 0.54347825050354
Training iter #13500:   Batch Loss = 0.663087, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7715792059898376, Accuracy = 0.52173912525177
Training iter #13600:   Batch Loss = 0.679990, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7930847406387329, Accuracy = 0.47826087474823
Training iter #13700:   Batch Loss = 0.679490, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7591930031776428, Accuracy = 0.52173912525177
Training iter #13800:   Batch Loss = 0.719007, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7511482238769531, Accuracy = 0.54347825050354
Training iter #13900:   Batch Loss = 0.677186, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.804393470287323, Accuracy = 0.46739131212234497
Training iter #14000:   Batch Loss = 0.719115, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.8895753622055054, Accuracy = 0.3695652186870575
Training iter #14100:   Batch Loss = 0.839725, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.8072312474250793, Accuracy = 0.3695652186870575
Training iter #14200:   Batch Loss = 0.724117, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7369703054428101, Accuracy = 0.45652174949645996
Training iter #14300:   Batch Loss = 0.718012, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7141165137290955, Accuracy = 0.6413043737411499
Training iter #14400:   Batch Loss = 0.735615, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7059728503227234, Accuracy = 0.6304348111152649
Training iter #14500:   Batch Loss = 0.773788, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7071917653083801, Accuracy = 0.6304348111152649
Training iter #14600:   Batch Loss = 0.731505, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7121092081069946, Accuracy = 0.6413043737411499
Training iter #14700:   Batch Loss = 0.716470, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7194724082946777, Accuracy = 0.6521739363670349
Training iter #14800:   Batch Loss = 0.726291, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7347750067710876, Accuracy = 0.510869562625885
Training iter #14900:   Batch Loss = 0.736711, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7491958737373352, Accuracy = 0.3586956560611725
Training iter #15000:   Batch Loss = 0.717049, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7574092745780945, Accuracy = 0.30434781312942505
Training iter #15100:   Batch Loss = 0.726926, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7684077024459839, Accuracy = 0.3478260934352875
Training iter #15200:   Batch Loss = 0.707003, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7671865224838257, Accuracy = 0.3586956560611725
Training iter #15300:   Batch Loss = 0.690081, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7648680210113525, Accuracy = 0.32608696818351746
Training iter #15400:   Batch Loss = 0.728786, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7557166814804077, Accuracy = 0.4021739065647125
Training iter #15500:   Batch Loss = 0.699255, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7426398396492004, Accuracy = 0.489130437374115
Training iter #15600:   Batch Loss = 0.708204, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7409051656723022, Accuracy = 0.489130437374115
Training iter #15700:   Batch Loss = 0.729081, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7381339073181152, Accuracy = 0.52173912525177
Training iter #15800:   Batch Loss = 0.705623, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7399842739105225, Accuracy = 0.52173912525177
Training iter #15900:   Batch Loss = 0.697790, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7560572624206543, Accuracy = 0.489130437374115
Training iter #16000:   Batch Loss = 0.687404, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7646238803863525, Accuracy = 0.45652174949645996
Training iter #16100:   Batch Loss = 0.693447, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7802773714065552, Accuracy = 0.45652174949645996
Training iter #16200:   Batch Loss = 0.720829, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7725847959518433, Accuracy = 0.46739131212234497
Training iter #16300:   Batch Loss = 0.679209, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7538734674453735, Accuracy = 0.510869562625885
Training iter #16400:   Batch Loss = 0.692639, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7592201828956604, Accuracy = 0.489130437374115
Training iter #16500:   Batch Loss = 0.733116, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7595596313476562, Accuracy = 0.5
Training iter #16600:   Batch Loss = 0.685648, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7619150876998901, Accuracy = 0.510869562625885
Training iter #16700:   Batch Loss = 0.683105, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7885774374008179, Accuracy = 0.5
Training iter #16800:   Batch Loss = 0.670140, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7756396532058716, Accuracy = 0.489130437374115
Training iter #16900:   Batch Loss = 0.684950, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7791926860809326, Accuracy = 0.489130437374115
Training iter #17000:   Batch Loss = 0.744821, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7675747871398926, Accuracy = 0.510869562625885
Training iter #17100:   Batch Loss = 0.649865, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7595868110656738, Accuracy = 0.532608687877655
Training iter #17200:   Batch Loss = 0.682840, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7788371443748474, Accuracy = 0.510869562625885
Training iter #17300:   Batch Loss = 0.741567, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7788656949996948, Accuracy = 0.510869562625885
Training iter #17400:   Batch Loss = 0.678147, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7666079998016357, Accuracy = 0.52173912525177
Training iter #17500:   Batch Loss = 0.683845, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7851690649986267, Accuracy = 0.52173912525177
Training iter #17600:   Batch Loss = 0.668127, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7742384076118469, Accuracy = 0.52173912525177
Training iter #17700:   Batch Loss = 0.673504, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7796157002449036, Accuracy = 0.52173912525177
Training iter #17800:   Batch Loss = 0.764637, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7766976952552795, Accuracy = 0.52173912525177
Training iter #17900:   Batch Loss = 0.624159, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7633978128433228, Accuracy = 0.532608687877655
Training iter #18000:   Batch Loss = 0.674256, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.780647337436676, Accuracy = 0.510869562625885
Training iter #18100:   Batch Loss = 0.746139, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7691044807434082, Accuracy = 0.52173912525177
Training iter #18200:   Batch Loss = 0.686935, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7539767622947693, Accuracy = 0.554347813129425
Training iter #18300:   Batch Loss = 0.689820, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7720994353294373, Accuracy = 0.510869562625885
Training iter #18400:   Batch Loss = 0.670400, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7628270983695984, Accuracy = 0.532608687877655
Training iter #18500:   Batch Loss = 0.641523, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7777764797210693, Accuracy = 0.510869562625885
Training iter #18600:   Batch Loss = 0.767936, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.755491316318512, Accuracy = 0.54347825050354
Training iter #18700:   Batch Loss = 0.605683, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.749057412147522, Accuracy = 0.54347825050354
Training iter #18800:   Batch Loss = 0.659294, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7789888978004456, Accuracy = 0.5
Training iter #18900:   Batch Loss = 0.721339, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7564458250999451, Accuracy = 0.554347813129425
Training iter #19000:   Batch Loss = 0.683329, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7426034808158875, Accuracy = 0.54347825050354
Training iter #19100:   Batch Loss = 0.683828, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7650367617607117, Accuracy = 0.489130437374115
Training iter #19200:   Batch Loss = 0.658260, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7571507692337036, Accuracy = 0.5
Training iter #19300:   Batch Loss = 0.652187, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.764858067035675, Accuracy = 0.489130437374115
Training iter #19400:   Batch Loss = 0.754468, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7558055520057678, Accuracy = 0.532608687877655
Training iter #19500:   Batch Loss = 0.597573, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.746660590171814, Accuracy = 0.54347825050354
Training iter #19600:   Batch Loss = 0.668513, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7525990009307861, Accuracy = 0.554347813129425
Training iter #19700:   Batch Loss = 0.710261, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7438793182373047, Accuracy = 0.554347813129425
Training iter #19800:   Batch Loss = 0.671824, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7430729866027832, Accuracy = 0.554347813129425
Training iter #19900:   Batch Loss = 0.695981, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7819920182228088, Accuracy = 0.47826087474823
Training iter #20000:   Batch Loss = 0.661171, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7366483807563782, Accuracy = 0.554347813129425
Training iter #20100:   Batch Loss = 0.677832, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7484605312347412, Accuracy = 0.54347825050354
Training iter #20200:   Batch Loss = 0.753468, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.787990152835846, Accuracy = 0.45652174949645996
Training iter #20300:   Batch Loss = 0.622764, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7629766464233398, Accuracy = 0.52173912525177
Training iter #20400:   Batch Loss = 0.662744, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.74458247423172, Accuracy = 0.554347813129425
Training iter #20500:   Batch Loss = 0.693789, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7359640002250671, Accuracy = 0.554347813129425
Training iter #20600:   Batch Loss = 0.678010, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7418075799942017, Accuracy = 0.554347813129425
Training iter #20700:   Batch Loss = 0.686510, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7898925542831421, Accuracy = 0.45652174949645996
Training iter #20800:   Batch Loss = 0.641926, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7574313879013062, Accuracy = 0.532608687877655
Training iter #20900:   Batch Loss = 0.651893, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7422102689743042, Accuracy = 0.54347825050354
Training iter #21000:   Batch Loss = 0.763450, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7410942316055298, Accuracy = 0.54347825050354
Training iter #21100:   Batch Loss = 0.624923, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7475159764289856, Accuracy = 0.54347825050354
Training iter #21200:   Batch Loss = 0.673993, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7588219046592712, Accuracy = 0.532608687877655
Training iter #21300:   Batch Loss = 0.685843, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7490792870521545, Accuracy = 0.532608687877655
Training iter #21400:   Batch Loss = 0.666125, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7383240461349487, Accuracy = 0.54347825050354
Training iter #21500:   Batch Loss = 0.689760, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7562535405158997, Accuracy = 0.532608687877655
Training iter #21600:   Batch Loss = 0.631179, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7465143203735352, Accuracy = 0.554347813129425
Training iter #21700:   Batch Loss = 0.642498, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7534347176551819, Accuracy = 0.52173912525177
Training iter #21800:   Batch Loss = 0.759132, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7550981044769287, Accuracy = 0.532608687877655
Training iter #21900:   Batch Loss = 0.630847, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7506482601165771, Accuracy = 0.554347813129425
Training iter #22000:   Batch Loss = 0.667134, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7485276460647583, Accuracy = 0.554347813129425
Training iter #22100:   Batch Loss = 0.682563, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7420405149459839, Accuracy = 0.54347825050354
Training iter #22200:   Batch Loss = 0.669317, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7464313507080078, Accuracy = 0.5652173757553101
Training iter #22300:   Batch Loss = 0.708532, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7754867672920227, Accuracy = 0.489130437374115
Training iter #22400:   Batch Loss = 0.605691, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7364344596862793, Accuracy = 0.54347825050354
Training iter #22500:   Batch Loss = 0.661016, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7395646572113037, Accuracy = 0.554347813129425
Training iter #22600:   Batch Loss = 0.751827, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7652807235717773, Accuracy = 0.489130437374115
Training iter #22700:   Batch Loss = 0.640133, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7473936080932617, Accuracy = 0.54347825050354
Training iter #22800:   Batch Loss = 0.665440, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7419793009757996, Accuracy = 0.554347813129425
Training iter #22900:   Batch Loss = 0.700632, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7410286664962769, Accuracy = 0.5652173757553101
Training iter #23000:   Batch Loss = 0.658879, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7688419222831726, Accuracy = 0.5
Training iter #23100:   Batch Loss = 0.743337, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7798340320587158, Accuracy = 0.46739131212234497
Training iter #23200:   Batch Loss = 0.594188, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.735427737236023, Accuracy = 0.5652173757553101
Training iter #23300:   Batch Loss = 0.658010, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.738846480846405, Accuracy = 0.554347813129425
Training iter #23400:   Batch Loss = 0.756868, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7687280774116516, Accuracy = 0.510869562625885
Training iter #23500:   Batch Loss = 0.631979, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7521205544471741, Accuracy = 0.47826087474823
Training iter #23600:   Batch Loss = 0.655159, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7417234778404236, Accuracy = 0.554347813129425
Training iter #23700:   Batch Loss = 0.688669, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7349311709403992, Accuracy = 0.554347813129425
Training iter #23800:   Batch Loss = 0.662168, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7456249594688416, Accuracy = 0.54347825050354
Training iter #23900:   Batch Loss = 0.732021, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7682066559791565, Accuracy = 0.489130437374115
Training iter #24000:   Batch Loss = 0.605211, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7367086410522461, Accuracy = 0.554347813129425
Training iter #24100:   Batch Loss = 0.649052, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7302595973014832, Accuracy = 0.554347813129425
Training iter #24200:   Batch Loss = 0.738113, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.734669029712677, Accuracy = 0.5652173757553101
Training iter #24300:   Batch Loss = 0.642499, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7489500045776367, Accuracy = 0.489130437374115
Training iter #24400:   Batch Loss = 0.663524, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7451530694961548, Accuracy = 0.5
Training iter #24500:   Batch Loss = 0.685365, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7300355434417725, Accuracy = 0.554347813129425
Training iter #24600:   Batch Loss = 0.668691, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7313389778137207, Accuracy = 0.5652173757553101
Training iter #24700:   Batch Loss = 0.719425, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7539533972740173, Accuracy = 0.489130437374115
Training iter #24800:   Batch Loss = 0.628581, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7352410554885864, Accuracy = 0.5652173757553101
Training iter #24900:   Batch Loss = 0.616823, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7313500642776489, Accuracy = 0.5652173757553101
Training iter #25000:   Batch Loss = 0.727660, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7313842177391052, Accuracy = 0.5652173757553101
Training iter #25100:   Batch Loss = 0.644941, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7392539381980896, Accuracy = 0.532608687877655
Training iter #25200:   Batch Loss = 0.665889, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7399500012397766, Accuracy = 0.554347813129425
Training iter #25300:   Batch Loss = 0.693140, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7308148145675659, Accuracy = 0.554347813129425
Training iter #25400:   Batch Loss = 0.685479, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7412593960762024, Accuracy = 0.554347813129425
Training iter #25500:   Batch Loss = 0.719841, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7623506188392639, Accuracy = 0.47826087474823
Training iter #25600:   Batch Loss = 0.637746, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7349143028259277, Accuracy = 0.554347813129425
Training iter #25700:   Batch Loss = 0.649598, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7338137626647949, Accuracy = 0.554347813129425
Training iter #25800:   Batch Loss = 0.736094, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7372623085975647, Accuracy = 0.554347813129425
Training iter #25900:   Batch Loss = 0.649896, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7362431883811951, Accuracy = 0.554347813129425
Training iter #26000:   Batch Loss = 0.643289, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7401954531669617, Accuracy = 0.54347825050354
Training iter #26100:   Batch Loss = 0.670644, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7389219403266907, Accuracy = 0.554347813129425
Training iter #26200:   Batch Loss = 0.691995, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7278900742530823, Accuracy = 0.5652173757553101
Training iter #26300:   Batch Loss = 0.731337, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7345297932624817, Accuracy = 0.54347825050354
Training iter #26400:   Batch Loss = 0.645972, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7329369783401489, Accuracy = 0.54347825050354
Training iter #26500:   Batch Loss = 0.650466, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7384803295135498, Accuracy = 0.554347813129425
Training iter #26600:   Batch Loss = 0.728576, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7369999289512634, Accuracy = 0.5652173757553101
Training iter #26700:   Batch Loss = 0.651775, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7332367300987244, Accuracy = 0.5652173757553101
Training iter #26800:   Batch Loss = 0.645964, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7425581812858582, Accuracy = 0.532608687877655
Training iter #26900:   Batch Loss = 0.661198, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.744659423828125, Accuracy = 0.532608687877655
Training iter #27000:   Batch Loss = 0.698343, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7291824221611023, Accuracy = 0.554347813129425
Training iter #27100:   Batch Loss = 0.715411, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.744061291217804, Accuracy = 0.554347813129425
Training iter #27200:   Batch Loss = 0.675189, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7410842776298523, Accuracy = 0.532608687877655
Training iter #27300:   Batch Loss = 0.611973, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7388895750045776, Accuracy = 0.554347813129425
Training iter #27400:   Batch Loss = 0.708315, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7336140275001526, Accuracy = 0.554347813129425
Training iter #27500:   Batch Loss = 0.646900, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7306020259857178, Accuracy = 0.554347813129425
Training iter #27600:   Batch Loss = 0.645474, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7496439218521118, Accuracy = 0.510869562625885
Training iter #27700:   Batch Loss = 0.631170, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7389118075370789, Accuracy = 0.532608687877655
Training iter #27800:   Batch Loss = 0.661366, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7292410135269165, Accuracy = 0.5652173757553101
Training iter #27900:   Batch Loss = 0.731637, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7362135648727417, Accuracy = 0.554347813129425
Training iter #28000:   Batch Loss = 0.666695, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7380814552307129, Accuracy = 0.52173912525177
Training iter #28100:   Batch Loss = 0.625266, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7432396411895752, Accuracy = 0.489130437374115
Training iter #28200:   Batch Loss = 0.713507, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7349370121955872, Accuracy = 0.5652173757553101
Training iter #28300:   Batch Loss = 0.672078, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7303954362869263, Accuracy = 0.554347813129425
Training iter #28400:   Batch Loss = 0.640128, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7445958256721497, Accuracy = 0.52173912525177
Training iter #28500:   Batch Loss = 0.624494, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7587244510650635, Accuracy = 0.5
Training iter #28600:   Batch Loss = 0.656537, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7325306534767151, Accuracy = 0.554347813129425
Training iter #28700:   Batch Loss = 0.717932, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7387384176254272, Accuracy = 0.5652173757553101
Training iter #28800:   Batch Loss = 0.656820, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7472564578056335, Accuracy = 0.510869562625885
Training iter #28900:   Batch Loss = 0.648282, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.754694402217865, Accuracy = 0.5
Training iter #29000:   Batch Loss = 0.733040, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7334839701652527, Accuracy = 0.554347813129425
Training iter #29100:   Batch Loss = 0.671870, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7260497212409973, Accuracy = 0.554347813129425
Training iter #29200:   Batch Loss = 0.655483, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7472561597824097, Accuracy = 0.52173912525177
Training iter #29300:   Batch Loss = 0.640842, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7511162161827087, Accuracy = 0.489130437374115
Training iter #29400:   Batch Loss = 0.637439, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7302861213684082, Accuracy = 0.54347825050354
Training iter #29500:   Batch Loss = 0.715165, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7292168736457825, Accuracy = 0.5652173757553101
Training iter #29600:   Batch Loss = 0.651296, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7344432473182678, Accuracy = 0.54347825050354
Training iter #29700:   Batch Loss = 0.633975, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7477042078971863, Accuracy = 0.510869562625885
Training iter #29800:   Batch Loss = 0.717362, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7382189631462097, Accuracy = 0.54347825050354
Training iter #29900:   Batch Loss = 0.667386, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7269763350486755, Accuracy = 0.554347813129425
Training iter #30000:   Batch Loss = 0.652902, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7378904819488525, Accuracy = 0.5652173757553101
Training iter #30100:   Batch Loss = 0.629633, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7497659921646118, Accuracy = 0.52173912525177
Training iter #30200:   Batch Loss = 0.643516, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7333443760871887, Accuracy = 0.54347825050354
Training iter #30300:   Batch Loss = 0.727296, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7320569157600403, Accuracy = 0.5652173757553101
Training iter #30400:   Batch Loss = 0.635040, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7360067963600159, Accuracy = 0.54347825050354
Training iter #30500:   Batch Loss = 0.635408, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7492581605911255, Accuracy = 0.52173912525177
Training iter #30600:   Batch Loss = 0.692334, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7388476729393005, Accuracy = 0.54347825050354
Training iter #30700:   Batch Loss = 0.699656, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.729732871055603, Accuracy = 0.5652173757553101
Training iter #30800:   Batch Loss = 0.653315, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7390254139900208, Accuracy = 0.532608687877655
Training iter #30900:   Batch Loss = 0.630041, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.766313910484314, Accuracy = 0.510869562625885
Training iter #31000:   Batch Loss = 0.639688, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.740275502204895, Accuracy = 0.54347825050354
Training iter #31100:   Batch Loss = 0.737256, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.734737753868103, Accuracy = 0.554347813129425
Training iter #31200:   Batch Loss = 0.589678, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7406749725341797, Accuracy = 0.554347813129425
Training iter #31300:   Batch Loss = 0.679963, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7721799612045288, Accuracy = 0.47826087474823
Training iter #31400:   Batch Loss = 0.734976, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7350043058395386, Accuracy = 0.554347813129425
Training iter #31500:   Batch Loss = 0.689445, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7244054079055786, Accuracy = 0.554347813129425
Training iter #31600:   Batch Loss = 0.672379, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7273286581039429, Accuracy = 0.532608687877655
Training iter #31700:   Batch Loss = 0.660399, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7485277056694031, Accuracy = 0.52173912525177
Training iter #31800:   Batch Loss = 0.624367, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7542293071746826, Accuracy = 0.47826087474823
Training iter #31900:   Batch Loss = 0.719067, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.722288191318512, Accuracy = 0.532608687877655
Training iter #32000:   Batch Loss = 0.604231, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7156949639320374, Accuracy = 0.5652173757553101
Training iter #32100:   Batch Loss = 0.633162, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7245771884918213, Accuracy = 0.532608687877655
Training iter #32200:   Batch Loss = 0.683507, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7459555268287659, Accuracy = 0.489130437374115
Training iter #32300:   Batch Loss = 0.648097, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7339537143707275, Accuracy = 0.554347813129425
Training iter #32400:   Batch Loss = 0.647868, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7296494245529175, Accuracy = 0.532608687877655
Training iter #32500:   Batch Loss = 0.646726, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7252660393714905, Accuracy = 0.554347813129425
Training iter #32600:   Batch Loss = 0.655609, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.736517608165741, Accuracy = 0.54347825050354
Training iter #32700:   Batch Loss = 0.721277, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7538106441497803, Accuracy = 0.5
Training iter #32800:   Batch Loss = 0.588602, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7389959096908569, Accuracy = 0.54347825050354
Training iter #32900:   Batch Loss = 0.633709, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7356041073799133, Accuracy = 0.554347813129425
Training iter #33000:   Batch Loss = 0.704642, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7394613027572632, Accuracy = 0.554347813129425
Training iter #33100:   Batch Loss = 0.635793, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7377182245254517, Accuracy = 0.54347825050354
Training iter #33200:   Batch Loss = 0.641383, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7443085312843323, Accuracy = 0.5
Training iter #33300:   Batch Loss = 0.614635, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7328875660896301, Accuracy = 0.54347825050354
Training iter #33400:   Batch Loss = 0.639775, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7309725880622864, Accuracy = 0.54347825050354
Training iter #33500:   Batch Loss = 0.725962, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7576609253883362, Accuracy = 0.510869562625885
Training iter #33600:   Batch Loss = 0.610154, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7298411726951599, Accuracy = 0.532608687877655
Training iter #33700:   Batch Loss = 0.633726, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7269324660301208, Accuracy = 0.554347813129425
Training iter #33800:   Batch Loss = 0.671859, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7347829341888428, Accuracy = 0.54347825050354
Training iter #33900:   Batch Loss = 0.623371, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.735821545124054, Accuracy = 0.554347813129425
Training iter #34000:   Batch Loss = 0.659793, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7399778366088867, Accuracy = 0.52173912525177
Training iter #34100:   Batch Loss = 0.630200, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7317139506340027, Accuracy = 0.54347825050354
Training iter #34200:   Batch Loss = 0.647258, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7304813861846924, Accuracy = 0.554347813129425
Training iter #34300:   Batch Loss = 0.778691, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7597565650939941, Accuracy = 0.52173912525177
Training iter #34400:   Batch Loss = 0.640037, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7322254776954651, Accuracy = 0.532608687877655
Training iter #34500:   Batch Loss = 0.625334, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7276500463485718, Accuracy = 0.554347813129425
Training iter #34600:   Batch Loss = 0.665684, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7329480648040771, Accuracy = 0.54347825050354
Training iter #34700:   Batch Loss = 0.628173, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7392070889472961, Accuracy = 0.554347813129425
Training iter #34800:   Batch Loss = 0.635657, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7477446794509888, Accuracy = 0.52173912525177
Training iter #34900:   Batch Loss = 0.610741, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7251430153846741, Accuracy = 0.54347825050354
Training iter #35000:   Batch Loss = 0.683844, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7232654690742493, Accuracy = 0.54347825050354
Training iter #35100:   Batch Loss = 0.745874, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7407622933387756, Accuracy = 0.54347825050354
Training iter #35200:   Batch Loss = 0.645273, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7414765954017639, Accuracy = 0.52173912525177
Training iter #35300:   Batch Loss = 0.650549, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7305629849433899, Accuracy = 0.54347825050354
Training iter #35400:   Batch Loss = 0.664114, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7296738624572754, Accuracy = 0.54347825050354
Training iter #35500:   Batch Loss = 0.641907, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7313669323921204, Accuracy = 0.54347825050354
Training iter #35600:   Batch Loss = 0.653551, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7588338255882263, Accuracy = 0.5
Training iter #35700:   Batch Loss = 0.587707, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7363483309745789, Accuracy = 0.532608687877655
Training iter #35800:   Batch Loss = 0.649642, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7257692813873291, Accuracy = 0.554347813129425
Training iter #35900:   Batch Loss = 0.724926, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7312448620796204, Accuracy = 0.5652173757553101
Training iter #36000:   Batch Loss = 0.627567, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.741102933883667, Accuracy = 0.5
Training iter #36100:   Batch Loss = 0.664731, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7371764779090881, Accuracy = 0.54347825050354
Training iter #36200:   Batch Loss = 0.660340, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7284697890281677, Accuracy = 0.554347813129425
Training iter #36300:   Batch Loss = 0.644494, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7221246957778931, Accuracy = 0.532608687877655
Training iter #36400:   Batch Loss = 0.683072, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.740416944026947, Accuracy = 0.554347813129425
Training iter #36500:   Batch Loss = 0.601597, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7369509339332581, Accuracy = 0.554347813129425
Training iter #36600:   Batch Loss = 0.627753, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7296737432479858, Accuracy = 0.54347825050354
Training iter #36700:   Batch Loss = 0.726699, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.731278121471405, Accuracy = 0.5652173757553101
Training iter #36800:   Batch Loss = 0.624121, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.73204106092453, Accuracy = 0.5652173757553101
Training iter #36900:   Batch Loss = 0.630390, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7292920351028442, Accuracy = 0.54347825050354
Training iter #37000:   Batch Loss = 0.664968, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7363482713699341, Accuracy = 0.54347825050354
Training iter #37100:   Batch Loss = 0.646434, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7341357469558716, Accuracy = 0.54347825050354
Training iter #37200:   Batch Loss = 0.692863, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.737457275390625, Accuracy = 0.554347813129425
Training iter #37300:   Batch Loss = 0.607582, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7220462560653687, Accuracy = 0.5652173757553101
Training iter #37400:   Batch Loss = 0.646050, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.726167619228363, Accuracy = 0.54347825050354
Training iter #37500:   Batch Loss = 0.749428, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7355170845985413, Accuracy = 0.510869562625885
Training iter #37600:   Batch Loss = 0.697416, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7261669039726257, Accuracy = 0.554347813129425
Training iter #37700:   Batch Loss = 0.645510, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.724258303642273, Accuracy = 0.554347813129425
Training iter #37800:   Batch Loss = 0.745853, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7253791093826294, Accuracy = 0.532608687877655
Training iter #37900:   Batch Loss = 0.680578, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7349556684494019, Accuracy = 0.52173912525177
Training iter #38000:   Batch Loss = 0.696821, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7411873936653137, Accuracy = 0.510869562625885
Training iter #38100:   Batch Loss = 0.653082, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7266848087310791, Accuracy = 0.554347813129425
Training iter #38200:   Batch Loss = 0.669956, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7362000942230225, Accuracy = 0.54347825050354
Training iter #38300:   Batch Loss = 0.758367, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7663652896881104, Accuracy = 0.510869562625885
Training iter #38400:   Batch Loss = 0.695729, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7530235052108765, Accuracy = 0.54347825050354
Training iter #38500:   Batch Loss = 0.648542, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7498718500137329, Accuracy = 0.54347825050354
Training iter #38600:   Batch Loss = 0.743514, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7551671862602234, Accuracy = 0.54347825050354
Training iter #38700:   Batch Loss = 0.678177, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7739062309265137, Accuracy = 0.52173912525177
Training iter #38800:   Batch Loss = 0.703112, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7801821827888489, Accuracy = 0.52173912525177
Training iter #38900:   Batch Loss = 0.637167, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7488873600959778, Accuracy = 0.554347813129425
Training iter #39000:   Batch Loss = 0.671977, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7596947550773621, Accuracy = 0.52173912525177
Training iter #39100:   Batch Loss = 0.793453, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7856138348579407, Accuracy = 0.489130437374115
Training iter #39200:   Batch Loss = 0.664905, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7578756213188171, Accuracy = 0.54347825050354
Training iter #39300:   Batch Loss = 0.653358, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7622854113578796, Accuracy = 0.554347813129425
Training iter #39400:   Batch Loss = 0.714316, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7807784676551819, Accuracy = 0.510869562625885
Training iter #39500:   Batch Loss = 0.663716, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7604631781578064, Accuracy = 0.54347825050354
Training iter #39600:   Batch Loss = 0.704389, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7622047662734985, Accuracy = 0.54347825050354
Training iter #39700:   Batch Loss = 0.625566, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7594587802886963, Accuracy = 0.532608687877655
Training iter #39800:   Batch Loss = 0.662263, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7833747863769531, Accuracy = 0.5
Training iter #39900:   Batch Loss = 0.770537, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7667035460472107, Accuracy = 0.554347813129425
Training iter #40000:   Batch Loss = 0.640778, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.758449912071228, Accuracy = 0.532608687877655
Training iter #40100:   Batch Loss = 0.650920, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7770746946334839, Accuracy = 0.510869562625885
Training iter #40200:   Batch Loss = 0.716977, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7883421182632446, Accuracy = 0.489130437374115
Training iter #40300:   Batch Loss = 0.657979, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7619234919548035, Accuracy = 0.54347825050354
Training iter #40400:   Batch Loss = 0.694061, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7617944478988647, Accuracy = 0.5652173757553101
Training iter #40500:   Batch Loss = 0.694788, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7627060413360596, Accuracy = 0.54347825050354
Training iter #40600:   Batch Loss = 0.619650, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7705860733985901, Accuracy = 0.52173912525177
Training iter #40700:   Batch Loss = 0.736330, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7576901912689209, Accuracy = 0.554347813129425
Training iter #40800:   Batch Loss = 0.638580, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7591863870620728, Accuracy = 0.54347825050354
Training iter #40900:   Batch Loss = 0.667532, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7946676015853882, Accuracy = 0.47826087474823
Training iter #41000:   Batch Loss = 0.679695, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7711342573165894, Accuracy = 0.52173912525177
Training iter #41100:   Batch Loss = 0.659483, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7553893327713013, Accuracy = 0.532608687877655
Training iter #41200:   Batch Loss = 0.695069, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7674094438552856, Accuracy = 0.52173912525177
Training iter #41300:   Batch Loss = 0.702270, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7702268958091736, Accuracy = 0.532608687877655
Training iter #41400:   Batch Loss = 0.633628, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7617020606994629, Accuracy = 0.510869562625885
Training iter #41500:   Batch Loss = 0.717451, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7547131180763245, Accuracy = 0.532608687877655
Training iter #41600:   Batch Loss = 0.663644, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7490609884262085, Accuracy = 0.54347825050354
Training iter #41700:   Batch Loss = 0.682407, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7545737028121948, Accuracy = 0.5760869383811951
Training iter #41800:   Batch Loss = 0.689151, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7536701560020447, Accuracy = 0.54347825050354
Training iter #41900:   Batch Loss = 0.696210, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7165791988372803, Accuracy = 0.6086956262588501
Training iter #42000:   Batch Loss = 0.722861, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.715741753578186, Accuracy = 0.5978260636329651
Training iter #42100:   Batch Loss = 0.635889, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7375552654266357, Accuracy = 0.54347825050354
Training iter #42200:   Batch Loss = 0.708036, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.8561238646507263, Accuracy = 0.3586956560611725
Training iter #42300:   Batch Loss = 0.797476, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.8152749538421631, Accuracy = 0.43478259444236755
Training iter #42400:   Batch Loss = 0.656425, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7101449370384216, Accuracy = 0.5978260636329651
Training iter #42500:   Batch Loss = 0.701010, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.696985125541687, Accuracy = 0.6086956262588501
Training iter #42600:   Batch Loss = 0.683171, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7051863670349121, Accuracy = 0.5978260636329651
Training iter #42700:   Batch Loss = 0.680153, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7550938129425049, Accuracy = 0.52173912525177
Training iter #42800:   Batch Loss = 0.684079, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7942479848861694, Accuracy = 0.43478259444236755
Training iter #42900:   Batch Loss = 0.707753, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7693344950675964, Accuracy = 0.489130437374115
Training iter #43000:   Batch Loss = 0.665913, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7483792901039124, Accuracy = 0.532608687877655
Training iter #43100:   Batch Loss = 0.726822, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7429685592651367, Accuracy = 0.532608687877655
Training iter #43200:   Batch Loss = 0.661417, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7434062957763672, Accuracy = 0.532608687877655
Training iter #43300:   Batch Loss = 0.661259, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7610410451889038, Accuracy = 0.489130437374115
Training iter #43400:   Batch Loss = 0.659248, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7680101990699768, Accuracy = 0.5
Training iter #43500:   Batch Loss = 0.649397, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.767867922782898, Accuracy = 0.489130437374115
Training iter #43600:   Batch Loss = 0.685634, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.750861644744873, Accuracy = 0.54347825050354
Training iter #43700:   Batch Loss = 0.630101, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.74066561460495, Accuracy = 0.54347825050354
Training iter #43800:   Batch Loss = 0.674329, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7575908899307251, Accuracy = 0.54347825050354
Training iter #43900:   Batch Loss = 0.727791, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7828961610794067, Accuracy = 0.47826087474823
Training iter #44000:   Batch Loss = 0.643874, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7568341493606567, Accuracy = 0.532608687877655
Training iter #44100:   Batch Loss = 0.652209, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7585252523422241, Accuracy = 0.54347825050354
Training iter #44200:   Batch Loss = 0.655297, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7649127244949341, Accuracy = 0.554347813129425
Training iter #44300:   Batch Loss = 0.655836, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.770682156085968, Accuracy = 0.52173912525177
Training iter #44400:   Batch Loss = 0.682998, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7683371305465698, Accuracy = 0.554347813129425
Training iter #44500:   Batch Loss = 0.604878, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7549930810928345, Accuracy = 0.554347813129425
Training iter #44600:   Batch Loss = 0.656652, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7716627717018127, Accuracy = 0.52173912525177
Training iter #44700:   Batch Loss = 0.749300, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7803428769111633, Accuracy = 0.510869562625885
Training iter #44800:   Batch Loss = 0.673999, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7379725575447083, Accuracy = 0.5652173757553101
Training iter #44900:   Batch Loss = 0.670486, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7548118233680725, Accuracy = 0.554347813129425
Training iter #45000:   Batch Loss = 0.654581, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7764683365821838, Accuracy = 0.510869562625885
Training iter #45100:   Batch Loss = 0.622283, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7806597352027893, Accuracy = 0.5
Training iter #45200:   Batch Loss = 0.707588, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7718175649642944, Accuracy = 0.52173912525177
Training iter #45300:   Batch Loss = 0.579291, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7546708583831787, Accuracy = 0.5652173757553101
Training iter #45400:   Batch Loss = 0.643913, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7616745829582214, Accuracy = 0.554347813129425
Training iter #45500:   Batch Loss = 0.682108, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7758470773696899, Accuracy = 0.5
Training iter #45600:   Batch Loss = 0.643375, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7646243572235107, Accuracy = 0.54347825050354
Training iter #45700:   Batch Loss = 0.634951, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7674776911735535, Accuracy = 0.5652173757553101
Training iter #45800:   Batch Loss = 0.637806, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7726244926452637, Accuracy = 0.532608687877655
Training iter #45900:   Batch Loss = 0.651560, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7637734413146973, Accuracy = 0.532608687877655
Training iter #46000:   Batch Loss = 0.705738, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7599021196365356, Accuracy = 0.54347825050354
Training iter #46100:   Batch Loss = 0.551743, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.7635294198989868, Accuracy = 0.532608687877655
Training iter #46200:   Batch Loss = 0.677418, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.800738513469696, Accuracy = 0.510869562625885
Training iter #46300:   Batch Loss = 0.750466, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7853026390075684, Accuracy = 0.489130437374115
Training iter #46400:   Batch Loss = 0.626279, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7568949460983276, Accuracy = 0.532608687877655
Training iter #46500:   Batch Loss = 0.640674, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7606664299964905, Accuracy = 0.554347813129425
Training iter #46600:   Batch Loss = 0.640401, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7677831053733826, Accuracy = 0.52173912525177
Training iter #46700:   Batch Loss = 0.640524, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7607995271682739, Accuracy = 0.532608687877655
Training iter #46800:   Batch Loss = 0.685622, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7560909390449524, Accuracy = 0.554347813129425
Training iter #46900:   Batch Loss = 0.564730, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7487043738365173, Accuracy = 0.532608687877655
Training iter #47000:   Batch Loss = 0.651108, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7600761651992798, Accuracy = 0.554347813129425
Training iter #47100:   Batch Loss = 0.690102, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7649739980697632, Accuracy = 0.54347825050354
Training iter #47200:   Batch Loss = 0.656129, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7601569294929504, Accuracy = 0.532608687877655
Training iter #47300:   Batch Loss = 0.644925, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7809949517250061, Accuracy = 0.5
Training iter #47400:   Batch Loss = 0.632992, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7522299885749817, Accuracy = 0.554347813129425
Training iter #47500:   Batch Loss = 0.725132, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7442331314086914, Accuracy = 0.54347825050354
Training iter #47600:   Batch Loss = 0.718240, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7906066179275513, Accuracy = 0.489130437374115
Training iter #47700:   Batch Loss = 0.672193, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.8483203649520874, Accuracy = 0.44565218687057495
Training iter #47800:   Batch Loss = 0.741267, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7747460603713989, Accuracy = 0.489130437374115
Training iter #47900:   Batch Loss = 0.675686, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7378169894218445, Accuracy = 0.554347813129425
Training iter #48000:   Batch Loss = 0.693004, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7259272336959839, Accuracy = 0.5652173757553101
Training iter #48100:   Batch Loss = 0.651890, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7477784752845764, Accuracy = 0.554347813129425
Training iter #48200:   Batch Loss = 0.650620, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7646026611328125, Accuracy = 0.5
Training iter #48300:   Batch Loss = 0.611277, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.774198055267334, Accuracy = 0.510869562625885
Training iter #48400:   Batch Loss = 0.727937, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.752350389957428, Accuracy = 0.5
Training iter #48500:   Batch Loss = 0.622330, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7369840741157532, Accuracy = 0.532608687877655
Training iter #48600:   Batch Loss = 0.653338, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7368571758270264, Accuracy = 0.54347825050354
Training iter #48700:   Batch Loss = 0.678443, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7413486242294312, Accuracy = 0.54347825050354
Training iter #48800:   Batch Loss = 0.654270, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7457532286643982, Accuracy = 0.54347825050354
Training iter #48900:   Batch Loss = 0.634441, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7647555470466614, Accuracy = 0.5
Training iter #49000:   Batch Loss = 0.609524, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7499747276306152, Accuracy = 0.532608687877655
Training iter #49100:   Batch Loss = 0.656735, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7553122639656067, Accuracy = 0.554347813129425
Training iter #49200:   Batch Loss = 0.683109, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7629907131195068, Accuracy = 0.532608687877655
Training iter #49300:   Batch Loss = 0.581472, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.743910551071167, Accuracy = 0.54347825050354
Training iter #49400:   Batch Loss = 0.670920, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7420546412467957, Accuracy = 0.54347825050354
Training iter #49500:   Batch Loss = 0.676393, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7742416858673096, Accuracy = 0.510869562625885
Training iter #49600:   Batch Loss = 0.637804, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7573431730270386, Accuracy = 0.54347825050354
Training iter #49700:   Batch Loss = 0.656547, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7698749303817749, Accuracy = 0.510869562625885
Training iter #49800:   Batch Loss = 0.622182, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7546630501747131, Accuracy = 0.532608687877655
Training iter #49900:   Batch Loss = 0.650708, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.736886203289032, Accuracy = 0.54347825050354
Training iter #50000:   Batch Loss = 0.716228, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.736751914024353, Accuracy = 0.554347813129425
Training iter #50100:   Batch Loss = 0.600234, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7482969164848328, Accuracy = 0.54347825050354
Training iter #50200:   Batch Loss = 0.649009, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7770802974700928, Accuracy = 0.5
Training iter #50300:   Batch Loss = 0.672559, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7884736061096191, Accuracy = 0.5
Training iter #50400:   Batch Loss = 0.630872, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7675785422325134, Accuracy = 0.510869562625885
Training iter #50500:   Batch Loss = 0.659034, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7536221742630005, Accuracy = 0.532608687877655
Training iter #50600:   Batch Loss = 0.605401, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7421516180038452, Accuracy = 0.54347825050354
Training iter #50700:   Batch Loss = 0.661594, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7505049705505371, Accuracy = 0.532608687877655
Training iter #50800:   Batch Loss = 0.724797, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7718888521194458, Accuracy = 0.510869562625885
Training iter #50900:   Batch Loss = 0.631919, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7733145952224731, Accuracy = 0.5
Training iter #51000:   Batch Loss = 0.636775, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7650191783905029, Accuracy = 0.532608687877655
Training iter #51100:   Batch Loss = 0.688027, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7473722100257874, Accuracy = 0.532608687877655
Training iter #51200:   Batch Loss = 0.661579, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7598172426223755, Accuracy = 0.52173912525177
Training iter #51300:   Batch Loss = 0.654531, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7820309400558472, Accuracy = 0.47826087474823
Training iter #51400:   Batch Loss = 0.623045, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7370179295539856, Accuracy = 0.54347825050354
Training iter #51500:   Batch Loss = 0.645674, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7277522683143616, Accuracy = 0.54347825050354
Training iter #51600:   Batch Loss = 0.714421, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7303513884544373, Accuracy = 0.54347825050354
Training iter #51700:   Batch Loss = 0.618300, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.739550769329071, Accuracy = 0.54347825050354
Training iter #51800:   Batch Loss = 0.638408, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.763091504573822, Accuracy = 0.47826087474823
Training iter #51900:   Batch Loss = 0.695551, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7668375968933105, Accuracy = 0.5
Training iter #52000:   Batch Loss = 0.651305, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7438225746154785, Accuracy = 0.54347825050354
Training iter #52100:   Batch Loss = 0.668350, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7369102239608765, Accuracy = 0.54347825050354
Training iter #52200:   Batch Loss = 0.634864, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7340202331542969, Accuracy = 0.54347825050354
Training iter #52300:   Batch Loss = 0.653843, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7444646954536438, Accuracy = 0.54347825050354
Training iter #52400:   Batch Loss = 0.703723, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7554856538772583, Accuracy = 0.532608687877655
Training iter #52500:   Batch Loss = 0.644998, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7564413547515869, Accuracy = 0.554347813129425
Training iter #52600:   Batch Loss = 0.617409, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.759077250957489, Accuracy = 0.54347825050354
Training iter #52700:   Batch Loss = 0.706197, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7719394564628601, Accuracy = 0.510869562625885
Training iter #52800:   Batch Loss = 0.638110, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.8137960433959961, Accuracy = 0.489130437374115
Training iter #52900:   Batch Loss = 0.657065, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7823293805122375, Accuracy = 0.5
Training iter #53000:   Batch Loss = 0.623298, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7099019289016724, Accuracy = 0.5760869383811951
Training iter #53100:   Batch Loss = 0.689192, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6802195310592651, Accuracy = 0.6521739363670349
Training iter #53200:   Batch Loss = 0.666906, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6765891313552856, Accuracy = 0.6521739363670349
Training iter #53300:   Batch Loss = 0.688455, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6982698440551758, Accuracy = 0.6086956262588501
Training iter #53400:   Batch Loss = 0.660687, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7545445561408997, Accuracy = 0.47826087474823
Training iter #53500:   Batch Loss = 0.690490, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7868210077285767, Accuracy = 0.5
Training iter #53600:   Batch Loss = 0.653032, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.773006021976471, Accuracy = 0.47826087474823
Training iter #53700:   Batch Loss = 0.644670, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7293737530708313, Accuracy = 0.54347825050354
Training iter #53800:   Batch Loss = 0.664546, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7134239077568054, Accuracy = 0.554347813129425
Training iter #53900:   Batch Loss = 0.694452, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7189599871635437, Accuracy = 0.5652173757553101
Training iter #54000:   Batch Loss = 0.689026, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7333381175994873, Accuracy = 0.532608687877655
Training iter #54100:   Batch Loss = 0.656434, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7484747767448425, Accuracy = 0.52173912525177
Training iter #54200:   Batch Loss = 0.664125, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7672277688980103, Accuracy = 0.510869562625885
Training iter #54300:   Batch Loss = 0.663174, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7522038817405701, Accuracy = 0.510869562625885
Training iter #54400:   Batch Loss = 0.634374, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7299977540969849, Accuracy = 0.532608687877655
Training iter #54500:   Batch Loss = 0.665759, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7192310690879822, Accuracy = 0.554347813129425
Training iter #54600:   Batch Loss = 0.639635, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7210875749588013, Accuracy = 0.54347825050354
Training iter #54700:   Batch Loss = 0.644509, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7408151626586914, Accuracy = 0.532608687877655
Training iter #54800:   Batch Loss = 0.720150, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7476435303688049, Accuracy = 0.5
Training iter #54900:   Batch Loss = 0.649848, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7282522916793823, Accuracy = 0.54347825050354
Training iter #55000:   Batch Loss = 0.640542, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7312456369400024, Accuracy = 0.54347825050354
Training iter #55100:   Batch Loss = 0.629696, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7395420670509338, Accuracy = 0.54347825050354
Training iter #55200:   Batch Loss = 0.618846, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.747934103012085, Accuracy = 0.510869562625885
Training iter #55300:   Batch Loss = 0.635739, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.742260754108429, Accuracy = 0.554347813129425
Training iter #55400:   Batch Loss = 0.641763, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7289594411849976, Accuracy = 0.54347825050354
Training iter #55500:   Batch Loss = 0.642882, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7334040999412537, Accuracy = 0.54347825050354
Training iter #55600:   Batch Loss = 0.710150, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7457485795021057, Accuracy = 0.532608687877655
Training iter #55700:   Batch Loss = 0.649728, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7339074611663818, Accuracy = 0.54347825050354
Training iter #55800:   Batch Loss = 0.645745, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7380889654159546, Accuracy = 0.554347813129425
Training iter #55900:   Batch Loss = 0.639771, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7464307546615601, Accuracy = 0.532608687877655
Training iter #56000:   Batch Loss = 0.618872, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7440723180770874, Accuracy = 0.5652173757553101
Training iter #56100:   Batch Loss = 0.641751, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7429174780845642, Accuracy = 0.554347813129425
Training iter #56200:   Batch Loss = 0.651083, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7395682334899902, Accuracy = 0.54347825050354
Training iter #56300:   Batch Loss = 0.621032, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7402264475822449, Accuracy = 0.54347825050354
Training iter #56400:   Batch Loss = 0.690568, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7479905486106873, Accuracy = 0.554347813129425
Training iter #56500:   Batch Loss = 0.645762, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7354277968406677, Accuracy = 0.532608687877655
Training iter #56600:   Batch Loss = 0.638662, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7415960431098938, Accuracy = 0.554347813129425
Training iter #56700:   Batch Loss = 0.621092, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.754751443862915, Accuracy = 0.510869562625885
Training iter #56800:   Batch Loss = 0.624371, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7686248421669006, Accuracy = 0.5
Training iter #56900:   Batch Loss = 0.645693, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7411202788352966, Accuracy = 0.5652173757553101
Training iter #57000:   Batch Loss = 0.611015, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7264876365661621, Accuracy = 0.54347825050354
Training iter #57100:   Batch Loss = 0.635514, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7223649621009827, Accuracy = 0.532608687877655
Training iter #57200:   Batch Loss = 0.667043, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7297289967536926, Accuracy = 0.554347813129425
Training iter #57300:   Batch Loss = 0.645724, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7336477041244507, Accuracy = 0.554347813129425
Training iter #57400:   Batch Loss = 0.627791, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7466563582420349, Accuracy = 0.510869562625885
Training iter #57500:   Batch Loss = 0.626879, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7499278783798218, Accuracy = 0.5
Training iter #57600:   Batch Loss = 0.625737, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7355633974075317, Accuracy = 0.5760869383811951
Training iter #57700:   Batch Loss = 0.664648, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7367821335792542, Accuracy = 0.5652173757553101
Training iter #57800:   Batch Loss = 0.636727, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7420217990875244, Accuracy = 0.554347813129425
Training iter #57900:   Batch Loss = 0.633498, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7440738081932068, Accuracy = 0.532608687877655
Training iter #58000:   Batch Loss = 0.667077, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7395975589752197, Accuracy = 0.554347813129425
Training iter #58100:   Batch Loss = 0.667840, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7249724864959717, Accuracy = 0.554347813129425
Training iter #58200:   Batch Loss = 0.635481, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7296613454818726, Accuracy = 0.554347813129425
Training iter #58300:   Batch Loss = 0.636044, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7367944717407227, Accuracy = 0.532608687877655
Training iter #58400:   Batch Loss = 0.597601, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7368988990783691, Accuracy = 0.54347825050354
Training iter #58500:   Batch Loss = 0.672885, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7380697131156921, Accuracy = 0.554347813129425
Training iter #58600:   Batch Loss = 0.590614, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7588232755661011, Accuracy = 0.5
Training iter #58700:   Batch Loss = 0.690111, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.8177546262741089, Accuracy = 0.47826087474823
Training iter #58800:   Batch Loss = 0.698381, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7232541441917419, Accuracy = 0.54347825050354
Training iter #58900:   Batch Loss = 0.648394, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6905664205551147, Accuracy = 0.6195651888847351
Training iter #59000:   Batch Loss = 0.672441, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6909294128417969, Accuracy = 0.6086956262588501
Training iter #59100:   Batch Loss = 0.730077, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6987677812576294, Accuracy = 0.5760869383811951
Training iter #59200:   Batch Loss = 0.658283, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7441469430923462, Accuracy = 0.5
Training iter #59300:   Batch Loss = 0.701835, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.8040636777877808, Accuracy = 0.489130437374115
Training iter #59400:   Batch Loss = 0.681438, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7912812232971191, Accuracy = 0.46739131212234497
Training iter #59500:   Batch Loss = 0.679918, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7458941340446472, Accuracy = 0.489130437374115
Training iter #59600:   Batch Loss = 0.692461, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7077270746231079, Accuracy = 0.554347813129425
Training iter #59700:   Batch Loss = 0.640701, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6893212795257568, Accuracy = 0.6086956262588501
Training iter #59800:   Batch Loss = 0.665253, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6864706873893738, Accuracy = 0.6086956262588501
Training iter #59900:   Batch Loss = 0.688492, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6841965913772583, Accuracy = 0.6086956262588501
Training iter #60000:   Batch Loss = 0.666274, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6987462639808655, Accuracy = 0.5760869383811951
Training iter #60100:   Batch Loss = 0.705433, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7253060340881348, Accuracy = 0.54347825050354
Training iter #60200:   Batch Loss = 0.584297, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7442110776901245, Accuracy = 0.489130437374115
Training iter #60300:   Batch Loss = 0.664614, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7443290948867798, Accuracy = 0.489130437374115
Training iter #60400:   Batch Loss = 0.692037, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7312609553337097, Accuracy = 0.54347825050354
Training iter #60500:   Batch Loss = 0.791590, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.731908917427063, Accuracy = 0.6086956262588501
Training iter #60600:   Batch Loss = 0.711063, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7428714632987976, Accuracy = 0.6195651888847351
Training iter #60700:   Batch Loss = 0.796278, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7436546087265015, Accuracy = 0.6195651888847351
Training iter #60800:   Batch Loss = 0.853971, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7444503903388977, Accuracy = 0.5869565010070801
Training iter #60900:   Batch Loss = 0.787047, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7481676340103149, Accuracy = 0.532608687877655
Training iter #61000:   Batch Loss = 0.675251, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7508653998374939, Accuracy = 0.52173912525177
Training iter #61100:   Batch Loss = 0.695239, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7574607729911804, Accuracy = 0.5
Training iter #61200:   Batch Loss = 0.717893, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7633633017539978, Accuracy = 0.47826087474823
Training iter #61300:   Batch Loss = 0.738567, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7676050662994385, Accuracy = 0.43478259444236755
Training iter #61400:   Batch Loss = 0.683250, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7735021710395813, Accuracy = 0.4021739065647125
Training iter #61500:   Batch Loss = 0.725763, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7737296223640442, Accuracy = 0.44565218687057495
Training iter #61600:   Batch Loss = 0.665129, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.771995484828949, Accuracy = 0.47826087474823
Training iter #61700:   Batch Loss = 0.742267, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7679374814033508, Accuracy = 0.45652174949645996
Training iter #61800:   Batch Loss = 0.683521, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7621228098869324, Accuracy = 0.43478259444236755
Training iter #61900:   Batch Loss = 0.717836, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7575997114181519, Accuracy = 0.43478259444236755
Training iter #62000:   Batch Loss = 0.713108, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7514663338661194, Accuracy = 0.44565218687057495
Training iter #62100:   Batch Loss = 0.719007, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7459453344345093, Accuracy = 0.489130437374115
Training iter #62200:   Batch Loss = 0.697874, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7434085607528687, Accuracy = 0.5
Training iter #62300:   Batch Loss = 0.700046, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7405738234519958, Accuracy = 0.532608687877655
Training iter #62400:   Batch Loss = 0.679155, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7402478456497192, Accuracy = 0.54347825050354
Training iter #62500:   Batch Loss = 0.734008, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7409864664077759, Accuracy = 0.54347825050354
Training iter #62600:   Batch Loss = 0.674790, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.741651177406311, Accuracy = 0.54347825050354
Training iter #62700:   Batch Loss = 0.698063, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7443782687187195, Accuracy = 0.52173912525177
Training iter #62800:   Batch Loss = 0.711616, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7464704513549805, Accuracy = 0.5
Training iter #62900:   Batch Loss = 0.728072, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7474182844161987, Accuracy = 0.489130437374115
Training iter #63000:   Batch Loss = 0.709358, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7506367564201355, Accuracy = 0.489130437374115
Training iter #63100:   Batch Loss = 0.693441, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7522542476654053, Accuracy = 0.5
Training iter #63200:   Batch Loss = 0.674039, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7491355538368225, Accuracy = 0.489130437374115
Training iter #63300:   Batch Loss = 0.741397, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7495145797729492, Accuracy = 0.489130437374115
Training iter #63400:   Batch Loss = 0.672554, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7485083937644958, Accuracy = 0.510869562625885
Training iter #63500:   Batch Loss = 0.690463, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7493639588356018, Accuracy = 0.532608687877655
Training iter #63600:   Batch Loss = 0.718947, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7495738863945007, Accuracy = 0.532608687877655
Training iter #63700:   Batch Loss = 0.703187, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7500015497207642, Accuracy = 0.532608687877655
Training iter #63800:   Batch Loss = 0.712564, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7513872981071472, Accuracy = 0.510869562625885
Training iter #63900:   Batch Loss = 0.677678, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7513537406921387, Accuracy = 0.52173912525177
Training iter #64000:   Batch Loss = 0.679909, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7516970038414001, Accuracy = 0.52173912525177
Training iter #64100:   Batch Loss = 0.737074, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7524375319480896, Accuracy = 0.52173912525177
Training iter #64200:   Batch Loss = 0.675547, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7519146203994751, Accuracy = 0.54347825050354
Training iter #64300:   Batch Loss = 0.675501, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.753282904624939, Accuracy = 0.54347825050354
Training iter #64400:   Batch Loss = 0.732075, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7534486055374146, Accuracy = 0.532608687877655
Training iter #64500:   Batch Loss = 0.702325, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7535563707351685, Accuracy = 0.54347825050354
Training iter #64600:   Batch Loss = 0.706149, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7550415396690369, Accuracy = 0.52173912525177
Training iter #64700:   Batch Loss = 0.670176, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7545751333236694, Accuracy = 0.52173912525177
Training iter #64800:   Batch Loss = 0.662621, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.754719078540802, Accuracy = 0.54347825050354
Training iter #64900:   Batch Loss = 0.732813, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.754784882068634, Accuracy = 0.54347825050354
Training iter #65000:   Batch Loss = 0.673525, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7538482546806335, Accuracy = 0.54347825050354
Training iter #65100:   Batch Loss = 0.673610, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.755204439163208, Accuracy = 0.54347825050354
Training iter #65200:   Batch Loss = 0.736579, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7553404569625854, Accuracy = 0.52173912525177
Training iter #65300:   Batch Loss = 0.709531, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7548831701278687, Accuracy = 0.54347825050354
Training iter #65400:   Batch Loss = 0.713073, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7556411623954773, Accuracy = 0.54347825050354
Training iter #65500:   Batch Loss = 0.677072, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7550773024559021, Accuracy = 0.54347825050354
Training iter #65600:   Batch Loss = 0.684605, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7550023198127747, Accuracy = 0.54347825050354
Training iter #65700:   Batch Loss = 0.740045, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7552159428596497, Accuracy = 0.54347825050354
Training iter #65800:   Batch Loss = 0.675138, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7538178563117981, Accuracy = 0.54347825050354
Training iter #65900:   Batch Loss = 0.670973, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7549551725387573, Accuracy = 0.532608687877655
Training iter #66000:   Batch Loss = 0.725024, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7558881640434265, Accuracy = 0.52173912525177
Training iter #66100:   Batch Loss = 0.708069, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7551631927490234, Accuracy = 0.54347825050354
Training iter #66200:   Batch Loss = 0.688485, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7554913759231567, Accuracy = 0.54347825050354
Training iter #66300:   Batch Loss = 0.680878, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7543177008628845, Accuracy = 0.532608687877655
Training iter #66400:   Batch Loss = 0.688307, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7544111013412476, Accuracy = 0.532608687877655
Training iter #66500:   Batch Loss = 0.716997, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7554740905761719, Accuracy = 0.54347825050354
Training iter #66600:   Batch Loss = 0.680459, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.754456639289856, Accuracy = 0.54347825050354
Training iter #66700:   Batch Loss = 0.671315, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7548981308937073, Accuracy = 0.54347825050354
Training iter #66800:   Batch Loss = 0.721096, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7552566528320312, Accuracy = 0.52173912525177
Training iter #66900:   Batch Loss = 0.698166, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.754361093044281, Accuracy = 0.54347825050354
Training iter #67000:   Batch Loss = 0.687581, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7548055052757263, Accuracy = 0.54347825050354
Training iter #67100:   Batch Loss = 0.693717, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7531344890594482, Accuracy = 0.54347825050354
Training iter #67200:   Batch Loss = 0.675408, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7590704560279846, Accuracy = 0.52173912525177
Training iter #67300:   Batch Loss = 0.704582, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7603002190589905, Accuracy = 0.532608687877655
Training iter #67400:   Batch Loss = 0.682850, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7517192363739014, Accuracy = 0.54347825050354
Training iter #67500:   Batch Loss = 0.668966, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7543473839759827, Accuracy = 0.54347825050354
Training iter #67600:   Batch Loss = 0.701374, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7541197538375854, Accuracy = 0.532608687877655
Training iter #67700:   Batch Loss = 0.682659, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7541626691818237, Accuracy = 0.532608687877655
Training iter #67800:   Batch Loss = 0.697778, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7549455761909485, Accuracy = 0.532608687877655
Training iter #67900:   Batch Loss = 0.691963, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7541148066520691, Accuracy = 0.532608687877655
Training iter #68000:   Batch Loss = 0.668949, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7541559934616089, Accuracy = 0.532608687877655
Training iter #68100:   Batch Loss = 0.704608, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7549307346343994, Accuracy = 0.532608687877655
Training iter #68200:   Batch Loss = 0.674668, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7535797953605652, Accuracy = 0.532608687877655
Training iter #68300:   Batch Loss = 0.673571, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7557528018951416, Accuracy = 0.532608687877655
Training iter #68400:   Batch Loss = 0.697586, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7560149431228638, Accuracy = 0.532608687877655
Training iter #68500:   Batch Loss = 0.672123, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7564257383346558, Accuracy = 0.532608687877655
Training iter #68600:   Batch Loss = 0.699397, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7574830651283264, Accuracy = 0.532608687877655
Training iter #68700:   Batch Loss = 0.681409, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7565927505493164, Accuracy = 0.532608687877655
Training iter #68800:   Batch Loss = 0.672345, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7569621801376343, Accuracy = 0.532608687877655
Training iter #68900:   Batch Loss = 0.702363, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7577937841415405, Accuracy = 0.52173912525177
Training iter #69000:   Batch Loss = 0.680339, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.756966769695282, Accuracy = 0.532608687877655
Training iter #69100:   Batch Loss = 0.680166, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7585169076919556, Accuracy = 0.52173912525177
Training iter #69200:   Batch Loss = 0.696422, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7580134868621826, Accuracy = 0.52173912525177
Training iter #69300:   Batch Loss = 0.667594, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7576438784599304, Accuracy = 0.532608687877655
Training iter #69400:   Batch Loss = 0.697314, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7591463923454285, Accuracy = 0.532608687877655
Training iter #69500:   Batch Loss = 0.674556, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7584211230278015, Accuracy = 0.532608687877655
Training iter #69600:   Batch Loss = 0.665878, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7593045830726624, Accuracy = 0.532608687877655
Training iter #69700:   Batch Loss = 0.710387, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7589068412780762, Accuracy = 0.532608687877655
Training iter #69800:   Batch Loss = 0.668584, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.754706084728241, Accuracy = 0.532608687877655
Training iter #69900:   Batch Loss = 0.683971, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7557997703552246, Accuracy = 0.532608687877655
Training iter #70000:   Batch Loss = 0.695761, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7578391432762146, Accuracy = 0.532608687877655
Training iter #70100:   Batch Loss = 0.670124, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7589179873466492, Accuracy = 0.52173912525177
Training iter #70200:   Batch Loss = 0.706306, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7593041658401489, Accuracy = 0.532608687877655
Training iter #70300:   Batch Loss = 0.697992, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.758182168006897, Accuracy = 0.532608687877655
Training iter #70400:   Batch Loss = 0.672233, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7588098645210266, Accuracy = 0.532608687877655
Training iter #70500:   Batch Loss = 0.720836, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.755628228187561, Accuracy = 0.532608687877655
Training iter #70600:   Batch Loss = 0.674318, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7545338869094849, Accuracy = 0.532608687877655
Training iter #70700:   Batch Loss = 0.665176, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7552022933959961, Accuracy = 0.532608687877655
Training iter #70800:   Batch Loss = 0.703557, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7544527649879456, Accuracy = 0.532608687877655
Training iter #70900:   Batch Loss = 0.675334, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7547953128814697, Accuracy = 0.532608687877655
Training iter #71000:   Batch Loss = 0.700486, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7544538974761963, Accuracy = 0.532608687877655
Training iter #71100:   Batch Loss = 0.650304, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7542276382446289, Accuracy = 0.532608687877655
Training iter #71200:   Batch Loss = 0.673417, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7558270692825317, Accuracy = 0.532608687877655
Training iter #71300:   Batch Loss = 0.707147, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7547821402549744, Accuracy = 0.532608687877655
Training iter #71400:   Batch Loss = 0.694095, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.753633439540863, Accuracy = 0.532608687877655
Training iter #71500:   Batch Loss = 0.675346, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7545443177223206, Accuracy = 0.532608687877655
Training iter #71600:   Batch Loss = 0.697575, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7540005445480347, Accuracy = 0.532608687877655
Training iter #71700:   Batch Loss = 0.676056, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7533658146858215, Accuracy = 0.532608687877655
Training iter #71800:   Batch Loss = 0.710805, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7573263049125671, Accuracy = 0.532608687877655
Training iter #71900:   Batch Loss = 0.639747, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7578520774841309, Accuracy = 0.532608687877655
Training iter #72000:   Batch Loss = 0.667620, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7619979381561279, Accuracy = 0.532608687877655
Training iter #72100:   Batch Loss = 0.692532, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7596258521080017, Accuracy = 0.532608687877655
Training iter #72200:   Batch Loss = 0.681317, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7552143335342407, Accuracy = 0.532608687877655
Training iter #72300:   Batch Loss = 0.672316, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7553055882453918, Accuracy = 0.54347825050354
Training iter #72400:   Batch Loss = 0.696472, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7511424422264099, Accuracy = 0.54347825050354
Training iter #72500:   Batch Loss = 0.692779, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.758892297744751, Accuracy = 0.532608687877655
Training iter #72600:   Batch Loss = 0.704393, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7587798237800598, Accuracy = 0.5
Training iter #72700:   Batch Loss = 0.663554, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7713920474052429, Accuracy = 0.5
Training iter #72800:   Batch Loss = 0.708099, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7703927755355835, Accuracy = 0.5
Training iter #72900:   Batch Loss = 0.720509, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7618221640586853, Accuracy = 0.510869562625885
Training iter #73000:   Batch Loss = 0.703813, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7468259930610657, Accuracy = 0.510869562625885
Training iter #73100:   Batch Loss = 0.680931, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7443158626556396, Accuracy = 0.510869562625885
Training iter #73200:   Batch Loss = 0.711608, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7417764067649841, Accuracy = 0.52173912525177
Training iter #73300:   Batch Loss = 0.713911, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7418562173843384, Accuracy = 0.532608687877655
Training iter #73400:   Batch Loss = 0.696571, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7394823431968689, Accuracy = 0.532608687877655
Training iter #73500:   Batch Loss = 0.650573, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7421292662620544, Accuracy = 0.532608687877655
Training iter #73600:   Batch Loss = 0.699871, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7449411749839783, Accuracy = 0.532608687877655
Training iter #73700:   Batch Loss = 0.711859, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7426645159721375, Accuracy = 0.52173912525177
Training iter #73800:   Batch Loss = 0.696083, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.74197918176651, Accuracy = 0.532608687877655
Training iter #73900:   Batch Loss = 0.660720, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7459362745285034, Accuracy = 0.532608687877655
Training iter #74000:   Batch Loss = 0.703643, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7444652915000916, Accuracy = 0.532608687877655
Training iter #74100:   Batch Loss = 0.705488, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7412813901901245, Accuracy = 0.54347825050354
Training iter #74200:   Batch Loss = 0.718079, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7500653862953186, Accuracy = 0.54347825050354
Training iter #74300:   Batch Loss = 0.650052, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.750038206577301, Accuracy = 0.54347825050354
Training iter #74400:   Batch Loss = 0.683281, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7520261406898499, Accuracy = 0.54347825050354
Training iter #74500:   Batch Loss = 0.714278, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7529521584510803, Accuracy = 0.54347825050354
Training iter #74600:   Batch Loss = 0.697369, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7536136507987976, Accuracy = 0.54347825050354
Training iter #74700:   Batch Loss = 0.664513, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7590088248252869, Accuracy = 0.54347825050354
Training iter #74800:   Batch Loss = 0.694184, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7562417984008789, Accuracy = 0.54347825050354
Training iter #74900:   Batch Loss = 0.690761, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7518888711929321, Accuracy = 0.54347825050354
Training iter #75000:   Batch Loss = 0.718080, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7593788504600525, Accuracy = 0.54347825050354
Training iter #75100:   Batch Loss = 0.646034, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7580885291099548, Accuracy = 0.54347825050354
Training iter #75200:   Batch Loss = 0.689628, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7614089250564575, Accuracy = 0.52173912525177
Training iter #75300:   Batch Loss = 0.711688, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7561885714530945, Accuracy = 0.54347825050354
Training iter #75400:   Batch Loss = 0.689207, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7540131211280823, Accuracy = 0.554347813129425
Training iter #75500:   Batch Loss = 0.665543, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7576155066490173, Accuracy = 0.554347813129425
Training iter #75600:   Batch Loss = 0.676704, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7491312623023987, Accuracy = 0.54347825050354
Training iter #75700:   Batch Loss = 0.680997, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7451819777488708, Accuracy = 0.54347825050354
Training iter #75800:   Batch Loss = 0.725145, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.757367730140686, Accuracy = 0.489130437374115
Training iter #75900:   Batch Loss = 0.629586, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7617412805557251, Accuracy = 0.532608687877655
Training iter #76000:   Batch Loss = 0.685684, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7614259719848633, Accuracy = 0.52173912525177
Training iter #76100:   Batch Loss = 0.720334, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7560037970542908, Accuracy = 0.554347813129425
Training iter #76200:   Batch Loss = 0.732564, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7290416955947876, Accuracy = 0.5652173757553101
Training iter #76300:   Batch Loss = 0.686088, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7074610590934753, Accuracy = 0.6086956262588501
Training iter #76400:   Batch Loss = 0.658859, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6938861608505249, Accuracy = 0.6413043737411499
Training iter #76500:   Batch Loss = 0.670751, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7383938431739807, Accuracy = 0.41304346919059753
Training iter #76600:   Batch Loss = 0.738111, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7210114002227783, Accuracy = 0.47826087474823
Training iter #76700:   Batch Loss = 0.669962, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6876261830329895, Accuracy = 0.6521739363670349
Training iter #76800:   Batch Loss = 0.685704, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6880165934562683, Accuracy = 0.6413043737411499
Training iter #76900:   Batch Loss = 0.698880, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6972303986549377, Accuracy = 0.6195651888847351
Training iter #77000:   Batch Loss = 0.677991, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7533301115036011, Accuracy = 0.45652174949645996
Training iter #77100:   Batch Loss = 0.701217, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.8124350905418396, Accuracy = 0.43478259444236755
Training iter #77200:   Batch Loss = 0.700116, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7875807285308838, Accuracy = 0.46739131212234497
Training iter #77300:   Batch Loss = 0.670008, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7531861066818237, Accuracy = 0.554347813129425
Training iter #77400:   Batch Loss = 0.725999, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7423919439315796, Accuracy = 0.54347825050354
Training iter #77500:   Batch Loss = 0.649650, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7394254207611084, Accuracy = 0.554347813129425
Training iter #77600:   Batch Loss = 0.652900, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7523771524429321, Accuracy = 0.532608687877655
Training iter #77700:   Batch Loss = 0.736339, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7561166286468506, Accuracy = 0.5
Training iter #77800:   Batch Loss = 0.668452, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7656461000442505, Accuracy = 0.47826087474823
Training iter #77900:   Batch Loss = 0.705942, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7672409415245056, Accuracy = 0.47826087474823
Training iter #78000:   Batch Loss = 0.685216, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7533493638038635, Accuracy = 0.510869562625885
Training iter #78100:   Batch Loss = 0.655126, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7508004903793335, Accuracy = 0.52173912525177
Training iter #78200:   Batch Loss = 0.721071, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7446826100349426, Accuracy = 0.554347813129425
Training iter #78300:   Batch Loss = 0.644675, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7424318194389343, Accuracy = 0.554347813129425
Training iter #78400:   Batch Loss = 0.650432, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7477359175682068, Accuracy = 0.532608687877655
Training iter #78500:   Batch Loss = 0.728940, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7485626339912415, Accuracy = 0.52173912525177
Training iter #78600:   Batch Loss = 0.671550, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7507886290550232, Accuracy = 0.532608687877655
Training iter #78700:   Batch Loss = 0.681027, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7504746317863464, Accuracy = 0.52173912525177
Training iter #78800:   Batch Loss = 0.661042, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.743529200553894, Accuracy = 0.54347825050354
Training iter #78900:   Batch Loss = 0.688259, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7390851974487305, Accuracy = 0.532608687877655
Training iter #79000:   Batch Loss = 0.718407, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7509474754333496, Accuracy = 0.54347825050354
Training iter #79100:   Batch Loss = 0.638126, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.75870281457901, Accuracy = 0.510869562625885
Training iter #79200:   Batch Loss = 0.662501, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7545714378356934, Accuracy = 0.54347825050354
Training iter #79300:   Batch Loss = 0.738248, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7581004500389099, Accuracy = 0.54347825050354
Training iter #79400:   Batch Loss = 0.686189, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7613884210586548, Accuracy = 0.54347825050354
Training iter #79500:   Batch Loss = 0.656497, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7742257118225098, Accuracy = 0.52173912525177
Training iter #79600:   Batch Loss = 0.676981, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7674235701560974, Accuracy = 0.54347825050354
Training iter #79700:   Batch Loss = 0.674249, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7604951858520508, Accuracy = 0.532608687877655
Training iter #79800:   Batch Loss = 0.714683, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7643833756446838, Accuracy = 0.532608687877655
Training iter #79900:   Batch Loss = 0.624448, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7686218619346619, Accuracy = 0.54347825050354
Training iter #80000:   Batch Loss = 0.649532, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7731247544288635, Accuracy = 0.54347825050354
Training iter #80100:   Batch Loss = 0.723594, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.771220326423645, Accuracy = 0.532608687877655
Training iter #80200:   Batch Loss = 0.652438, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7649251818656921, Accuracy = 0.510869562625885
Training iter #80300:   Batch Loss = 0.667435, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7683145999908447, Accuracy = 0.54347825050354
Training iter #80400:   Batch Loss = 0.681773, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7640659809112549, Accuracy = 0.52173912525177
Training iter #80500:   Batch Loss = 0.638330, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7842823266983032, Accuracy = 0.510869562625885
Training iter #80600:   Batch Loss = 0.706434, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.771000325679779, Accuracy = 0.54347825050354
Training iter #80700:   Batch Loss = 0.614613, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.747590184211731, Accuracy = 0.554347813129425
Training iter #80800:   Batch Loss = 0.649203, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7639667987823486, Accuracy = 0.52173912525177
Training iter #80900:   Batch Loss = 0.680535, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7779986262321472, Accuracy = 0.489130437374115
Training iter #81000:   Batch Loss = 0.628903, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7724013328552246, Accuracy = 0.489130437374115
Optimization Finished!
FINAL RESULT: Batch Loss = 0.7724013328552246, Accuracy = 0.489130437374115
