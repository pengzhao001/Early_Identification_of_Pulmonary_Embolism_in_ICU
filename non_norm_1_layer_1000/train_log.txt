Training iter #50:   Batch Loss = 0.876467, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7405622601509094, Accuracy = 0.6304348111152649
Training iter #100:   Batch Loss = 0.919318, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7343010902404785, Accuracy = 0.6304348111152649
Training iter #200:   Batch Loss = 0.788745, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7261391878128052, Accuracy = 0.6304348111152649
Training iter #300:   Batch Loss = 0.798358, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7208968997001648, Accuracy = 0.6304348111152649
Training iter #400:   Batch Loss = 0.796937, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7200927138328552, Accuracy = 0.6304348111152649
Training iter #500:   Batch Loss = 0.762973, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7266577482223511, Accuracy = 0.6304348111152649
Training iter #600:   Batch Loss = 0.756944, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7472039461135864, Accuracy = 0.5652173757553101
Training iter #700:   Batch Loss = 0.759581, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7736563086509705, Accuracy = 0.3804347813129425
Training iter #800:   Batch Loss = 0.737718, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.777827799320221, Accuracy = 0.3586956560611725
Training iter #900:   Batch Loss = 0.738869, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.78119957447052, Accuracy = 0.3695652186870575
Training iter #1000:   Batch Loss = 0.764834, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7780112624168396, Accuracy = 0.3695652186870575
Training iter #1100:   Batch Loss = 0.756845, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7747451066970825, Accuracy = 0.3695652186870575
Training iter #1200:   Batch Loss = 0.743746, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7732109427452087, Accuracy = 0.3695652186870575
Training iter #1300:   Batch Loss = 0.743629, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7704398036003113, Accuracy = 0.3695652186870575
Training iter #1400:   Batch Loss = 0.744409, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7677665948867798, Accuracy = 0.3695652186870575
Training iter #1500:   Batch Loss = 0.757648, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7658381462097168, Accuracy = 0.3695652186870575
Training iter #1600:   Batch Loss = 0.736602, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7632240056991577, Accuracy = 0.3695652186870575
Training iter #1700:   Batch Loss = 0.740973, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7613959312438965, Accuracy = 0.3695652186870575
Training iter #1800:   Batch Loss = 0.756135, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7590461373329163, Accuracy = 0.3695652186870575
Training iter #1900:   Batch Loss = 0.748300, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7568590641021729, Accuracy = 0.3695652186870575
Training iter #2000:   Batch Loss = 0.738764, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7555025815963745, Accuracy = 0.3695652186870575
Training iter #2100:   Batch Loss = 0.743404, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7536240816116333, Accuracy = 0.3695652186870575
Training iter #2200:   Batch Loss = 0.741781, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7519551515579224, Accuracy = 0.3695652186870575
Training iter #2300:   Batch Loss = 0.746620, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7507147192955017, Accuracy = 0.3695652186870575
Training iter #2400:   Batch Loss = 0.738690, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7491663694381714, Accuracy = 0.3695652186870575
Training iter #2500:   Batch Loss = 0.740455, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7481937408447266, Accuracy = 0.3695652186870575
Training iter #2600:   Batch Loss = 0.745547, Accuracy = 0.3799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 0.7469066977500916, Accuracy = 0.3695652186870575
Training iter #2700:   Batch Loss = 0.740224, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7456678152084351, Accuracy = 0.3913043439388275
Training iter #2800:   Batch Loss = 0.738754, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7451726794242859, Accuracy = 0.43478259444236755
Training iter #2900:   Batch Loss = 0.740052, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7440440058708191, Accuracy = 0.43478259444236755
Training iter #3000:   Batch Loss = 0.738381, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7432527542114258, Accuracy = 0.43478259444236755
Training iter #3100:   Batch Loss = 0.740403, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7429090738296509, Accuracy = 0.44565218687057495
Training iter #3200:   Batch Loss = 0.736104, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7421484589576721, Accuracy = 0.44565218687057495
Training iter #3300:   Batch Loss = 0.738859, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7420566082000732, Accuracy = 0.45652174949645996
Training iter #3400:   Batch Loss = 0.739857, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7413505911827087, Accuracy = 0.45652174949645996
Training iter #3500:   Batch Loss = 0.737312, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7406542897224426, Accuracy = 0.47826087474823
Training iter #3600:   Batch Loss = 0.737239, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7408713102340698, Accuracy = 0.45652174949645996
Training iter #3700:   Batch Loss = 0.735492, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7402804493904114, Accuracy = 0.46739131212234497
Training iter #3800:   Batch Loss = 0.735422, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7399420738220215, Accuracy = 0.46739131212234497
Training iter #3900:   Batch Loss = 0.737961, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7401757836341858, Accuracy = 0.44565218687057495
Training iter #4000:   Batch Loss = 0.733338, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7395825386047363, Accuracy = 0.46739131212234497
Training iter #4100:   Batch Loss = 0.735049, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7397693395614624, Accuracy = 0.44565218687057495
Training iter #4200:   Batch Loss = 0.738124, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7389593124389648, Accuracy = 0.45652174949645996
Training iter #4300:   Batch Loss = 0.734174, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7379679083824158, Accuracy = 0.46739131212234497
Training iter #4400:   Batch Loss = 0.730577, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7384049892425537, Accuracy = 0.46739131212234497
Training iter #4500:   Batch Loss = 0.730078, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7370948791503906, Accuracy = 0.489130437374115
Training iter #4600:   Batch Loss = 0.732307, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7362400889396667, Accuracy = 0.5
Training iter #4700:   Batch Loss = 0.733044, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.894950270652771, Accuracy = 0.42391303181648254
Training iter #4800:   Batch Loss = 0.727642, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7356926202774048, Accuracy = 0.510869562625885
Training iter #4900:   Batch Loss = 0.728809, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7374881505966187, Accuracy = 0.47826087474823
Training iter #5000:   Batch Loss = 0.739886, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7366156578063965, Accuracy = 0.47826087474823
Training iter #5100:   Batch Loss = 0.730550, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7358623743057251, Accuracy = 0.46739131212234497
Training iter #5200:   Batch Loss = 0.721516, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7380577325820923, Accuracy = 0.46739131212234497
Training iter #5300:   Batch Loss = 0.727770, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7362609505653381, Accuracy = 0.489130437374115
Training iter #5400:   Batch Loss = 0.727895, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7369441986083984, Accuracy = 0.489130437374115
Training iter #5500:   Batch Loss = 0.730622, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7383115887641907, Accuracy = 0.47826087474823
Training iter #5600:   Batch Loss = 0.722483, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7361594438552856, Accuracy = 0.46739131212234497
Training iter #5700:   Batch Loss = 0.718822, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7371392250061035, Accuracy = 0.45652174949645996
Training iter #5800:   Batch Loss = 0.740848, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7352098822593689, Accuracy = 0.43478259444236755
Training iter #5900:   Batch Loss = 0.734295, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.734679102897644, Accuracy = 0.43478259444236755
Training iter #6000:   Batch Loss = 0.714775, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7368208765983582, Accuracy = 0.43478259444236755
Training iter #6100:   Batch Loss = 0.725168, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.736152172088623, Accuracy = 0.42391303181648254
Training iter #6200:   Batch Loss = 0.730158, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7369602918624878, Accuracy = 0.42391303181648254
Training iter #6300:   Batch Loss = 0.719942, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7391310334205627, Accuracy = 0.42391303181648254
Training iter #6400:   Batch Loss = 0.719008, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7396816611289978, Accuracy = 0.42391303181648254
Training iter #6500:   Batch Loss = 0.707683, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7411845922470093, Accuracy = 0.42391303181648254
Training iter #6600:   Batch Loss = 0.741468, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7395617365837097, Accuracy = 0.42391303181648254
Training iter #6700:   Batch Loss = 0.732708, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7389928698539734, Accuracy = 0.42391303181648254
Training iter #6800:   Batch Loss = 0.704810, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7404135465621948, Accuracy = 0.43478259444236755
Training iter #6900:   Batch Loss = 0.719701, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7385773658752441, Accuracy = 0.41304346919059753
Training iter #7000:   Batch Loss = 0.728994, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7392549514770508, Accuracy = 0.43478259444236755
Training iter #7100:   Batch Loss = 0.715622, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7413488626480103, Accuracy = 0.44565218687057495
Training iter #7200:   Batch Loss = 0.718760, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.741859495639801, Accuracy = 0.43478259444236755
Training iter #7300:   Batch Loss = 0.700998, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7464693784713745, Accuracy = 0.42391303181648254
Training iter #7400:   Batch Loss = 0.732622, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7400478720664978, Accuracy = 0.43478259444236755
Training iter #7500:   Batch Loss = 0.726775, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7323248982429504, Accuracy = 0.489130437374115
Training iter #7600:   Batch Loss = 0.710189, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.737833559513092, Accuracy = 0.46739131212234497
Training iter #7700:   Batch Loss = 0.717973, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7375220060348511, Accuracy = 0.46739131212234497
Training iter #7800:   Batch Loss = 0.722487, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7385246157646179, Accuracy = 0.45652174949645996
Training iter #7900:   Batch Loss = 0.710526, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7433066964149475, Accuracy = 0.44565218687057495
Training iter #8000:   Batch Loss = 0.714848, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7441380023956299, Accuracy = 0.44565218687057495
Training iter #8100:   Batch Loss = 0.696774, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7456998229026794, Accuracy = 0.41304346919059753
Training iter #8200:   Batch Loss = 0.728557, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7408435344696045, Accuracy = 0.45652174949645996
Training iter #8300:   Batch Loss = 0.720676, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7404462099075317, Accuracy = 0.46739131212234497
Training iter #8400:   Batch Loss = 0.700321, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7453603148460388, Accuracy = 0.42391303181648254
Training iter #8500:   Batch Loss = 0.710978, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7379372715950012, Accuracy = 0.46739131212234497
Training iter #8600:   Batch Loss = 0.723410, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7362576723098755, Accuracy = 0.47826087474823
Training iter #8700:   Batch Loss = 0.704536, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7414393424987793, Accuracy = 0.43478259444236755
Training iter #8800:   Batch Loss = 0.711992, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7362203001976013, Accuracy = 0.47826087474823
Training iter #8900:   Batch Loss = 0.691225, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7349485754966736, Accuracy = 0.46739131212234497
Training iter #9000:   Batch Loss = 0.715570, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7321409583091736, Accuracy = 0.5
Training iter #9100:   Batch Loss = 0.716069, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7311028242111206, Accuracy = 0.489130437374115
Training iter #9200:   Batch Loss = 0.702236, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7461744546890259, Accuracy = 0.46739131212234497
Training iter #9300:   Batch Loss = 0.702118, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7151979804039001, Accuracy = 0.5652173757553101
Training iter #9400:   Batch Loss = 0.741703, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7182596921920776, Accuracy = 0.554347813129425
Training iter #9500:   Batch Loss = 0.703744, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7419330477714539, Accuracy = 0.43478259444236755
Training iter #9600:   Batch Loss = 0.707162, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7378814220428467, Accuracy = 0.46739131212234497
Training iter #9700:   Batch Loss = 0.672314, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.735481321811676, Accuracy = 0.46739131212234497
Training iter #9800:   Batch Loss = 0.712888, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7304560542106628, Accuracy = 0.510869562625885
Training iter #9900:   Batch Loss = 0.713742, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7296736240386963, Accuracy = 0.510869562625885
Training iter #10000:   Batch Loss = 0.693264, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7397701740264893, Accuracy = 0.44565218687057495
Training iter #10100:   Batch Loss = 0.693936, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7311839461326599, Accuracy = 0.510869562625885
Training iter #10200:   Batch Loss = 0.716541, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7340320944786072, Accuracy = 0.47826087474823
Training iter #10300:   Batch Loss = 0.701765, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.740606427192688, Accuracy = 0.44565218687057495
Training iter #10400:   Batch Loss = 0.697591, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7331191301345825, Accuracy = 0.47826087474823
Training iter #10500:   Batch Loss = 0.676997, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.736061155796051, Accuracy = 0.44565218687057495
Training iter #10600:   Batch Loss = 0.708941, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7330772876739502, Accuracy = 0.47826087474823
Training iter #10700:   Batch Loss = 0.709630, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.728856086730957, Accuracy = 0.532608687877655
Training iter #10800:   Batch Loss = 0.701883, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7354224324226379, Accuracy = 0.46739131212234497
Training iter #10900:   Batch Loss = 0.690559, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7283779382705688, Accuracy = 0.52173912525177
Training iter #11000:   Batch Loss = 0.712075, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7329962849617004, Accuracy = 0.46739131212234497
Training iter #11100:   Batch Loss = 0.710324, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7374178767204285, Accuracy = 0.44565218687057495
Training iter #11200:   Batch Loss = 0.691400, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7318222522735596, Accuracy = 0.489130437374115
Training iter #11300:   Batch Loss = 0.677327, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7367280721664429, Accuracy = 0.44565218687057495
Training iter #11400:   Batch Loss = 0.701565, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7325263023376465, Accuracy = 0.489130437374115
Training iter #11500:   Batch Loss = 0.709394, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7282156348228455, Accuracy = 0.532608687877655
Training iter #11600:   Batch Loss = 0.692861, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7334501147270203, Accuracy = 0.46739131212234497
Training iter #11700:   Batch Loss = 0.683590, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7259358167648315, Accuracy = 0.52173912525177
Training iter #11800:   Batch Loss = 0.698652, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7309575080871582, Accuracy = 0.5
Training iter #11900:   Batch Loss = 0.705835, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7364466786384583, Accuracy = 0.43478259444236755
Training iter #12000:   Batch Loss = 0.686947, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7298014760017395, Accuracy = 0.52173912525177
Training iter #12100:   Batch Loss = 0.679997, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7360243201255798, Accuracy = 0.44565218687057495
Training iter #12200:   Batch Loss = 0.709518, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7298961877822876, Accuracy = 0.489130437374115
Training iter #12300:   Batch Loss = 0.706783, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7269978523254395, Accuracy = 0.510869562625885
Training iter #12400:   Batch Loss = 0.702124, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7336857318878174, Accuracy = 0.46739131212234497
Training iter #12500:   Batch Loss = 0.676348, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.724716067314148, Accuracy = 0.532608687877655
Training iter #12600:   Batch Loss = 0.680035, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7367078065872192, Accuracy = 0.47826087474823
Training iter #12700:   Batch Loss = 0.687769, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7396913170814514, Accuracy = 0.45652174949645996
Training iter #12800:   Batch Loss = 0.684210, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7298879623413086, Accuracy = 0.489130437374115
Training iter #12900:   Batch Loss = 0.675808, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7353906631469727, Accuracy = 0.47826087474823
Training iter #13000:   Batch Loss = 0.711095, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7334299683570862, Accuracy = 0.47826087474823
Training iter #13100:   Batch Loss = 0.705168, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7306791543960571, Accuracy = 0.5
Training iter #13200:   Batch Loss = 0.698802, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7389786839485168, Accuracy = 0.47826087474823
Training iter #13300:   Batch Loss = 0.669664, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7291347980499268, Accuracy = 0.54347825050354
Training iter #13400:   Batch Loss = 0.668949, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.735693633556366, Accuracy = 0.47826087474823
Training iter #13500:   Batch Loss = 0.689165, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7331351637840271, Accuracy = 0.489130437374115
Training iter #13600:   Batch Loss = 0.671753, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7295280694961548, Accuracy = 0.510869562625885
Training iter #13700:   Batch Loss = 0.674877, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7393470406532288, Accuracy = 0.489130437374115
Training iter #13800:   Batch Loss = 0.704838, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7345296740531921, Accuracy = 0.47826087474823
Training iter #13900:   Batch Loss = 0.697603, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7275869846343994, Accuracy = 0.532608687877655
Training iter #14000:   Batch Loss = 0.699401, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7360367178916931, Accuracy = 0.47826087474823
Training iter #14100:   Batch Loss = 0.676306, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7278425693511963, Accuracy = 0.532608687877655
Training iter #14200:   Batch Loss = 0.664459, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7387171983718872, Accuracy = 0.489130437374115
Training iter #14300:   Batch Loss = 0.680946, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7330068349838257, Accuracy = 0.5
Training iter #14400:   Batch Loss = 0.674284, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7252424955368042, Accuracy = 0.5652173757553101
Training iter #14500:   Batch Loss = 0.674491, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7329543232917786, Accuracy = 0.5
Training iter #14600:   Batch Loss = 0.710518, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7331511378288269, Accuracy = 0.5
Training iter #14700:   Batch Loss = 0.694629, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7288446426391602, Accuracy = 0.532608687877655
Training iter #14800:   Batch Loss = 0.692106, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7355096340179443, Accuracy = 0.47826087474823
Training iter #14900:   Batch Loss = 0.680905, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7272640466690063, Accuracy = 0.532608687877655
Training iter #15000:   Batch Loss = 0.669024, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7341739535331726, Accuracy = 0.489130437374115
Training iter #15100:   Batch Loss = 0.706457, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7296580076217651, Accuracy = 0.532608687877655
Training iter #15200:   Batch Loss = 0.681088, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7247627377510071, Accuracy = 0.5652173757553101
Training iter #15300:   Batch Loss = 0.660139, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7348005771636963, Accuracy = 0.489130437374115
Training iter #15400:   Batch Loss = 0.704471, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7319602966308594, Accuracy = 0.52173912525177
Training iter #15500:   Batch Loss = 0.696499, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.72707599401474, Accuracy = 0.532608687877655
Training iter #15600:   Batch Loss = 0.700432, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7313977479934692, Accuracy = 0.532608687877655
Training iter #15700:   Batch Loss = 0.666364, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7256015539169312, Accuracy = 0.554347813129425
Training iter #15800:   Batch Loss = 0.675870, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7320183515548706, Accuracy = 0.5
Training iter #15900:   Batch Loss = 0.701897, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7289615273475647, Accuracy = 0.532608687877655
Training iter #16000:   Batch Loss = 0.676603, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7231246829032898, Accuracy = 0.5869565010070801
Training iter #16100:   Batch Loss = 0.657511, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7315108180046082, Accuracy = 0.5
Training iter #16200:   Batch Loss = 0.700445, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7277808785438538, Accuracy = 0.532608687877655
Training iter #16300:   Batch Loss = 0.683080, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7262344360351562, Accuracy = 0.54347825050354
Training iter #16400:   Batch Loss = 0.711748, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7353515625, Accuracy = 0.5
Training iter #16500:   Batch Loss = 0.668208, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7275077700614929, Accuracy = 0.532608687877655
Training iter #16600:   Batch Loss = 0.679296, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7320324778556824, Accuracy = 0.5
Training iter #16700:   Batch Loss = 0.696916, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7245904207229614, Accuracy = 0.5652173757553101
Training iter #16800:   Batch Loss = 0.688773, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7203605771064758, Accuracy = 0.5978260636329651
Training iter #16900:   Batch Loss = 0.659683, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7313709259033203, Accuracy = 0.5
Training iter #17000:   Batch Loss = 0.692289, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7301667928695679, Accuracy = 0.532608687877655
Training iter #17100:   Batch Loss = 0.688782, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7266415953636169, Accuracy = 0.54347825050354
Training iter #17200:   Batch Loss = 0.692928, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7335406541824341, Accuracy = 0.5
Training iter #17300:   Batch Loss = 0.674613, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7230506539344788, Accuracy = 0.5652173757553101
Training iter #17400:   Batch Loss = 0.663791, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.726105809211731, Accuracy = 0.554347813129425
Training iter #17500:   Batch Loss = 0.676066, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7227141857147217, Accuracy = 0.5869565010070801
Training iter #17600:   Batch Loss = 0.680896, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7219849228858948, Accuracy = 0.6086956262588501
Training iter #17700:   Batch Loss = 0.657764, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7357022166252136, Accuracy = 0.489130437374115
Training iter #17800:   Batch Loss = 0.685338, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7267647981643677, Accuracy = 0.54347825050354
Training iter #17900:   Batch Loss = 0.693977, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.722125232219696, Accuracy = 0.5869565010070801
Training iter #18000:   Batch Loss = 0.681079, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7273690700531006, Accuracy = 0.532608687877655
Training iter #18100:   Batch Loss = 0.664828, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7232367396354675, Accuracy = 0.5652173757553101
Training iter #18200:   Batch Loss = 0.670608, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7293016910552979, Accuracy = 0.52173912525177
Training iter #18300:   Batch Loss = 0.685824, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.724213182926178, Accuracy = 0.5652173757553101
Training iter #18400:   Batch Loss = 0.685829, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7195485234260559, Accuracy = 0.6086956262588501
Training iter #18500:   Batch Loss = 0.656048, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7287430763244629, Accuracy = 0.532608687877655
Training iter #18600:   Batch Loss = 0.686868, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7235485315322876, Accuracy = 0.5652173757553101
Training iter #18700:   Batch Loss = 0.692625, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7234440445899963, Accuracy = 0.5652173757553101
Training iter #18800:   Batch Loss = 0.662077, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7284818291664124, Accuracy = 0.510869562625885
Training iter #18900:   Batch Loss = 0.657344, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7228392958641052, Accuracy = 0.5652173757553101
Training iter #19000:   Batch Loss = 0.655376, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7268472909927368, Accuracy = 0.554347813129425
Training iter #19100:   Batch Loss = 0.674147, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7205751538276672, Accuracy = 0.5978260636329651
Training iter #19200:   Batch Loss = 0.697618, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7189986705780029, Accuracy = 0.6086956262588501
Training iter #19300:   Batch Loss = 0.676156, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7288469672203064, Accuracy = 0.5
Training iter #19400:   Batch Loss = 0.685205, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7240222096443176, Accuracy = 0.5652173757553101
Training iter #19500:   Batch Loss = 0.691749, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7215760946273804, Accuracy = 0.5869565010070801
Training iter #19600:   Batch Loss = 0.660496, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7240390181541443, Accuracy = 0.554347813129425
Training iter #19700:   Batch Loss = 0.638587, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.721907377243042, Accuracy = 0.5760869383811951
Training iter #19800:   Batch Loss = 0.647777, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7248052954673767, Accuracy = 0.5652173757553101
Training iter #19900:   Batch Loss = 0.685497, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.721095085144043, Accuracy = 0.5978260636329651
Training iter #20000:   Batch Loss = 0.688101, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.719892144203186, Accuracy = 0.5978260636329651
Training iter #20100:   Batch Loss = 0.680848, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7254358530044556, Accuracy = 0.554347813129425
Training iter #20200:   Batch Loss = 0.682471, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7217188477516174, Accuracy = 0.5760869383811951
Training iter #20300:   Batch Loss = 0.699627, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7224164009094238, Accuracy = 0.5652173757553101
Training iter #20400:   Batch Loss = 0.655176, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7240122556686401, Accuracy = 0.554347813129425
Training iter #20500:   Batch Loss = 0.648634, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7204568386077881, Accuracy = 0.5869565010070801
Training iter #20600:   Batch Loss = 0.638415, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7243255972862244, Accuracy = 0.5652173757553101
Training iter #20700:   Batch Loss = 0.684940, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7199151515960693, Accuracy = 0.5978260636329651
Training iter #20800:   Batch Loss = 0.679212, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7200829982757568, Accuracy = 0.5978260636329651
Training iter #20900:   Batch Loss = 0.684046, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7259087562561035, Accuracy = 0.554347813129425
Training iter #21000:   Batch Loss = 0.693974, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7198694348335266, Accuracy = 0.6086956262588501
Training iter #21100:   Batch Loss = 0.687223, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7178217172622681, Accuracy = 0.6086956262588501
Training iter #21200:   Batch Loss = 0.658107, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7199381589889526, Accuracy = 0.5869565010070801
Training iter #21300:   Batch Loss = 0.634734, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7204914093017578, Accuracy = 0.5869565010070801
Training iter #21400:   Batch Loss = 0.652651, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7251808047294617, Accuracy = 0.54347825050354
Training iter #21500:   Batch Loss = 0.685305, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7186739444732666, Accuracy = 0.5978260636329651
Training iter #21600:   Batch Loss = 0.657640, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7185035943984985, Accuracy = 0.5978260636329651
Training iter #21700:   Batch Loss = 0.670703, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7244902849197388, Accuracy = 0.5652173757553101
Training iter #21800:   Batch Loss = 0.666792, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7188364267349243, Accuracy = 0.5978260636329651
Training iter #21900:   Batch Loss = 0.685740, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.718828558921814, Accuracy = 0.5978260636329651
Training iter #22000:   Batch Loss = 0.667126, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7219467163085938, Accuracy = 0.5652173757553101
Training iter #22100:   Batch Loss = 0.651965, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.720147967338562, Accuracy = 0.5760869383811951
Training iter #22200:   Batch Loss = 0.626532, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7214515209197998, Accuracy = 0.5652173757553101
Training iter #22300:   Batch Loss = 0.675725, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7170038819313049, Accuracy = 0.5978260636329651
Training iter #22400:   Batch Loss = 0.660922, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7185677886009216, Accuracy = 0.5978260636329651
Training iter #22500:   Batch Loss = 0.670111, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7257223129272461, Accuracy = 0.532608687877655
Training iter #22600:   Batch Loss = 0.666094, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7164924144744873, Accuracy = 0.5978260636329651
Training iter #22700:   Batch Loss = 0.694710, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7160665988922119, Accuracy = 0.5869565010070801
Training iter #22800:   Batch Loss = 0.658017, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7184176445007324, Accuracy = 0.5978260636329651
Training iter #22900:   Batch Loss = 0.665709, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7182300090789795, Accuracy = 0.5978260636329651
Training iter #23000:   Batch Loss = 0.629902, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7231807112693787, Accuracy = 0.554347813129425
Training iter #23100:   Batch Loss = 0.658545, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7186813950538635, Accuracy = 0.5978260636329651
Training iter #23200:   Batch Loss = 0.652343, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7177630662918091, Accuracy = 0.5869565010070801
Training iter #23300:   Batch Loss = 0.666551, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7213661670684814, Accuracy = 0.5760869383811951
Training iter #23400:   Batch Loss = 0.661979, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7142719626426697, Accuracy = 0.5869565010070801
Training iter #23500:   Batch Loss = 0.691836, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7166101336479187, Accuracy = 0.5869565010070801
Training iter #23600:   Batch Loss = 0.662219, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7225441336631775, Accuracy = 0.54347825050354
Training iter #23700:   Batch Loss = 0.658676, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7179161310195923, Accuracy = 0.6086956262588501
Training iter #23800:   Batch Loss = 0.647565, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7195223569869995, Accuracy = 0.5869565010070801
Training iter #23900:   Batch Loss = 0.656000, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7158579230308533, Accuracy = 0.5869565010070801
Training iter #24000:   Batch Loss = 0.654333, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7137725949287415, Accuracy = 0.5869565010070801
Training iter #24100:   Batch Loss = 0.691235, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7189486026763916, Accuracy = 0.5869565010070801
Training iter #24200:   Batch Loss = 0.662979, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7139519453048706, Accuracy = 0.5869565010070801
Training iter #24300:   Batch Loss = 0.684463, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7172660827636719, Accuracy = 0.5978260636329651
Training iter #24400:   Batch Loss = 0.679519, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7182767987251282, Accuracy = 0.5760869383811951
Training iter #24500:   Batch Loss = 0.660193, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7124291658401489, Accuracy = 0.5978260636329651
Training iter #24600:   Batch Loss = 0.652046, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7169349789619446, Accuracy = 0.5978260636329651
Training iter #24700:   Batch Loss = 0.654769, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7170999646186829, Accuracy = 0.5978260636329651
Training iter #24800:   Batch Loss = 0.661487, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.716765820980072, Accuracy = 0.5978260636329651
Training iter #24900:   Batch Loss = 0.677299, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7208178043365479, Accuracy = 0.5760869383811951
Training iter #25000:   Batch Loss = 0.659173, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7143018245697021, Accuracy = 0.5978260636329651
Training iter #25100:   Batch Loss = 0.666410, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7159852385520935, Accuracy = 0.5869565010070801
Training iter #25200:   Batch Loss = 0.677475, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7184318900108337, Accuracy = 0.5869565010070801
Training iter #25300:   Batch Loss = 0.662568, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7142152786254883, Accuracy = 0.5978260636329651
Training iter #25400:   Batch Loss = 0.660591, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7196107506752014, Accuracy = 0.5652173757553101
Training iter #25500:   Batch Loss = 0.677176, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7164402604103088, Accuracy = 0.5978260636329651
Training iter #25600:   Batch Loss = 0.669129, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7147212624549866, Accuracy = 0.5869565010070801
Training iter #25700:   Batch Loss = 0.688994, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7147852778434753, Accuracy = 0.5869565010070801
Training iter #25800:   Batch Loss = 0.652136, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7102859020233154, Accuracy = 0.5978260636329651
Training iter #25900:   Batch Loss = 0.660868, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7189935445785522, Accuracy = 0.5760869383811951
Training iter #26000:   Batch Loss = 0.651216, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7207233905792236, Accuracy = 0.54347825050354
Training iter #26100:   Batch Loss = 0.662121, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7113991975784302, Accuracy = 0.5978260636329651
Training iter #26200:   Batch Loss = 0.664230, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7116153836250305, Accuracy = 0.6086956262588501
Training iter #26300:   Batch Loss = 0.692699, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.711712658405304, Accuracy = 0.6086956262588501
Training iter #26400:   Batch Loss = 0.684434, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7121872901916504, Accuracy = 0.6086956262588501
Training iter #26500:   Batch Loss = 0.691326, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7183265686035156, Accuracy = 0.5652173757553101
Training iter #26600:   Batch Loss = 0.649260, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7139452695846558, Accuracy = 0.6086956262588501
Training iter #26700:   Batch Loss = 0.654998, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7161351442337036, Accuracy = 0.5978260636329651
Training iter #26800:   Batch Loss = 0.654425, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7147665619850159, Accuracy = 0.6086956262588501
Training iter #26900:   Batch Loss = 0.646718, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7138586640357971, Accuracy = 0.6086956262588501
Training iter #27000:   Batch Loss = 0.661043, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7172660231590271, Accuracy = 0.5760869383811951
Training iter #27100:   Batch Loss = 0.688784, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7158361673355103, Accuracy = 0.5978260636329651
Training iter #27200:   Batch Loss = 0.679794, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7139050364494324, Accuracy = 0.5869565010070801
Training iter #27300:   Batch Loss = 0.689361, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7174950242042542, Accuracy = 0.5869565010070801
Training iter #27400:   Batch Loss = 0.659971, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7130635976791382, Accuracy = 0.5869565010070801
Training iter #27500:   Batch Loss = 0.653192, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7172790765762329, Accuracy = 0.5869565010070801
Training iter #27600:   Batch Loss = 0.647488, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7137788534164429, Accuracy = 0.6086956262588501
Training iter #27700:   Batch Loss = 0.652769, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7091042399406433, Accuracy = 0.6086956262588501
Training iter #27800:   Batch Loss = 0.661183, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7120705246925354, Accuracy = 0.5978260636329651
Training iter #27900:   Batch Loss = 0.702028, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7133625745773315, Accuracy = 0.6086956262588501
Training iter #28000:   Batch Loss = 0.675167, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7133063077926636, Accuracy = 0.5869565010070801
Training iter #28100:   Batch Loss = 0.678021, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7176485657691956, Accuracy = 0.5760869383811951
Training iter #28200:   Batch Loss = 0.666746, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7113485932350159, Accuracy = 0.5978260636329651
Training iter #28300:   Batch Loss = 0.664480, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7142878770828247, Accuracy = 0.5869565010070801
Training iter #28400:   Batch Loss = 0.673601, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7124031782150269, Accuracy = 0.5978260636329651
Training iter #28500:   Batch Loss = 0.664432, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7099800705909729, Accuracy = 0.6086956262588501
Training iter #28600:   Batch Loss = 0.644173, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7155939936637878, Accuracy = 0.5869565010070801
Training iter #28700:   Batch Loss = 0.694321, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7134613394737244, Accuracy = 0.5978260636329651
Training iter #28800:   Batch Loss = 0.687262, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7109994292259216, Accuracy = 0.6195651888847351
Training iter #28900:   Batch Loss = 0.687722, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7126955986022949, Accuracy = 0.5978260636329651
Training iter #29000:   Batch Loss = 0.652365, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7100878357887268, Accuracy = 0.6086956262588501
Training iter #29100:   Batch Loss = 0.673341, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7154380083084106, Accuracy = 0.5760869383811951
Training iter #29200:   Batch Loss = 0.671086, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7129769921302795, Accuracy = 0.5869565010070801
Training iter #29300:   Batch Loss = 0.661787, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7074846029281616, Accuracy = 0.6086956262588501
Training iter #29400:   Batch Loss = 0.644015, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7111797332763672, Accuracy = 0.6086956262588501
Training iter #29500:   Batch Loss = 0.690888, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7110631465911865, Accuracy = 0.5978260636329651
Training iter #29600:   Batch Loss = 0.674633, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7126058340072632, Accuracy = 0.5978260636329651
Training iter #29700:   Batch Loss = 0.700672, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7201324105262756, Accuracy = 0.5652173757553101
Training iter #29800:   Batch Loss = 0.656690, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7109397053718567, Accuracy = 0.6086956262588501
Training iter #29900:   Batch Loss = 0.680809, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7133526802062988, Accuracy = 0.5869565010070801
Training iter #30000:   Batch Loss = 0.663113, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7108908295631409, Accuracy = 0.5978260636329651
Training iter #30100:   Batch Loss = 0.672470, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7099998593330383, Accuracy = 0.6086956262588501
Training iter #30200:   Batch Loss = 0.643636, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7187479138374329, Accuracy = 0.554347813129425
Training iter #30300:   Batch Loss = 0.680492, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7141773700714111, Accuracy = 0.5869565010070801
Training iter #30400:   Batch Loss = 0.682723, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7093632817268372, Accuracy = 0.6086956262588501
Training iter #30500:   Batch Loss = 0.674922, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7135219573974609, Accuracy = 0.5978260636329651
Training iter #30600:   Batch Loss = 0.664040, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7090898752212524, Accuracy = 0.6086956262588501
Training iter #30700:   Batch Loss = 0.662900, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.713017463684082, Accuracy = 0.5869565010070801
Training iter #30800:   Batch Loss = 0.650958, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.710519552230835, Accuracy = 0.6086956262588501
Training iter #30900:   Batch Loss = 0.660646, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7079587578773499, Accuracy = 0.6086956262588501
Training iter #31000:   Batch Loss = 0.646688, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7144184112548828, Accuracy = 0.5760869383811951
Training iter #31100:   Batch Loss = 0.668568, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7093461155891418, Accuracy = 0.6195651888847351
Training iter #31200:   Batch Loss = 0.692032, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7084570527076721, Accuracy = 0.6086956262588501
Training iter #31300:   Batch Loss = 0.664173, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.714535653591156, Accuracy = 0.5869565010070801
Training iter #31400:   Batch Loss = 0.652563, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7099058032035828, Accuracy = 0.6086956262588501
Training iter #31500:   Batch Loss = 0.673878, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7136204242706299, Accuracy = 0.5869565010070801
Training iter #31600:   Batch Loss = 0.662020, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7107632160186768, Accuracy = 0.6086956262588501
Training iter #31700:   Batch Loss = 0.667138, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7080057263374329, Accuracy = 0.6086956262588501
Training iter #31800:   Batch Loss = 0.645353, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7150508165359497, Accuracy = 0.5869565010070801
Training iter #31900:   Batch Loss = 0.671745, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7092551589012146, Accuracy = 0.6086956262588501
Training iter #32000:   Batch Loss = 0.689206, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7082541584968567, Accuracy = 0.6086956262588501
Training iter #32100:   Batch Loss = 0.640943, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7125938534736633, Accuracy = 0.5978260636329651
Training iter #32200:   Batch Loss = 0.638983, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7096025347709656, Accuracy = 0.6195651888847351
Training iter #32300:   Batch Loss = 0.656472, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7143994569778442, Accuracy = 0.5760869383811951
Training iter #32400:   Batch Loss = 0.652677, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7098332047462463, Accuracy = 0.5978260636329651
Training iter #32500:   Batch Loss = 0.679425, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7075687050819397, Accuracy = 0.6086956262588501
Training iter #32600:   Batch Loss = 0.667862, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7131409645080566, Accuracy = 0.5760869383811951
Training iter #32700:   Batch Loss = 0.670835, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7084715962409973, Accuracy = 0.6086956262588501
Training iter #32800:   Batch Loss = 0.686469, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7075462341308594, Accuracy = 0.6086956262588501
Training iter #32900:   Batch Loss = 0.640997, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7112521529197693, Accuracy = 0.5978260636329651
Training iter #33000:   Batch Loss = 0.615633, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7095118761062622, Accuracy = 0.6195651888847351
Training iter #33100:   Batch Loss = 0.647167, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7127544283866882, Accuracy = 0.5869565010070801
Training iter #33200:   Batch Loss = 0.667023, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7095796465873718, Accuracy = 0.6086956262588501
Training iter #33300:   Batch Loss = 0.664151, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7079483270645142, Accuracy = 0.6086956262588501
Training iter #33400:   Batch Loss = 0.672231, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7125642895698547, Accuracy = 0.5869565010070801
Training iter #33500:   Batch Loss = 0.670977, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7095266580581665, Accuracy = 0.6086956262588501
Training iter #33600:   Batch Loss = 0.696249, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7100670337677002, Accuracy = 0.5978260636329651
Training iter #33700:   Batch Loss = 0.633182, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7114086151123047, Accuracy = 0.5978260636329651
Training iter #33800:   Batch Loss = 0.624686, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.708855926990509, Accuracy = 0.6086956262588501
Training iter #33900:   Batch Loss = 0.643737, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7108641862869263, Accuracy = 0.5978260636329651
Training iter #34000:   Batch Loss = 0.664362, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7066769003868103, Accuracy = 0.6195651888847351
Training iter #34100:   Batch Loss = 0.658131, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7085347175598145, Accuracy = 0.5978260636329651
Training iter #34200:   Batch Loss = 0.690122, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7187777757644653, Accuracy = 0.532608687877655
Training iter #34300:   Batch Loss = 0.683035, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7073912024497986, Accuracy = 0.6195651888847351
Training iter #34400:   Batch Loss = 0.684403, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.702591061592102, Accuracy = 0.6086956262588501
Training iter #34500:   Batch Loss = 0.645019, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7039825916290283, Accuracy = 0.6086956262588501
Training iter #34600:   Batch Loss = 0.611074, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.707378625869751, Accuracy = 0.6086956262588501
Training iter #34700:   Batch Loss = 0.669583, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7152169942855835, Accuracy = 0.5652173757553101
Training iter #34800:   Batch Loss = 0.649090, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.709118664264679, Accuracy = 0.6086956262588501
Training iter #34900:   Batch Loss = 0.639762, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7076979279518127, Accuracy = 0.6086956262588501
Training iter #35000:   Batch Loss = 0.685769, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7130728363990784, Accuracy = 0.5978260636329651
Training iter #35100:   Batch Loss = 0.659017, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7115383148193359, Accuracy = 0.5869565010070801
Training iter #35200:   Batch Loss = 0.675878, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.712995707988739, Accuracy = 0.5978260636329651
Training iter #35300:   Batch Loss = 0.651577, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7153106331825256, Accuracy = 0.5869565010070801
Training iter #35400:   Batch Loss = 0.633910, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7102039456367493, Accuracy = 0.5978260636329651
Training iter #35500:   Batch Loss = 0.637028, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7107849717140198, Accuracy = 0.5869565010070801
Training iter #35600:   Batch Loss = 0.638358, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7092781662940979, Accuracy = 0.6086956262588501
Training iter #35700:   Batch Loss = 0.644012, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7117952704429626, Accuracy = 0.5978260636329651
Training iter #35800:   Batch Loss = 0.680300, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.718881368637085, Accuracy = 0.554347813129425
Training iter #35900:   Batch Loss = 0.658913, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7065601348876953, Accuracy = 0.6086956262588501
Training iter #36000:   Batch Loss = 0.692424, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7049290537834167, Accuracy = 0.6195651888847351
Training iter #36100:   Batch Loss = 0.638334, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7079840898513794, Accuracy = 0.6086956262588501
Training iter #36200:   Batch Loss = 0.648502, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7119119167327881, Accuracy = 0.5869565010070801
Training iter #36300:   Batch Loss = 0.616381, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.72129887342453, Accuracy = 0.5652173757553101
Training iter #36400:   Batch Loss = 0.654747, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.711025595664978, Accuracy = 0.5869565010070801
Training iter #36500:   Batch Loss = 0.638240, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.707260012626648, Accuracy = 0.6086956262588501
Training iter #36600:   Batch Loss = 0.662505, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7103464603424072, Accuracy = 0.5869565010070801
Training iter #36700:   Batch Loss = 0.658435, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7096796631813049, Accuracy = 0.6086956262588501
Training iter #36800:   Batch Loss = 0.683043, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7164947390556335, Accuracy = 0.5652173757553101
Training iter #36900:   Batch Loss = 0.656778, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7208437323570251, Accuracy = 0.54347825050354
Training iter #37000:   Batch Loss = 0.640976, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7075450420379639, Accuracy = 0.6086956262588501
Training iter #37100:   Batch Loss = 0.643538, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7072800397872925, Accuracy = 0.5978260636329651
Training iter #37200:   Batch Loss = 0.639579, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7082939147949219, Accuracy = 0.6086956262588501
Training iter #37300:   Batch Loss = 0.637513, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7105802893638611, Accuracy = 0.5869565010070801
Training iter #37400:   Batch Loss = 0.679210, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7159877419471741, Accuracy = 0.5760869383811951
Training iter #37500:   Batch Loss = 0.655789, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7062696814537048, Accuracy = 0.6086956262588501
Training iter #37600:   Batch Loss = 0.682972, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7050439119338989, Accuracy = 0.5978260636329651
Training iter #37700:   Batch Loss = 0.658678, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7060085535049438, Accuracy = 0.6086956262588501
Training iter #37800:   Batch Loss = 0.640472, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.705653727054596, Accuracy = 0.6086956262588501
Training iter #37900:   Batch Loss = 0.639815, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7145527005195618, Accuracy = 0.5869565010070801
Training iter #38000:   Batch Loss = 0.642394, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7088549137115479, Accuracy = 0.5978260636329651
Training iter #38100:   Batch Loss = 0.644631, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7053971290588379, Accuracy = 0.6086956262588501
Training iter #38200:   Batch Loss = 0.662650, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7079470157623291, Accuracy = 0.5978260636329651
Training iter #38300:   Batch Loss = 0.653268, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7071866989135742, Accuracy = 0.6086956262588501
Training iter #38400:   Batch Loss = 0.652596, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7134283781051636, Accuracy = 0.5760869383811951
Training iter #38500:   Batch Loss = 0.677283, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7123059630393982, Accuracy = 0.5869565010070801
Training iter #38600:   Batch Loss = 0.641190, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.702362596988678, Accuracy = 0.5978260636329651
Training iter #38700:   Batch Loss = 0.661094, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7035136818885803, Accuracy = 0.5978260636329651
Training iter #38800:   Batch Loss = 0.660795, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7043972015380859, Accuracy = 0.5978260636329651
Training iter #38900:   Batch Loss = 0.659738, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7082833647727966, Accuracy = 0.6195651888847351
Training iter #39000:   Batch Loss = 0.681857, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7116228342056274, Accuracy = 0.5869565010070801
Training iter #39100:   Batch Loss = 0.634826, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7028314471244812, Accuracy = 0.6086956262588501
Training iter #39200:   Batch Loss = 0.658047, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7041115164756775, Accuracy = 0.6086956262588501
Training iter #39300:   Batch Loss = 0.632691, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7055800557136536, Accuracy = 0.6086956262588501
Training iter #39400:   Batch Loss = 0.642777, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7050284147262573, Accuracy = 0.6086956262588501
Training iter #39500:   Batch Loss = 0.656690, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7105619311332703, Accuracy = 0.5869565010070801
Training iter #39600:   Batch Loss = 0.684560, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7071114182472229, Accuracy = 0.6086956262588501
Training iter #39700:   Batch Loss = 0.672271, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7049562931060791, Accuracy = 0.6086956262588501
Training iter #39800:   Batch Loss = 0.673018, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7091357707977295, Accuracy = 0.5978260636329651
Training iter #39900:   Batch Loss = 0.643551, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7079457640647888, Accuracy = 0.6086956262588501
Training iter #40000:   Batch Loss = 0.645654, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7138433456420898, Accuracy = 0.5869565010070801
Training iter #40100:   Batch Loss = 0.643764, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.707744836807251, Accuracy = 0.5978260636329651
Training iter #40200:   Batch Loss = 0.624883, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7030755281448364, Accuracy = 0.5978260636329651
Training iter #40300:   Batch Loss = 0.656900, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7050623297691345, Accuracy = 0.6086956262588501
Training iter #40400:   Batch Loss = 0.681581, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7057361602783203, Accuracy = 0.5978260636329651
Training iter #40500:   Batch Loss = 0.670681, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7079609036445618, Accuracy = 0.6086956262588501
Training iter #40600:   Batch Loss = 0.677680, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.712651252746582, Accuracy = 0.5869565010070801
Training iter #40700:   Batch Loss = 0.651520, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7049667835235596, Accuracy = 0.6086956262588501
Training iter #40800:   Batch Loss = 0.650406, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7073776721954346, Accuracy = 0.6086956262588501
Training iter #40900:   Batch Loss = 0.632341, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7050001621246338, Accuracy = 0.5978260636329651
Training iter #41000:   Batch Loss = 0.634107, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7025023698806763, Accuracy = 0.5978260636329651
Training iter #41100:   Batch Loss = 0.655648, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7067100405693054, Accuracy = 0.6195651888847351
Training iter #41200:   Batch Loss = 0.700527, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7061725854873657, Accuracy = 0.6195651888847351
Training iter #41300:   Batch Loss = 0.664850, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7050793766975403, Accuracy = 0.6195651888847351
Training iter #41400:   Batch Loss = 0.684257, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7074287533760071, Accuracy = 0.6086956262588501
Training iter #41500:   Batch Loss = 0.672000, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7048474550247192, Accuracy = 0.6195651888847351
Training iter #41600:   Batch Loss = 0.662518, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7113144993782043, Accuracy = 0.5978260636329651
Training iter #41700:   Batch Loss = 0.671339, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7122711539268494, Accuracy = 0.5978260636329651
Training iter #41800:   Batch Loss = 0.654422, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7067862749099731, Accuracy = 0.5978260636329651
Training iter #41900:   Batch Loss = 0.643028, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7078813314437866, Accuracy = 0.5978260636329651
Training iter #42000:   Batch Loss = 0.694219, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7058329582214355, Accuracy = 0.6086956262588501
Training iter #42100:   Batch Loss = 0.683959, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7050614356994629, Accuracy = 0.5978260636329651
Training iter #42200:   Batch Loss = 0.683757, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7080327272415161, Accuracy = 0.6195651888847351
Training iter #42300:   Batch Loss = 0.643838, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7058145403862, Accuracy = 0.6086956262588501
Training iter #42400:   Batch Loss = 0.673003, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7088947892189026, Accuracy = 0.5978260636329651
Training iter #42500:   Batch Loss = 0.656414, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7055647373199463, Accuracy = 0.6086956262588501
Training iter #42600:   Batch Loss = 0.649219, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7016265392303467, Accuracy = 0.6086956262588501
Training iter #42700:   Batch Loss = 0.644470, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7042531371116638, Accuracy = 0.5978260636329651
Training iter #42800:   Batch Loss = 0.690896, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7038511037826538, Accuracy = 0.6086956262588501
Training iter #42900:   Batch Loss = 0.672067, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7048439979553223, Accuracy = 0.6195651888847351
Training iter #43000:   Batch Loss = 0.695288, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7093392014503479, Accuracy = 0.5869565010070801
Training iter #43100:   Batch Loss = 0.650872, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7031407952308655, Accuracy = 0.6195651888847351
Training iter #43200:   Batch Loss = 0.682672, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7064731121063232, Accuracy = 0.6195651888847351
Training iter #43300:   Batch Loss = 0.650193, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7055523991584778, Accuracy = 0.6195651888847351
Training iter #43400:   Batch Loss = 0.657753, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7047447562217712, Accuracy = 0.5978260636329651
Training iter #43500:   Batch Loss = 0.637186, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7128574848175049, Accuracy = 0.5869565010070801
Training iter #43600:   Batch Loss = 0.681631, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7060905694961548, Accuracy = 0.6195651888847351
Training iter #43700:   Batch Loss = 0.681701, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7025391459465027, Accuracy = 0.6086956262588501
Training iter #43800:   Batch Loss = 0.666464, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7050405740737915, Accuracy = 0.6086956262588501
Training iter #43900:   Batch Loss = 0.658568, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7037342190742493, Accuracy = 0.6086956262588501
Training iter #44000:   Batch Loss = 0.661308, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7092176675796509, Accuracy = 0.5978260636329651
Training iter #44100:   Batch Loss = 0.642498, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.704440712928772, Accuracy = 0.6195651888847351
Training iter #44200:   Batch Loss = 0.643953, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7003982067108154, Accuracy = 0.6086956262588501
Training iter #44300:   Batch Loss = 0.639777, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7038503289222717, Accuracy = 0.6195651888847351
Training iter #44400:   Batch Loss = 0.665392, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7015504837036133, Accuracy = 0.6086956262588501
Training iter #44500:   Batch Loss = 0.693334, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7020410299301147, Accuracy = 0.6195651888847351
Training iter #44600:   Batch Loss = 0.655327, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7061915993690491, Accuracy = 0.6086956262588501
Training iter #44700:   Batch Loss = 0.644650, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7014850378036499, Accuracy = 0.6195651888847351
Training iter #44800:   Batch Loss = 0.675944, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7053722739219666, Accuracy = 0.6086956262588501
Training iter #44900:   Batch Loss = 0.650130, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7044503688812256, Accuracy = 0.6413043737411499
Training iter #45000:   Batch Loss = 0.650046, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7029063105583191, Accuracy = 0.6195651888847351
Training iter #45100:   Batch Loss = 0.638032, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7109134793281555, Accuracy = 0.5869565010070801
Training iter #45200:   Batch Loss = 0.669628, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7014758586883545, Accuracy = 0.6086956262588501
Training iter #45300:   Batch Loss = 0.689914, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6993975043296814, Accuracy = 0.6086956262588501
Training iter #45400:   Batch Loss = 0.625389, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7017152905464172, Accuracy = 0.6086956262588501
Training iter #45500:   Batch Loss = 0.625102, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7017031311988831, Accuracy = 0.6195651888847351
Training iter #45600:   Batch Loss = 0.656156, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7085294723510742, Accuracy = 0.5978260636329651
Training iter #45700:   Batch Loss = 0.642653, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.702655017375946, Accuracy = 0.6086956262588501
Training iter #45800:   Batch Loss = 0.663671, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6994596123695374, Accuracy = 0.6304348111152649
Training iter #45900:   Batch Loss = 0.656050, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7035616040229797, Accuracy = 0.6195651888847351
Training iter #46000:   Batch Loss = 0.667761, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7012250423431396, Accuracy = 0.6086956262588501
Training iter #46100:   Batch Loss = 0.683995, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7017481923103333, Accuracy = 0.6195651888847351
Training iter #46200:   Batch Loss = 0.626396, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7030770778656006, Accuracy = 0.6195651888847351
Training iter #46300:   Batch Loss = 0.597983, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7011464238166809, Accuracy = 0.6086956262588501
Training iter #46400:   Batch Loss = 0.648642, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7056201696395874, Accuracy = 0.6086956262588501
Training iter #46500:   Batch Loss = 0.655819, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.703625500202179, Accuracy = 0.6086956262588501
Training iter #46600:   Batch Loss = 0.642584, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7022939920425415, Accuracy = 0.6195651888847351
Training iter #46700:   Batch Loss = 0.664809, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7059385180473328, Accuracy = 0.6086956262588501
Training iter #46800:   Batch Loss = 0.668645, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.700730562210083, Accuracy = 0.6086956262588501
Training iter #46900:   Batch Loss = 0.697706, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.700685977935791, Accuracy = 0.6086956262588501
Training iter #47000:   Batch Loss = 0.616394, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7023491859436035, Accuracy = 0.6195651888847351
Training iter #47100:   Batch Loss = 0.605138, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7018927931785583, Accuracy = 0.6195651888847351
Training iter #47200:   Batch Loss = 0.641162, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7062734365463257, Accuracy = 0.6195651888847351
Training iter #47300:   Batch Loss = 0.654662, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7025597095489502, Accuracy = 0.6304348111152649
Training iter #47400:   Batch Loss = 0.639373, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7023481130599976, Accuracy = 0.6086956262588501
Training iter #47500:   Batch Loss = 0.673527, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.710544228553772, Accuracy = 0.5869565010070801
Training iter #47600:   Batch Loss = 0.679729, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7017329335212708, Accuracy = 0.6086956262588501
Training iter #47700:   Batch Loss = 0.681467, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6985797882080078, Accuracy = 0.6086956262588501
Training iter #47800:   Batch Loss = 0.624791, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6995971202850342, Accuracy = 0.6086956262588501
Training iter #47900:   Batch Loss = 0.592290, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7012309432029724, Accuracy = 0.5869565010070801
Training iter #48000:   Batch Loss = 0.650906, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7100876569747925, Accuracy = 0.5869565010070801
Training iter #48100:   Batch Loss = 0.655938, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7044289112091064, Accuracy = 0.5978260636329651
Training iter #48200:   Batch Loss = 0.624173, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7033268213272095, Accuracy = 0.5978260636329651
Training iter #48300:   Batch Loss = 0.650653, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7069576978683472, Accuracy = 0.6086956262588501
Training iter #48400:   Batch Loss = 0.660033, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7026278376579285, Accuracy = 0.6086956262588501
Training iter #48500:   Batch Loss = 0.674965, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7025617361068726, Accuracy = 0.6086956262588501
Training iter #48600:   Batch Loss = 0.634187, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7043935656547546, Accuracy = 0.6086956262588501
Training iter #48700:   Batch Loss = 0.611919, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7021647095680237, Accuracy = 0.6086956262588501
Training iter #48800:   Batch Loss = 0.616191, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7044521570205688, Accuracy = 0.6195651888847351
Training iter #48900:   Batch Loss = 0.649675, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7018644213676453, Accuracy = 0.6195651888847351
Training iter #49000:   Batch Loss = 0.628985, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7033576965332031, Accuracy = 0.6195651888847351
Training iter #49100:   Batch Loss = 0.642986, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7084916830062866, Accuracy = 0.5978260636329651
Training iter #49200:   Batch Loss = 0.661814, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6985005140304565, Accuracy = 0.5978260636329651
Training iter #49300:   Batch Loss = 0.692540, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6994720697402954, Accuracy = 0.6086956262588501
Training iter #49400:   Batch Loss = 0.623084, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7029203772544861, Accuracy = 0.6304348111152649
Training iter #49500:   Batch Loss = 0.624117, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7019200921058655, Accuracy = 0.6195651888847351
Training iter #49600:   Batch Loss = 0.612297, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7048915028572083, Accuracy = 0.6195651888847351
Training iter #49700:   Batch Loss = 0.631274, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6996615529060364, Accuracy = 0.6086956262588501
Training iter #49800:   Batch Loss = 0.626259, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6985575556755066, Accuracy = 0.6195651888847351
Training iter #49900:   Batch Loss = 0.641765, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7002573013305664, Accuracy = 0.6195651888847351
Training iter #50000:   Batch Loss = 0.662373, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6999861001968384, Accuracy = 0.6195651888847351
Training iter #50100:   Batch Loss = 0.683578, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7084165215492249, Accuracy = 0.5978260636329651
Training iter #50200:   Batch Loss = 0.652420, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7130654454231262, Accuracy = 0.5869565010070801
Training iter #50300:   Batch Loss = 0.625624, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7018121480941772, Accuracy = 0.6195651888847351
Training iter #50400:   Batch Loss = 0.657225, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7042841911315918, Accuracy = 0.6195651888847351
Training iter #50500:   Batch Loss = 0.614615, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7060186862945557, Accuracy = 0.5869565010070801
Training iter #50600:   Batch Loss = 0.626719, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7115603685379028, Accuracy = 0.5978260636329651
Training iter #50700:   Batch Loss = 0.692054, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7164045572280884, Accuracy = 0.5869565010070801
Training iter #50800:   Batch Loss = 0.657706, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7058144211769104, Accuracy = 0.6086956262588501
Training iter #50900:   Batch Loss = 0.687855, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7045524716377258, Accuracy = 0.6195651888847351
Training iter #51000:   Batch Loss = 0.642065, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7028074264526367, Accuracy = 0.5978260636329651
Training iter #51100:   Batch Loss = 0.624697, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7027727365493774, Accuracy = 0.6086956262588501
Training iter #51200:   Batch Loss = 0.650663, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7124925851821899, Accuracy = 0.5760869383811951
Training iter #51300:   Batch Loss = 0.616182, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7042089104652405, Accuracy = 0.5978260636329651
Training iter #51400:   Batch Loss = 0.639198, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7044432163238525, Accuracy = 0.6195651888847351
Training iter #51500:   Batch Loss = 0.674132, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.707568883895874, Accuracy = 0.6195651888847351
Training iter #51600:   Batch Loss = 0.654360, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7075318098068237, Accuracy = 0.6195651888847351
Training iter #51700:   Batch Loss = 0.646417, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7159926891326904, Accuracy = 0.5652173757553101
Training iter #51800:   Batch Loss = 0.673472, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.714352011680603, Accuracy = 0.5978260636329651
Training iter #51900:   Batch Loss = 0.627039, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7059926986694336, Accuracy = 0.6086956262588501
Training iter #52000:   Batch Loss = 0.679065, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7035637497901917, Accuracy = 0.6086956262588501
Training iter #52100:   Batch Loss = 0.640413, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7023105621337891, Accuracy = 0.5978260636329651
Training iter #52200:   Batch Loss = 0.654995, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7059799432754517, Accuracy = 0.6195651888847351
Training iter #52300:   Batch Loss = 0.694993, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7087103724479675, Accuracy = 0.5978260636329651
Training iter #52400:   Batch Loss = 0.632352, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6994152665138245, Accuracy = 0.6195651888847351
Training iter #52500:   Batch Loss = 0.650220, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6988796591758728, Accuracy = 0.6195651888847351
Training iter #52600:   Batch Loss = 0.623391, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.69976806640625, Accuracy = 0.5978260636329651
Training iter #52700:   Batch Loss = 0.624205, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7012420296669006, Accuracy = 0.6086956262588501
Training iter #52800:   Batch Loss = 0.665348, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7084887623786926, Accuracy = 0.6086956262588501
Training iter #52900:   Batch Loss = 0.667038, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7053183317184448, Accuracy = 0.6086956262588501
Training iter #53000:   Batch Loss = 0.670978, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7058489918708801, Accuracy = 0.6195651888847351
Training iter #53100:   Batch Loss = 0.666848, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7078350186347961, Accuracy = 0.6195651888847351
Training iter #53200:   Batch Loss = 0.641620, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7051908373832703, Accuracy = 0.5978260636329651
Training iter #53300:   Batch Loss = 0.637343, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7085868716239929, Accuracy = 0.5869565010070801
Training iter #53400:   Batch Loss = 0.643477, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7020431756973267, Accuracy = 0.6086956262588501
Training iter #53500:   Batch Loss = 0.604671, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6976515650749207, Accuracy = 0.5869565010070801
Training iter #53600:   Batch Loss = 0.648287, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6991235017776489, Accuracy = 0.5869565010070801
Training iter #53700:   Batch Loss = 0.671756, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6998097896575928, Accuracy = 0.5978260636329651
Training iter #53800:   Batch Loss = 0.668274, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7029831409454346, Accuracy = 0.6195651888847351
Training iter #53900:   Batch Loss = 0.666401, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7087055444717407, Accuracy = 0.5869565010070801
Training iter #54000:   Batch Loss = 0.645796, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7021236419677734, Accuracy = 0.5978260636329651
Training iter #54100:   Batch Loss = 0.641382, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7025579810142517, Accuracy = 0.5978260636329651
Training iter #54200:   Batch Loss = 0.625425, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6992673873901367, Accuracy = 0.6086956262588501
Training iter #54300:   Batch Loss = 0.617795, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6960917711257935, Accuracy = 0.5978260636329651
Training iter #54400:   Batch Loss = 0.667219, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6998701691627502, Accuracy = 0.6086956262588501
Training iter #54500:   Batch Loss = 0.687699, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6983808279037476, Accuracy = 0.5978260636329651
Training iter #54600:   Batch Loss = 0.662063, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6980008482933044, Accuracy = 0.6086956262588501
Training iter #54700:   Batch Loss = 0.650395, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7035350799560547, Accuracy = 0.5869565010070801
Training iter #54800:   Batch Loss = 0.655131, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7015186548233032, Accuracy = 0.5869565010070801
Training iter #54900:   Batch Loss = 0.656259, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7071148157119751, Accuracy = 0.5978260636329651
Training iter #55000:   Batch Loss = 0.642069, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7023447155952454, Accuracy = 0.5978260636329651
Training iter #55100:   Batch Loss = 0.634391, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6996817588806152, Accuracy = 0.6086956262588501
Training iter #55200:   Batch Loss = 0.649387, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7005208730697632, Accuracy = 0.6086956262588501
Training iter #55300:   Batch Loss = 0.678372, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7012860178947449, Accuracy = 0.5978260636329651
Training iter #55400:   Batch Loss = 0.676642, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6995788216590881, Accuracy = 0.5978260636329651
Training iter #55500:   Batch Loss = 0.664386, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7010020017623901, Accuracy = 0.5869565010070801
Training iter #55600:   Batch Loss = 0.633812, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6985548734664917, Accuracy = 0.6086956262588501
Training iter #55700:   Batch Loss = 0.669887, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7033479809761047, Accuracy = 0.5978260636329651
Training iter #55800:   Batch Loss = 0.631321, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7012852430343628, Accuracy = 0.6086956262588501
Training iter #55900:   Batch Loss = 0.633058, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6978514790534973, Accuracy = 0.5978260636329651
Training iter #56000:   Batch Loss = 0.642209, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7003096342086792, Accuracy = 0.5869565010070801
Training iter #56100:   Batch Loss = 0.675838, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7003525495529175, Accuracy = 0.5869565010070801
Training iter #56200:   Batch Loss = 0.661969, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7012432217597961, Accuracy = 0.6086956262588501
Training iter #56300:   Batch Loss = 0.678312, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.705612063407898, Accuracy = 0.5978260636329651
Training iter #56400:   Batch Loss = 0.636661, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7004157900810242, Accuracy = 0.5978260636329651
Training iter #56500:   Batch Loss = 0.675565, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7045344710350037, Accuracy = 0.5869565010070801
Training iter #56600:   Batch Loss = 0.625209, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7014220952987671, Accuracy = 0.6195651888847351
Training iter #56700:   Batch Loss = 0.640152, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6981370449066162, Accuracy = 0.5978260636329651
Training iter #56800:   Batch Loss = 0.632154, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7059019804000854, Accuracy = 0.5978260636329651
Training iter #56900:   Batch Loss = 0.661955, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7001417279243469, Accuracy = 0.5869565010070801
Training iter #57000:   Batch Loss = 0.673874, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6990541815757751, Accuracy = 0.6086956262588501
Training iter #57100:   Batch Loss = 0.654561, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7031375765800476, Accuracy = 0.5978260636329651
Training iter #57200:   Batch Loss = 0.644681, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7015416026115417, Accuracy = 0.5978260636329651
Training iter #57300:   Batch Loss = 0.650599, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7067196369171143, Accuracy = 0.6086956262588501
Training iter #57400:   Batch Loss = 0.612761, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.699798047542572, Accuracy = 0.5760869383811951
Training iter #57500:   Batch Loss = 0.628025, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6962083578109741, Accuracy = 0.6086956262588501
Training iter #57600:   Batch Loss = 0.624767, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7012354731559753, Accuracy = 0.6086956262588501
Training iter #57700:   Batch Loss = 0.649674, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6975685358047485, Accuracy = 0.5978260636329651
Training iter #57800:   Batch Loss = 0.685293, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6968182921409607, Accuracy = 0.5869565010070801
Training iter #57900:   Batch Loss = 0.640891, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7008318901062012, Accuracy = 0.5978260636329651
Training iter #58000:   Batch Loss = 0.629257, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7005785703659058, Accuracy = 0.5978260636329651
Training iter #58100:   Batch Loss = 0.665948, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7060911655426025, Accuracy = 0.5978260636329651
Training iter #58200:   Batch Loss = 0.631758, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7013946771621704, Accuracy = 0.5869565010070801
Training iter #58300:   Batch Loss = 0.637802, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6972567439079285, Accuracy = 0.6086956262588501
Training iter #58400:   Batch Loss = 0.622809, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7022237181663513, Accuracy = 0.6086956262588501
Training iter #58500:   Batch Loss = 0.654784, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.697708249092102, Accuracy = 0.5869565010070801
Training iter #58600:   Batch Loss = 0.677923, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6981776356697083, Accuracy = 0.5869565010070801
Training iter #58700:   Batch Loss = 0.617636, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7021719217300415, Accuracy = 0.5978260636329651
Training iter #58800:   Batch Loss = 0.605892, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7010003328323364, Accuracy = 0.5869565010070801
Training iter #58900:   Batch Loss = 0.642621, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.707604706287384, Accuracy = 0.5869565010070801
Training iter #59000:   Batch Loss = 0.615637, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7023799419403076, Accuracy = 0.5978260636329651
Training iter #59100:   Batch Loss = 0.651065, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6986874938011169, Accuracy = 0.5869565010070801
Training iter #59200:   Batch Loss = 0.645210, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7055975198745728, Accuracy = 0.5978260636329651
Training iter #59300:   Batch Loss = 0.655441, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6979619264602661, Accuracy = 0.5869565010070801
Training iter #59400:   Batch Loss = 0.674881, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6980686783790588, Accuracy = 0.6086956262588501
Training iter #59500:   Batch Loss = 0.614653, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7013173699378967, Accuracy = 0.5869565010070801
Training iter #59600:   Batch Loss = 0.576898, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7029147148132324, Accuracy = 0.5978260636329651
Training iter #59700:   Batch Loss = 0.628740, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.707725465297699, Accuracy = 0.5978260636329651
Training iter #59800:   Batch Loss = 0.635389, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7015595436096191, Accuracy = 0.5978260636329651
Training iter #59900:   Batch Loss = 0.627584, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6986343264579773, Accuracy = 0.5760869383811951
Training iter #60000:   Batch Loss = 0.650408, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7040061354637146, Accuracy = 0.6086956262588501
Training iter #60100:   Batch Loss = 0.652938, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6991538405418396, Accuracy = 0.5869565010070801
Training iter #60200:   Batch Loss = 0.690243, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7001643776893616, Accuracy = 0.5869565010070801
Training iter #60300:   Batch Loss = 0.606589, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.702852725982666, Accuracy = 0.5869565010070801
Training iter #60400:   Batch Loss = 0.585612, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7038861513137817, Accuracy = 0.5978260636329651
Training iter #60500:   Batch Loss = 0.616640, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7097253799438477, Accuracy = 0.5869565010070801
Training iter #60600:   Batch Loss = 0.639123, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7013880610466003, Accuracy = 0.5978260636329651
Training iter #60700:   Batch Loss = 0.620889, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6990745663642883, Accuracy = 0.5869565010070801
Training iter #60800:   Batch Loss = 0.651089, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7057207226753235, Accuracy = 0.6086956262588501
Training iter #60900:   Batch Loss = 0.665999, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6996654868125916, Accuracy = 0.5869565010070801
Training iter #61000:   Batch Loss = 0.671550, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7010697722434998, Accuracy = 0.5869565010070801
Training iter #61100:   Batch Loss = 0.614441, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7047455310821533, Accuracy = 0.5760869383811951
Training iter #61200:   Batch Loss = 0.576406, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7044063806533813, Accuracy = 0.5978260636329651
Training iter #61300:   Batch Loss = 0.639944, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7060921788215637, Accuracy = 0.5869565010070801
Training iter #61400:   Batch Loss = 0.626612, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6991996765136719, Accuracy = 0.5978260636329651
Training iter #61500:   Batch Loss = 0.607558, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6996638774871826, Accuracy = 0.5978260636329651
Training iter #61600:   Batch Loss = 0.631336, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7108729481697083, Accuracy = 0.6195651888847351
Training iter #61700:   Batch Loss = 0.658634, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7008941173553467, Accuracy = 0.5652173757553101
Training iter #61800:   Batch Loss = 0.674258, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7032790184020996, Accuracy = 0.5869565010070801
Training iter #61900:   Batch Loss = 0.617602, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7082545161247253, Accuracy = 0.5760869383811951
Training iter #62000:   Batch Loss = 0.597227, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7090094089508057, Accuracy = 0.5869565010070801
Training iter #62100:   Batch Loss = 0.604098, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7083117961883545, Accuracy = 0.5760869383811951
Training iter #62200:   Batch Loss = 0.628600, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7002368569374084, Accuracy = 0.5978260636329651
Training iter #62300:   Batch Loss = 0.615828, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7017036080360413, Accuracy = 0.5760869383811951
Training iter #62400:   Batch Loss = 0.633480, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7102187871932983, Accuracy = 0.6195651888847351
Training iter #62500:   Batch Loss = 0.656194, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6993728280067444, Accuracy = 0.5760869383811951
Training iter #62600:   Batch Loss = 0.679719, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7011924982070923, Accuracy = 0.5869565010070801
Training iter #62700:   Batch Loss = 0.611082, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.705957293510437, Accuracy = 0.5652173757553101
Training iter #62800:   Batch Loss = 0.608897, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7068064212799072, Accuracy = 0.5978260636329651
Training iter #62900:   Batch Loss = 0.601679, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7119795083999634, Accuracy = 0.5760869383811951
Training iter #63000:   Batch Loss = 0.609170, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7013266086578369, Accuracy = 0.5869565010070801
Training iter #63100:   Batch Loss = 0.611797, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7017055153846741, Accuracy = 0.5760869383811951
Training iter #63200:   Batch Loss = 0.631606, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7060829401016235, Accuracy = 0.6086956262588501
Training iter #63300:   Batch Loss = 0.651343, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6988493204116821, Accuracy = 0.5869565010070801
Training iter #63400:   Batch Loss = 0.675116, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7015432119369507, Accuracy = 0.5869565010070801
Training iter #63500:   Batch Loss = 0.626641, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7069675326347351, Accuracy = 0.5978260636329651
Training iter #63600:   Batch Loss = 0.598299, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7000372409820557, Accuracy = 0.5978260636329651
Training iter #63700:   Batch Loss = 0.622799, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7052985429763794, Accuracy = 0.5978260636329651
Training iter #63800:   Batch Loss = 0.608158, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7033975124359131, Accuracy = 0.5869565010070801
Training iter #63900:   Batch Loss = 0.603839, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7042787075042725, Accuracy = 0.6086956262588501
Training iter #64000:   Batch Loss = 0.657420, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7048008441925049, Accuracy = 0.6086956262588501
Training iter #64100:   Batch Loss = 0.653367, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6979672312736511, Accuracy = 0.5869565010070801
Training iter #64200:   Batch Loss = 0.673851, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7036073207855225, Accuracy = 0.6086956262588501
Training iter #64300:   Batch Loss = 0.657138, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7079480290412903, Accuracy = 0.5978260636329651
Training iter #64400:   Batch Loss = 0.603306, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6973520517349243, Accuracy = 0.6086956262588501
Training iter #64500:   Batch Loss = 0.632589, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7008547782897949, Accuracy = 0.5978260636329651
Training iter #64600:   Batch Loss = 0.606569, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7023118734359741, Accuracy = 0.5869565010070801
Training iter #64700:   Batch Loss = 0.614055, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7062140107154846, Accuracy = 0.6086956262588501
Training iter #64800:   Batch Loss = 0.643259, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7077145576477051, Accuracy = 0.6086956262588501
Training iter #64900:   Batch Loss = 0.637800, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7009454965591431, Accuracy = 0.5978260636329651
Training iter #65000:   Batch Loss = 0.654423, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7025831937789917, Accuracy = 0.5760869383811951
Training iter #65100:   Batch Loss = 0.652089, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7065906524658203, Accuracy = 0.5978260636329651
Training iter #65200:   Batch Loss = 0.601048, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6965833306312561, Accuracy = 0.5760869383811951
Training iter #65300:   Batch Loss = 0.643835, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6994560956954956, Accuracy = 0.5869565010070801
Training iter #65400:   Batch Loss = 0.636504, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7002348899841309, Accuracy = 0.5869565010070801
Training iter #65500:   Batch Loss = 0.640486, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.70242840051651, Accuracy = 0.6086956262588501
Training iter #65600:   Batch Loss = 0.664267, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7027817964553833, Accuracy = 0.5869565010070801
Training iter #65700:   Batch Loss = 0.622118, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6986634135246277, Accuracy = 0.5760869383811951
Training iter #65800:   Batch Loss = 0.631430, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.707680344581604, Accuracy = 0.6086956262588501
Training iter #65900:   Batch Loss = 0.629945, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7101185321807861, Accuracy = 0.5760869383811951
Training iter #66000:   Batch Loss = 0.601424, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7002728581428528, Accuracy = 0.5978260636329651
Training iter #66100:   Batch Loss = 0.650313, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7015113830566406, Accuracy = 0.5978260636329651
Training iter #66200:   Batch Loss = 0.659118, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7016177773475647, Accuracy = 0.6086956262588501
Training iter #66300:   Batch Loss = 0.656412, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.700018584728241, Accuracy = 0.6086956262588501
Training iter #66400:   Batch Loss = 0.656363, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.699354887008667, Accuracy = 0.5869565010070801
Training iter #66500:   Batch Loss = 0.620594, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6967881917953491, Accuracy = 0.5869565010070801
Training iter #66600:   Batch Loss = 0.629654, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7043521404266357, Accuracy = 0.6086956262588501
Training iter #66700:   Batch Loss = 0.631388, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7046599388122559, Accuracy = 0.6086956262588501
Training iter #66800:   Batch Loss = 0.573525, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6997750401496887, Accuracy = 0.5869565010070801
Training iter #66900:   Batch Loss = 0.641770, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7008160352706909, Accuracy = 0.5978260636329651
Training iter #67000:   Batch Loss = 0.663958, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6999287605285645, Accuracy = 0.5869565010070801
Training iter #67100:   Batch Loss = 0.643788, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6989601850509644, Accuracy = 0.6086956262588501
Training iter #67200:   Batch Loss = 0.652393, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7000629305839539, Accuracy = 0.5978260636329651
Training iter #67300:   Batch Loss = 0.626770, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6957135200500488, Accuracy = 0.5869565010070801
Training iter #67400:   Batch Loss = 0.627016, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7042315602302551, Accuracy = 0.6086956262588501
Training iter #67500:   Batch Loss = 0.620020, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7016692161560059, Accuracy = 0.5869565010070801
Training iter #67600:   Batch Loss = 0.583144, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.697797417640686, Accuracy = 0.5978260636329651
Training iter #67700:   Batch Loss = 0.640400, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7013682126998901, Accuracy = 0.5869565010070801
Training iter #67800:   Batch Loss = 0.684511, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7039440870285034, Accuracy = 0.5978260636329651
Training iter #67900:   Batch Loss = 0.630933, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.69735187292099, Accuracy = 0.5869565010070801
Training iter #68000:   Batch Loss = 0.644289, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7003893256187439, Accuracy = 0.5869565010070801
Training iter #68100:   Batch Loss = 0.627433, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6988816261291504, Accuracy = 0.5760869383811951
Training iter #68200:   Batch Loss = 0.645621, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7004573345184326, Accuracy = 0.6086956262588501
Training iter #68300:   Batch Loss = 0.629388, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6940706968307495, Accuracy = 0.5760869383811951
Training iter #68400:   Batch Loss = 0.605279, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6943801045417786, Accuracy = 0.5869565010070801
Training iter #68500:   Batch Loss = 0.621203, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7083523869514465, Accuracy = 0.5978260636329651
Training iter #68600:   Batch Loss = 0.670405, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7071803212165833, Accuracy = 0.5978260636329651
Training iter #68700:   Batch Loss = 0.636553, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6957777142524719, Accuracy = 0.5978260636329651
Training iter #68800:   Batch Loss = 0.655811, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6965733766555786, Accuracy = 0.6086956262588501
Training iter #68900:   Batch Loss = 0.616411, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6980445384979248, Accuracy = 0.5869565010070801
Training iter #69000:   Batch Loss = 0.649045, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7109460830688477, Accuracy = 0.6195651888847351
Training iter #69100:   Batch Loss = 0.637984, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7018038034439087, Accuracy = 0.5760869383811951
Training iter #69200:   Batch Loss = 0.606832, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6987017393112183, Accuracy = 0.5978260636329651
Training iter #69300:   Batch Loss = 0.619940, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6986567378044128, Accuracy = 0.5869565010070801
Training iter #69400:   Batch Loss = 0.669302, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7011101841926575, Accuracy = 0.5978260636329651
Training iter #69500:   Batch Loss = 0.635923, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6997031569480896, Accuracy = 0.6086956262588501
Training iter #69600:   Batch Loss = 0.671641, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6996601819992065, Accuracy = 0.5869565010070801
Training iter #69700:   Batch Loss = 0.615050, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6964176297187805, Accuracy = 0.5978260636329651
Training iter #69800:   Batch Loss = 0.650999, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.705111026763916, Accuracy = 0.6086956262588501
Training iter #69900:   Batch Loss = 0.634381, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6961801648139954, Accuracy = 0.5978260636329651
Training iter #70000:   Batch Loss = 0.604391, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6913110017776489, Accuracy = 0.5869565010070801
Training iter #70100:   Batch Loss = 0.607575, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7021366357803345, Accuracy = 0.6086956262588501
Training iter #70200:   Batch Loss = 0.652006, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7034091949462891, Accuracy = 0.6086956262588501
Training iter #70300:   Batch Loss = 0.645131, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6955810785293579, Accuracy = 0.5978260636329651
Training iter #70400:   Batch Loss = 0.661330, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6947540044784546, Accuracy = 0.5760869383811951
Training iter #70500:   Batch Loss = 0.626942, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.694841742515564, Accuracy = 0.5978260636329651
Training iter #70600:   Batch Loss = 0.628175, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7112686634063721, Accuracy = 0.6086956262588501
Training iter #70700:   Batch Loss = 0.614924, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7025822401046753, Accuracy = 0.5869565010070801
Training iter #70800:   Batch Loss = 0.603042, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.698719322681427, Accuracy = 0.5978260636329651
Training iter #70900:   Batch Loss = 0.608178, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7004345655441284, Accuracy = 0.5978260636329651
Training iter #71000:   Batch Loss = 0.643951, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6974967122077942, Accuracy = 0.5978260636329651
Training iter #71100:   Batch Loss = 0.661710, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6935651898384094, Accuracy = 0.5760869383811951
Training iter #71200:   Batch Loss = 0.637721, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6959646940231323, Accuracy = 0.5869565010070801
Training iter #71300:   Batch Loss = 0.596059, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6964694261550903, Accuracy = 0.6195651888847351
Training iter #71400:   Batch Loss = 0.650162, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6922800540924072, Accuracy = 0.5760869383811951
Training iter #71500:   Batch Loss = 0.635497, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6875357627868652, Accuracy = 0.5760869383811951
Training iter #71600:   Batch Loss = 0.621104, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6882880330085754, Accuracy = 0.5760869383811951
Training iter #71700:   Batch Loss = 0.602033, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7040621042251587, Accuracy = 0.5869565010070801
Training iter #71800:   Batch Loss = 0.641962, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7073450684547424, Accuracy = 0.5652173757553101
Training iter #71900:   Batch Loss = 0.651385, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7038260102272034, Accuracy = 0.5978260636329651
Training iter #72000:   Batch Loss = 0.615436, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7013077139854431, Accuracy = 0.5978260636329651
Training iter #72100:   Batch Loss = 0.589592, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6953960657119751, Accuracy = 0.5760869383811951
Training iter #72200:   Batch Loss = 0.620396, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7005446553230286, Accuracy = 0.5760869383811951
Training iter #72300:   Batch Loss = 0.602619, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7002416253089905, Accuracy = 0.5869565010070801
Training iter #72400:   Batch Loss = 0.619222, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7000276446342468, Accuracy = 0.5978260636329651
Training iter #72500:   Batch Loss = 0.628843, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7029204964637756, Accuracy = 0.6086956262588501
Training iter #72600:   Batch Loss = 0.644071, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.693310558795929, Accuracy = 0.5869565010070801
Training iter #72700:   Batch Loss = 0.645182, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6926506757736206, Accuracy = 0.5978260636329651
Training iter #72800:   Batch Loss = 0.609583, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6970952749252319, Accuracy = 0.6195651888847351
Training iter #72900:   Batch Loss = 0.541827, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7017319202423096, Accuracy = 0.6086956262588501
Training iter #73000:   Batch Loss = 0.611397, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6898047924041748, Accuracy = 0.5978260636329651
Training iter #73100:   Batch Loss = 0.635423, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6849984526634216, Accuracy = 0.5978260636329651
Training iter #73200:   Batch Loss = 0.613422, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6872733235359192, Accuracy = 0.5869565010070801
Training iter #73300:   Batch Loss = 0.629357, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7002432942390442, Accuracy = 0.6195651888847351
Training iter #73400:   Batch Loss = 0.642724, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7029334306716919, Accuracy = 0.5869565010070801
Training iter #73500:   Batch Loss = 0.659264, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7045046091079712, Accuracy = 0.5978260636329651
Training iter #73600:   Batch Loss = 0.602871, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7036808729171753, Accuracy = 0.5869565010070801
Training iter #73700:   Batch Loss = 0.571640, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7016589641571045, Accuracy = 0.5978260636329651
Training iter #73800:   Batch Loss = 0.595606, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7036283612251282, Accuracy = 0.5869565010070801
Training iter #73900:   Batch Loss = 0.631068, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6990261077880859, Accuracy = 0.5760869383811951
Training iter #74000:   Batch Loss = 0.597442, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6974681615829468, Accuracy = 0.5760869383811951
Training iter #74100:   Batch Loss = 0.635937, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6998338103294373, Accuracy = 0.6195651888847351
Training iter #74200:   Batch Loss = 0.653347, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6926461458206177, Accuracy = 0.5978260636329651
Training iter #74300:   Batch Loss = 0.634644, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6922364234924316, Accuracy = 0.5978260636329651
Training iter #74400:   Batch Loss = 0.611318, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6922287940979004, Accuracy = 0.5869565010070801
Training iter #74500:   Batch Loss = 0.534742, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6951111555099487, Accuracy = 0.6195651888847351
Training iter #74600:   Batch Loss = 0.632249, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.693873941898346, Accuracy = 0.6195651888847351
Training iter #74700:   Batch Loss = 0.624338, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.689565122127533, Accuracy = 0.5978260636329651
Training iter #74800:   Batch Loss = 0.581594, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6901534199714661, Accuracy = 0.5978260636329651
Training iter #74900:   Batch Loss = 0.614882, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6951186656951904, Accuracy = 0.6304348111152649
Training iter #75000:   Batch Loss = 0.647858, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6899804472923279, Accuracy = 0.5760869383811951
Training iter #75100:   Batch Loss = 0.637928, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6956236362457275, Accuracy = 0.5760869383811951
Training iter #75200:   Batch Loss = 0.612685, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6995522379875183, Accuracy = 0.5869565010070801
Training iter #75300:   Batch Loss = 0.555048, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6987050771713257, Accuracy = 0.5869565010070801
Training iter #75400:   Batch Loss = 0.602619, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6966251134872437, Accuracy = 0.5869565010070801
Training iter #75500:   Batch Loss = 0.626707, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6910102367401123, Accuracy = 0.5978260636329651
Training iter #75600:   Batch Loss = 0.584942, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6962045431137085, Accuracy = 0.6304348111152649
Training iter #75700:   Batch Loss = 0.616848, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7051602602005005, Accuracy = 0.6521739363670349
Training iter #75800:   Batch Loss = 0.639981, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6918652057647705, Accuracy = 0.5869565010070801
Training iter #75900:   Batch Loss = 0.654472, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6927905678749084, Accuracy = 0.5978260636329651
Training iter #76000:   Batch Loss = 0.603037, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6943725347518921, Accuracy = 0.5978260636329651
Training iter #76100:   Batch Loss = 0.577270, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6963282227516174, Accuracy = 0.6304348111152649
Training iter #76200:   Batch Loss = 0.593256, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6931697726249695, Accuracy = 0.5869565010070801
Training iter #76300:   Batch Loss = 0.598871, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6876072287559509, Accuracy = 0.5869565010070801
Training iter #76400:   Batch Loss = 0.582984, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6941601634025574, Accuracy = 0.6195651888847351
Training iter #76500:   Batch Loss = 0.627741, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7039129734039307, Accuracy = 0.6521739363670349
Training iter #76600:   Batch Loss = 0.633921, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6907106041908264, Accuracy = 0.5869565010070801
Training iter #76700:   Batch Loss = 0.653401, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6904475688934326, Accuracy = 0.5978260636329651
Training iter #76800:   Batch Loss = 0.609799, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6947497725486755, Accuracy = 0.6086956262588501
Training iter #76900:   Batch Loss = 0.571319, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6950398683547974, Accuracy = 0.6304348111152649
Training iter #77000:   Batch Loss = 0.603866, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6947484612464905, Accuracy = 0.5978260636329651
Training iter #77100:   Batch Loss = 0.598256, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6899575591087341, Accuracy = 0.5869565010070801
Training iter #77200:   Batch Loss = 0.578305, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6936203241348267, Accuracy = 0.5978260636329651
Training iter #77300:   Batch Loss = 0.646949, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6978083848953247, Accuracy = 0.6413043737411499
Training iter #77400:   Batch Loss = 0.639169, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.690056324005127, Accuracy = 0.5869565010070801
Training iter #77500:   Batch Loss = 0.642420, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6922588348388672, Accuracy = 0.5869565010070801
Training iter #77600:   Batch Loss = 0.634064, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6942026019096375, Accuracy = 0.6086956262588501
Training iter #77700:   Batch Loss = 0.567816, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6934502720832825, Accuracy = 0.5978260636329651
Training iter #77800:   Batch Loss = 0.610120, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6937263607978821, Accuracy = 0.5869565010070801
Training iter #77900:   Batch Loss = 0.589994, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6951391100883484, Accuracy = 0.5869565010070801
Training iter #78000:   Batch Loss = 0.589746, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.694185733795166, Accuracy = 0.5869565010070801
Training iter #78100:   Batch Loss = 0.626506, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6921949982643127, Accuracy = 0.5869565010070801
Training iter #78200:   Batch Loss = 0.631924, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6900390982627869, Accuracy = 0.5760869383811951
Training iter #78300:   Batch Loss = 0.613810, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6945984959602356, Accuracy = 0.5869565010070801
Training iter #78400:   Batch Loss = 0.645615, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6978580355644226, Accuracy = 0.5869565010070801
Training iter #78500:   Batch Loss = 0.570436, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.698826789855957, Accuracy = 0.5978260636329651
Training iter #78600:   Batch Loss = 0.629014, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6942258477210999, Accuracy = 0.5869565010070801
Training iter #78700:   Batch Loss = 0.628428, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6952202916145325, Accuracy = 0.5978260636329651
Training iter #78800:   Batch Loss = 0.609907, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6935445666313171, Accuracy = 0.6086956262588501
Training iter #78900:   Batch Loss = 0.654117, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6965319514274597, Accuracy = 0.5978260636329651
Training iter #79000:   Batch Loss = 0.607249, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6984164714813232, Accuracy = 0.5978260636329651
Training iter #79100:   Batch Loss = 0.610432, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6969664692878723, Accuracy = 0.6304348111152649
Training iter #79200:   Batch Loss = 0.622205, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6937440633773804, Accuracy = 0.6304348111152649
Training iter #79300:   Batch Loss = 0.568863, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6871767640113831, Accuracy = 0.5869565010070801
Training iter #79400:   Batch Loss = 0.631611, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6978023052215576, Accuracy = 0.5978260636329651
Training iter #79500:   Batch Loss = 0.656171, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7055578231811523, Accuracy = 0.6195651888847351
Training iter #79600:   Batch Loss = 0.652262, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6974946856498718, Accuracy = 0.6086956262588501
Training iter #79700:   Batch Loss = 0.638558, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6869842410087585, Accuracy = 0.5760869383811951
Training iter #79800:   Batch Loss = 0.615648, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6852023005485535, Accuracy = 0.5760869383811951
Training iter #79900:   Batch Loss = 0.621203, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6915988922119141, Accuracy = 0.6086956262588501
Training iter #80000:   Batch Loss = 0.620866, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6976426243782043, Accuracy = 0.6086956262588501
Training iter #80100:   Batch Loss = 0.559849, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7010511755943298, Accuracy = 0.5978260636329651
Training iter #80200:   Batch Loss = 0.622889, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7004538774490356, Accuracy = 0.5869565010070801
Training iter #80300:   Batch Loss = 0.658742, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6988879442214966, Accuracy = 0.5869565010070801
Training iter #80400:   Batch Loss = 0.617073, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6972379684448242, Accuracy = 0.6086956262588501
Training iter #80500:   Batch Loss = 0.638885, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6984138488769531, Accuracy = 0.5869565010070801
Training iter #80600:   Batch Loss = 0.608702, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6977798938751221, Accuracy = 0.5760869383811951
Training iter #80700:   Batch Loss = 0.613649, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6951287984848022, Accuracy = 0.6086956262588501
Training iter #80800:   Batch Loss = 0.603737, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6928239464759827, Accuracy = 0.6086956262588501
Training iter #80900:   Batch Loss = 0.567690, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6909905076026917, Accuracy = 0.5760869383811951
Training iter #81000:   Batch Loss = 0.619704, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6955004930496216, Accuracy = 0.5978260636329651
Optimization Finished!
FINAL RESULT: Batch Loss = 0.6955004930496216, Accuracy = 0.5978260636329651
