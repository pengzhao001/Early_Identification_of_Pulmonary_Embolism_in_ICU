Training iter #50:   Batch Loss = 0.760868, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.8339576721191406, Accuracy = 0.3804347813129425
Training iter #100:   Batch Loss = 0.787102, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7870717644691467, Accuracy = 0.41304346919059753
Training iter #200:   Batch Loss = 0.782279, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7382636666297913, Accuracy = 0.5978260636329651
Training iter #300:   Batch Loss = 0.762901, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7605643272399902, Accuracy = 0.5
Training iter #400:   Batch Loss = 0.753316, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7533028721809387, Accuracy = 0.46739131212234497
Training iter #500:   Batch Loss = 0.739471, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7564362287521362, Accuracy = 0.5
Training iter #600:   Batch Loss = 0.767617, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7519317865371704, Accuracy = 0.489130437374115
Training iter #700:   Batch Loss = 0.762913, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7452690005302429, Accuracy = 0.532608687877655
Training iter #800:   Batch Loss = 0.743516, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7472805976867676, Accuracy = 0.52173912525177
Training iter #900:   Batch Loss = 0.749050, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7590373158454895, Accuracy = 0.5
Training iter #1000:   Batch Loss = 0.756921, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.754744827747345, Accuracy = 0.510869562625885
Training iter #1100:   Batch Loss = 0.769166, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7695866823196411, Accuracy = 0.489130437374115
Training iter #1200:   Batch Loss = 0.735721, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7474642395973206, Accuracy = 0.54347825050354
Training iter #1300:   Batch Loss = 0.748244, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.744314968585968, Accuracy = 0.532608687877655
Training iter #1400:   Batch Loss = 0.751436, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7468028664588928, Accuracy = 0.532608687877655
Training iter #1500:   Batch Loss = 0.749381, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7439220547676086, Accuracy = 0.510869562625885
Training iter #1600:   Batch Loss = 0.724577, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7451602220535278, Accuracy = 0.52173912525177
Training iter #1700:   Batch Loss = 0.740164, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7453786134719849, Accuracy = 0.510869562625885
Training iter #1800:   Batch Loss = 0.751841, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.732489824295044, Accuracy = 0.5869565010070801
Training iter #1900:   Batch Loss = 0.735010, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7349709272384644, Accuracy = 0.5760869383811951
Training iter #2000:   Batch Loss = 0.713558, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.732551634311676, Accuracy = 0.5869565010070801
Training iter #2100:   Batch Loss = 0.732491, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7363885045051575, Accuracy = 0.5652173757553101
Training iter #2200:   Batch Loss = 0.743029, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7424691915512085, Accuracy = 0.532608687877655
Training iter #2300:   Batch Loss = 0.741754, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7389085292816162, Accuracy = 0.5
Training iter #2400:   Batch Loss = 0.727060, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7397322058677673, Accuracy = 0.489130437374115
Training iter #2500:   Batch Loss = 0.731978, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7441961765289307, Accuracy = 0.532608687877655
Training iter #2600:   Batch Loss = 0.749237, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7359120845794678, Accuracy = 0.5652173757553101
Training iter #2700:   Batch Loss = 0.730159, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7375407814979553, Accuracy = 0.532608687877655
Training iter #2800:   Batch Loss = 0.708291, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7335214614868164, Accuracy = 0.5869565010070801
Training iter #2900:   Batch Loss = 0.721503, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7359734773635864, Accuracy = 0.54347825050354
Training iter #3000:   Batch Loss = 0.736537, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7405959963798523, Accuracy = 0.510869562625885
Training iter #3100:   Batch Loss = 0.736637, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7365529537200928, Accuracy = 0.532608687877655
Training iter #3200:   Batch Loss = 0.722230, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7365949749946594, Accuracy = 0.532608687877655
Training iter #3300:   Batch Loss = 0.727043, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7410148978233337, Accuracy = 0.510869562625885
Training iter #3400:   Batch Loss = 0.767005, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7117680311203003, Accuracy = 0.554347813129425
Training iter #3500:   Batch Loss = 0.806088, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7132786512374878, Accuracy = 0.6304348111152649
Training iter #3600:   Batch Loss = 0.722228, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6993647217750549, Accuracy = 0.6195651888847351
Training iter #3700:   Batch Loss = 0.771917, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7368695139884949, Accuracy = 0.54347825050354
Training iter #3800:   Batch Loss = 0.745754, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7652897834777832, Accuracy = 0.45652174949645996
Training iter #3900:   Batch Loss = 0.759474, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7799208164215088, Accuracy = 0.46739131212234497
Training iter #4000:   Batch Loss = 0.780932, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7868098020553589, Accuracy = 0.44565218687057495
Training iter #4100:   Batch Loss = 0.730135, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7856884002685547, Accuracy = 0.41304346919059753
Training iter #4200:   Batch Loss = 0.750752, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7760359048843384, Accuracy = 0.44565218687057495
Training iter #4300:   Batch Loss = 0.769427, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7683364152908325, Accuracy = 0.46739131212234497
Training iter #4400:   Batch Loss = 0.774478, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7499721050262451, Accuracy = 0.489130437374115
Training iter #4500:   Batch Loss = 0.757553, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7337188124656677, Accuracy = 0.5978260636329651
Training iter #4600:   Batch Loss = 0.733542, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.720686674118042, Accuracy = 0.6086956262588501
Training iter #4700:   Batch Loss = 0.745591, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.712794840335846, Accuracy = 0.6304348111152649
Training iter #4800:   Batch Loss = 0.737549, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7114656567573547, Accuracy = 0.6304348111152649
Training iter #4900:   Batch Loss = 0.747128, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.711485743522644, Accuracy = 0.6304348111152649
Training iter #5000:   Batch Loss = 0.751844, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.712001383304596, Accuracy = 0.6304348111152649
Training iter #5100:   Batch Loss = 0.736929, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7132230401039124, Accuracy = 0.6304348111152649
Training iter #5200:   Batch Loss = 0.723545, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.715080976486206, Accuracy = 0.6304348111152649
Training iter #5300:   Batch Loss = 0.763952, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 0.7190217971801758, Accuracy = 0.6086956262588501
Training iter #5400:   Batch Loss = 0.724483, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7249715924263, Accuracy = 0.5978260636329651
Training iter #5500:   Batch Loss = 0.745577, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7253578901290894, Accuracy = 0.5978260636329651
Training iter #5600:   Batch Loss = 0.739965, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7271023392677307, Accuracy = 0.6195651888847351
Training iter #5700:   Batch Loss = 0.731176, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7294368147850037, Accuracy = 0.5760869383811951
Training iter #5800:   Batch Loss = 0.742528, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7267242074012756, Accuracy = 0.5869565010070801
Training iter #5900:   Batch Loss = 0.729248, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.726652979850769, Accuracy = 0.6086956262588501
Training iter #6000:   Batch Loss = 0.729922, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.728801965713501, Accuracy = 0.5978260636329651
Training iter #6100:   Batch Loss = 0.752492, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7308357357978821, Accuracy = 0.5652173757553101
Training iter #6200:   Batch Loss = 0.713946, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7487407922744751, Accuracy = 0.5
Training iter #6300:   Batch Loss = 0.743936, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7142041325569153, Accuracy = 0.6304348111152649
Training iter #6400:   Batch Loss = 0.722852, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7162708044052124, Accuracy = 0.6521739363670349
Training iter #6500:   Batch Loss = 0.726650, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7242943644523621, Accuracy = 0.6086956262588501
Training iter #6600:   Batch Loss = 0.739733, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7286670804023743, Accuracy = 0.5869565010070801
Training iter #6700:   Batch Loss = 0.714233, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7458347082138062, Accuracy = 0.5
Training iter #6800:   Batch Loss = 0.743121, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7409901022911072, Accuracy = 0.54347825050354
Training iter #6900:   Batch Loss = 0.742828, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7241482138633728, Accuracy = 0.5760869383811951
Training iter #7000:   Batch Loss = 0.717936, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7161197662353516, Accuracy = 0.6521739363670349
Training iter #7100:   Batch Loss = 0.743513, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7115436792373657, Accuracy = 0.6413043737411499
Training iter #7200:   Batch Loss = 0.722593, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7223764061927795, Accuracy = 0.5978260636329651
Training iter #7300:   Batch Loss = 0.720171, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7364643812179565, Accuracy = 0.554347813129425
Training iter #7400:   Batch Loss = 0.746011, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7381822466850281, Accuracy = 0.5652173757553101
Training iter #7500:   Batch Loss = 0.704362, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7543083429336548, Accuracy = 0.3804347813129425
Training iter #7600:   Batch Loss = 0.754511, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7381514310836792, Accuracy = 0.54347825050354
Training iter #7700:   Batch Loss = 0.747000, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7204365730285645, Accuracy = 0.6195651888847351
Training iter #7800:   Batch Loss = 0.721531, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7169815897941589, Accuracy = 0.6413043737411499
Training iter #7900:   Batch Loss = 0.739461, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7137637734413147, Accuracy = 0.6521739363670349
Training iter #8000:   Batch Loss = 0.724317, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7136456370353699, Accuracy = 0.6521739363670349
Training iter #8100:   Batch Loss = 0.723940, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7150672078132629, Accuracy = 0.6521739363670349
Training iter #8200:   Batch Loss = 0.742491, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7153933644294739, Accuracy = 0.6739130616188049
Training iter #8300:   Batch Loss = 0.728317, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7198136448860168, Accuracy = 0.5978260636329651
Training iter #8400:   Batch Loss = 0.726680, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.723880410194397, Accuracy = 0.5760869383811951
Training iter #8500:   Batch Loss = 0.727051, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7295160293579102, Accuracy = 0.54347825050354
Training iter #8600:   Batch Loss = 0.719017, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7340871095657349, Accuracy = 0.46739131212234497
Training iter #8700:   Batch Loss = 0.735501, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 0.7113556265830994, Accuracy = 0.6195651888847351
Training iter #8800:   Batch Loss = 0.710873, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7302118539810181, Accuracy = 0.45652174949645996
Training iter #8900:   Batch Loss = 0.715160, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7428919076919556, Accuracy = 0.41304346919059753
Training iter #9000:   Batch Loss = 0.744072, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7128833532333374, Accuracy = 0.6413043737411499
Training iter #9100:   Batch Loss = 0.721752, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7113752365112305, Accuracy = 0.6413043737411499
Training iter #9200:   Batch Loss = 0.711224, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7147781252861023, Accuracy = 0.6086956262588501
Training iter #9300:   Batch Loss = 0.735420, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7264916896820068, Accuracy = 0.5652173757553101
Training iter #9400:   Batch Loss = 0.713902, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7415623068809509, Accuracy = 0.3695652186870575
Training iter #9500:   Batch Loss = 0.759488, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7145230174064636, Accuracy = 0.6195651888847351
Training iter #9600:   Batch Loss = 0.720218, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7163625359535217, Accuracy = 0.6304348111152649
Training iter #9700:   Batch Loss = 0.714626, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7346131801605225, Accuracy = 0.45652174949645996
Training iter #9800:   Batch Loss = 0.739033, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7341383695602417, Accuracy = 0.46739131212234497
Training iter #9900:   Batch Loss = 0.701162, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7437445521354675, Accuracy = 0.41304346919059753
Training iter #10000:   Batch Loss = 0.724736, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.73041832447052, Accuracy = 0.43478259444236755
Training iter #10100:   Batch Loss = 0.744108, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7177897691726685, Accuracy = 0.6413043737411499
Training iter #10200:   Batch Loss = 0.716300, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7169589400291443, Accuracy = 0.6413043737411499
Training iter #10300:   Batch Loss = 0.840584, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.9549348950386047, Accuracy = 0.4021739065647125
Training iter #10400:   Batch Loss = 0.814609, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7618626356124878, Accuracy = 0.510869562625885
Training iter #10500:   Batch Loss = 1.440833, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7878257036209106, Accuracy = 0.6304348111152649
Training iter #10600:   Batch Loss = 0.751159, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.8261526226997375, Accuracy = 0.42391303181648254
Training iter #10700:   Batch Loss = 0.731483, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7855213284492493, Accuracy = 0.43478259444236755
Training iter #10800:   Batch Loss = 0.684899, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7572025060653687, Accuracy = 0.6304348111152649
Training iter #10900:   Batch Loss = 0.847019, Accuracy = 0.3799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 0.8134836554527283, Accuracy = 0.3913043439388275
Training iter #11000:   Batch Loss = 0.731122, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7706279158592224, Accuracy = 0.3804347813129425
Training iter #11100:   Batch Loss = 0.737514, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.751639723777771, Accuracy = 0.46739131212234497
Training iter #11200:   Batch Loss = 0.742131, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.75132155418396, Accuracy = 0.489130437374115
Training iter #11300:   Batch Loss = 0.718086, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7432661652565002, Accuracy = 0.47826087474823
Training iter #11400:   Batch Loss = 0.760224, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7359437942504883, Accuracy = 0.532608687877655
Training iter #11500:   Batch Loss = 0.748039, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7318753004074097, Accuracy = 0.52173912525177
Training iter #11600:   Batch Loss = 0.692541, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7274308204650879, Accuracy = 0.5
Training iter #11700:   Batch Loss = 0.754777, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7276330590248108, Accuracy = 0.510869562625885
Training iter #11800:   Batch Loss = 0.734656, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7291026711463928, Accuracy = 0.52173912525177
Training iter #11900:   Batch Loss = 0.752500, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.725170373916626, Accuracy = 0.510869562625885
Training iter #12000:   Batch Loss = 0.726073, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7343470454216003, Accuracy = 0.489130437374115
Training iter #12100:   Batch Loss = 0.711375, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7343122959136963, Accuracy = 0.489130437374115
Training iter #12200:   Batch Loss = 0.737534, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7348501682281494, Accuracy = 0.489130437374115
Training iter #12300:   Batch Loss = 0.738925, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7360424995422363, Accuracy = 0.489130437374115
Training iter #12400:   Batch Loss = 0.683665, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7377784848213196, Accuracy = 0.46739131212234497
Training iter #12500:   Batch Loss = 0.744019, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7521288394927979, Accuracy = 0.46739131212234497
Training iter #12600:   Batch Loss = 0.760775, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7467999458312988, Accuracy = 0.489130437374115
Training iter #12700:   Batch Loss = 0.765147, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.717469334602356, Accuracy = 0.5869565010070801
Training iter #12800:   Batch Loss = 0.709858, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7220075726509094, Accuracy = 0.532608687877655
Training iter #12900:   Batch Loss = 0.694661, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.727160632610321, Accuracy = 0.510869562625885
Training iter #13000:   Batch Loss = 0.732807, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7357353568077087, Accuracy = 0.47826087474823
Training iter #13100:   Batch Loss = 0.954542, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 1.0223768949508667, Accuracy = 0.3695652186870575
Training iter #13200:   Batch Loss = 0.850285, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.887904703617096, Accuracy = 0.3804347813129425
Training iter #13300:   Batch Loss = 0.732303, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7476564049720764, Accuracy = 0.5
Training iter #13400:   Batch Loss = 0.759801, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7292464375495911, Accuracy = 0.5978260636329651
Training iter #13500:   Batch Loss = 0.799716, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7233282923698425, Accuracy = 0.6304348111152649
Training iter #13600:   Batch Loss = 0.737723, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.72533118724823, Accuracy = 0.6304348111152649
Training iter #13700:   Batch Loss = 0.695225, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7267311215400696, Accuracy = 0.6304348111152649
Training iter #13800:   Batch Loss = 0.835081, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7290725708007812, Accuracy = 0.6304348111152649
Training iter #13900:   Batch Loss = 0.719845, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7322001457214355, Accuracy = 0.6304348111152649
Training iter #14000:   Batch Loss = 0.688994, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7340219616889954, Accuracy = 0.6086956262588501
Training iter #14100:   Batch Loss = 0.754623, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7336028814315796, Accuracy = 0.5978260636329651
Training iter #14200:   Batch Loss = 0.767398, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7331458330154419, Accuracy = 0.5869565010070801
Training iter #14300:   Batch Loss = 0.742648, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7356377840042114, Accuracy = 0.6304348111152649
Training iter #14400:   Batch Loss = 0.706916, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7429692149162292, Accuracy = 0.47826087474823
Training iter #14500:   Batch Loss = 0.724062, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7506560683250427, Accuracy = 0.4021739065647125
Training iter #14600:   Batch Loss = 0.711208, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7596868872642517, Accuracy = 0.3913043439388275
Training iter #14700:   Batch Loss = 0.712247, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7675410509109497, Accuracy = 0.41304346919059753
Training iter #14800:   Batch Loss = 0.716903, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7626826167106628, Accuracy = 0.42391303181648254
Training iter #14900:   Batch Loss = 0.728671, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.75636225938797, Accuracy = 0.4021739065647125
Training iter #15000:   Batch Loss = 0.758599, Accuracy = 0.4000000059604645
PERFORMANCE ON TEST SET: Batch Loss = 0.7546862363815308, Accuracy = 0.4021739065647125
Training iter #15100:   Batch Loss = 0.717848, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7497000098228455, Accuracy = 0.3804347813129425
Training iter #15200:   Batch Loss = 0.704687, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7484211921691895, Accuracy = 0.3913043439388275
Training iter #15300:   Batch Loss = 0.727889, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.745613694190979, Accuracy = 0.4021739065647125
Training iter #15400:   Batch Loss = 0.705365, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7440845370292664, Accuracy = 0.4021739065647125
Training iter #15500:   Batch Loss = 0.707786, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7441447973251343, Accuracy = 0.4021739065647125
Training iter #15600:   Batch Loss = 0.693162, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7420512437820435, Accuracy = 0.42391303181648254
Training iter #15700:   Batch Loss = 0.726244, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7411198616027832, Accuracy = 0.42391303181648254
Training iter #15800:   Batch Loss = 0.753392, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7436385154724121, Accuracy = 0.4021739065647125
Training iter #15900:   Batch Loss = 0.716965, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7403653860092163, Accuracy = 0.43478259444236755
Training iter #16000:   Batch Loss = 0.698432, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7501370310783386, Accuracy = 0.4021739065647125
Training iter #16100:   Batch Loss = 0.713781, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7496091723442078, Accuracy = 0.3913043439388275
Training iter #16200:   Batch Loss = 0.692572, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7503877878189087, Accuracy = 0.4021739065647125
Training iter #16300:   Batch Loss = 0.706575, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7529584765434265, Accuracy = 0.3913043439388275
Training iter #16400:   Batch Loss = 0.697000, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7432290315628052, Accuracy = 0.43478259444236755
Training iter #16500:   Batch Loss = 0.722964, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7412828207015991, Accuracy = 0.43478259444236755
Training iter #16600:   Batch Loss = 0.747625, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7487453818321228, Accuracy = 0.42391303181648254
Training iter #16700:   Batch Loss = 0.717155, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7408101558685303, Accuracy = 0.46739131212234497
Training iter #16800:   Batch Loss = 0.695447, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7554824352264404, Accuracy = 0.43478259444236755
Training iter #16900:   Batch Loss = 0.693563, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7450022101402283, Accuracy = 0.45652174949645996
Training iter #17000:   Batch Loss = 0.696931, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7388558983802795, Accuracy = 0.44565218687057495
Training iter #17100:   Batch Loss = 0.708215, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.737897515296936, Accuracy = 0.46739131212234497
Training iter #17200:   Batch Loss = 0.682144, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7330387234687805, Accuracy = 0.46739131212234497
Training iter #17300:   Batch Loss = 0.712893, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7374717593193054, Accuracy = 0.45652174949645996
Training iter #17400:   Batch Loss = 0.737689, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7451789975166321, Accuracy = 0.44565218687057495
Training iter #17500:   Batch Loss = 0.711900, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7407968640327454, Accuracy = 0.44565218687057495
Training iter #17600:   Batch Loss = 0.694950, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7482655048370361, Accuracy = 0.45652174949645996
Training iter #17700:   Batch Loss = 0.672989, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7306995391845703, Accuracy = 0.489130437374115
Training iter #17800:   Batch Loss = 0.711158, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7323795557022095, Accuracy = 0.46739131212234497
Training iter #17900:   Batch Loss = 0.707257, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7418212890625, Accuracy = 0.46739131212234497
Training iter #18000:   Batch Loss = 0.688895, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7393388748168945, Accuracy = 0.46739131212234497
Training iter #18100:   Batch Loss = 0.704672, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7422341704368591, Accuracy = 0.45652174949645996
Training iter #18200:   Batch Loss = 0.731033, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7454434037208557, Accuracy = 0.44565218687057495
Training iter #18300:   Batch Loss = 0.707124, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.742451548576355, Accuracy = 0.44565218687057495
Training iter #18400:   Batch Loss = 0.683463, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7505930662155151, Accuracy = 0.42391303181648254
Training iter #18500:   Batch Loss = 0.647579, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.7420943975448608, Accuracy = 0.45652174949645996
Training iter #18600:   Batch Loss = 0.702632, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.742598831653595, Accuracy = 0.45652174949645996
Training iter #18700:   Batch Loss = 0.710653, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7475899457931519, Accuracy = 0.44565218687057495
Training iter #18800:   Batch Loss = 0.705099, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7392691969871521, Accuracy = 0.5
Training iter #18900:   Batch Loss = 0.699727, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7378324270248413, Accuracy = 0.52173912525177
Training iter #19000:   Batch Loss = 0.701904, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.738231360912323, Accuracy = 0.5
Training iter #19100:   Batch Loss = 0.715998, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.738505482673645, Accuracy = 0.489130437374115
Training iter #19200:   Batch Loss = 0.670056, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7452195286750793, Accuracy = 0.43478259444236755
Training iter #19300:   Batch Loss = 0.642237, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7332633137702942, Accuracy = 0.5
Training iter #19400:   Batch Loss = 0.716146, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7256864309310913, Accuracy = 0.554347813129425
Training iter #19500:   Batch Loss = 0.697943, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7292934656143188, Accuracy = 0.54347825050354
Training iter #19600:   Batch Loss = 0.704900, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7214207053184509, Accuracy = 0.5760869383811951
Training iter #19700:   Batch Loss = 0.687882, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7379552125930786, Accuracy = 0.489130437374115
Training iter #19800:   Batch Loss = 0.648065, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7532694339752197, Accuracy = 0.44565218687057495
Training iter #19900:   Batch Loss = 0.732926, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7066649198532104, Accuracy = 0.6195651888847351
Training iter #20000:   Batch Loss = 0.688291, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7209672331809998, Accuracy = 0.5652173757553101
Training iter #20100:   Batch Loss = 0.633457, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.7342720031738281, Accuracy = 0.510869562625885
Training iter #20200:   Batch Loss = 0.702779, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7519990801811218, Accuracy = 0.41304346919059753
Training iter #20300:   Batch Loss = 0.714002, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7557366490364075, Accuracy = 0.42391303181648254
Training iter #20400:   Batch Loss = 0.703361, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7369006276130676, Accuracy = 0.5
Training iter #20500:   Batch Loss = 0.709536, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.730444073677063, Accuracy = 0.54347825050354
Training iter #20600:   Batch Loss = 0.695340, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7286285758018494, Accuracy = 0.54347825050354
Training iter #20700:   Batch Loss = 0.735336, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7221061587333679, Accuracy = 0.5652173757553101
Training iter #20800:   Batch Loss = 0.682105, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7471403479576111, Accuracy = 0.43478259444236755
Training iter #20900:   Batch Loss = 0.657212, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7543413043022156, Accuracy = 0.43478259444236755
Training iter #21000:   Batch Loss = 0.690104, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7540973424911499, Accuracy = 0.43478259444236755
Training iter #21100:   Batch Loss = 0.721064, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.740710437297821, Accuracy = 0.45652174949645996
Training iter #21200:   Batch Loss = 0.719018, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7199103832244873, Accuracy = 0.5760869383811951
Training iter #21300:   Batch Loss = 0.716444, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7301301956176758, Accuracy = 0.532608687877655
Training iter #21400:   Batch Loss = 0.687079, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7371631860733032, Accuracy = 0.5
Training iter #21500:   Batch Loss = 0.737662, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7380754351615906, Accuracy = 0.510869562625885
Training iter #21600:   Batch Loss = 0.679375, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7506102323532104, Accuracy = 0.44565218687057495
Training iter #21700:   Batch Loss = 0.654757, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.747878909111023, Accuracy = 0.45652174949645996
Training iter #21800:   Batch Loss = 0.718601, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7424006462097168, Accuracy = 0.46739131212234497
Training iter #21900:   Batch Loss = 0.702897, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7352771759033203, Accuracy = 0.510869562625885
Training iter #22000:   Batch Loss = 0.740144, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7241002917289734, Accuracy = 0.5869565010070801
Training iter #22100:   Batch Loss = 0.724563, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7307271361351013, Accuracy = 0.532608687877655
Training iter #22200:   Batch Loss = 0.683782, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7410325407981873, Accuracy = 0.47826087474823
Training iter #22300:   Batch Loss = 0.735870, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7314373850822449, Accuracy = 0.532608687877655
Training iter #22400:   Batch Loss = 0.680810, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7358096837997437, Accuracy = 0.5
Training iter #22500:   Batch Loss = 0.648654, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7368764877319336, Accuracy = 0.5
Training iter #22600:   Batch Loss = 0.734345, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7317160367965698, Accuracy = 0.532608687877655
Training iter #22700:   Batch Loss = 0.698733, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7300866842269897, Accuracy = 0.532608687877655
Training iter #22800:   Batch Loss = 0.757884, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7198498845100403, Accuracy = 0.5978260636329651
Training iter #22900:   Batch Loss = 0.728440, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7258099317550659, Accuracy = 0.5652173757553101
Training iter #23000:   Batch Loss = 0.678014, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7362253069877625, Accuracy = 0.5
Training iter #23100:   Batch Loss = 0.725821, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7236679792404175, Accuracy = 0.5760869383811951
Training iter #23200:   Batch Loss = 0.691418, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.725742518901825, Accuracy = 0.554347813129425
Training iter #23300:   Batch Loss = 0.644449, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7286250591278076, Accuracy = 0.52173912525177
Training iter #23400:   Batch Loss = 0.738261, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7341969013214111, Accuracy = 0.46739131212234497
Training iter #23500:   Batch Loss = 0.706781, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7370052337646484, Accuracy = 0.45652174949645996
Training iter #23600:   Batch Loss = 0.755399, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.720355749130249, Accuracy = 0.5760869383811951
Training iter #23700:   Batch Loss = 0.710422, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7209251523017883, Accuracy = 0.5760869383811951
Training iter #23800:   Batch Loss = 0.687259, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7217676043510437, Accuracy = 0.5652173757553101
Training iter #23900:   Batch Loss = 0.730323, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7208295464515686, Accuracy = 0.5760869383811951
Training iter #24000:   Batch Loss = 0.683660, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7284018397331238, Accuracy = 0.489130437374115
Training iter #24100:   Batch Loss = 0.698736, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.723705530166626, Accuracy = 0.532608687877655
Training iter #24200:   Batch Loss = 0.736637, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7159758806228638, Accuracy = 0.5760869383811951
Training iter #24300:   Batch Loss = 0.703599, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7179363369941711, Accuracy = 0.532608687877655
Training iter #24400:   Batch Loss = 0.757059, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7042709589004517, Accuracy = 0.6195651888847351
Training iter #24500:   Batch Loss = 0.696301, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.724750280380249, Accuracy = 0.510869562625885
Training iter #24600:   Batch Loss = 0.686150, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7311921119689941, Accuracy = 0.52173912525177
Training iter #24700:   Batch Loss = 0.730517, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7171427607536316, Accuracy = 0.5869565010070801
Training iter #24800:   Batch Loss = 0.681752, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7161288857460022, Accuracy = 0.5760869383811951
Training iter #24900:   Batch Loss = 0.677390, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7167670726776123, Accuracy = 0.5760869383811951
Training iter #25000:   Batch Loss = 0.738784, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7218758463859558, Accuracy = 0.554347813129425
Training iter #25100:   Batch Loss = 0.717076, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7297370433807373, Accuracy = 0.510869562625885
Training iter #25200:   Batch Loss = 0.766458, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7125654816627502, Accuracy = 0.5869565010070801
Training iter #25300:   Batch Loss = 0.689084, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.717972993850708, Accuracy = 0.54347825050354
Training iter #25400:   Batch Loss = 0.661424, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7135090827941895, Accuracy = 0.554347813129425
Training iter #25500:   Batch Loss = 0.725926, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7069318890571594, Accuracy = 0.5978260636329651
Training iter #25600:   Batch Loss = 0.694334, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7119984030723572, Accuracy = 0.554347813129425
Training iter #25700:   Batch Loss = 0.712603, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.716626763343811, Accuracy = 0.5652173757553101
Training iter #25800:   Batch Loss = 0.731707, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7114383578300476, Accuracy = 0.5652173757553101
Training iter #25900:   Batch Loss = 0.699726, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7155292630195618, Accuracy = 0.554347813129425
Training iter #26000:   Batch Loss = 0.773120, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.6999648213386536, Accuracy = 0.6304348111152649
Training iter #26100:   Batch Loss = 0.691462, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7236724495887756, Accuracy = 0.52173912525177
Training iter #26200:   Batch Loss = 0.661839, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7256505489349365, Accuracy = 0.510869562625885
Training iter #26300:   Batch Loss = 0.717463, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7152847051620483, Accuracy = 0.5760869383811951
Training iter #26400:   Batch Loss = 0.691327, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7136330008506775, Accuracy = 0.5652173757553101
Training iter #26500:   Batch Loss = 0.687386, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7137869596481323, Accuracy = 0.5760869383811951
Training iter #26600:   Batch Loss = 0.722813, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7166109085083008, Accuracy = 0.5869565010070801
Training iter #26700:   Batch Loss = 0.699468, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7269684672355652, Accuracy = 0.52173912525177
Training iter #26800:   Batch Loss = 0.766611, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7153338193893433, Accuracy = 0.554347813129425
Training iter #26900:   Batch Loss = 0.699427, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7179340124130249, Accuracy = 0.532608687877655
Training iter #27000:   Batch Loss = 0.633804, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.7100086212158203, Accuracy = 0.5978260636329651
Training iter #27100:   Batch Loss = 0.728099, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7077297568321228, Accuracy = 0.5869565010070801
Training iter #27200:   Batch Loss = 0.681179, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7129661440849304, Accuracy = 0.554347813129425
Training iter #27300:   Batch Loss = 0.692938, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7128022313117981, Accuracy = 0.554347813129425
Training iter #27400:   Batch Loss = 0.703049, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7087273597717285, Accuracy = 0.554347813129425
Training iter #27500:   Batch Loss = 0.676758, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7205720543861389, Accuracy = 0.532608687877655
Training iter #27600:   Batch Loss = 0.768456, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.705315887928009, Accuracy = 0.5652173757553101
Training iter #27700:   Batch Loss = 0.686989, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.715550422668457, Accuracy = 0.510869562625885
Training iter #27800:   Batch Loss = 0.620880, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.710017740726471, Accuracy = 0.554347813129425
Training iter #27900:   Batch Loss = 0.718433, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7059999704360962, Accuracy = 0.5869565010070801
Training iter #28000:   Batch Loss = 0.672274, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7106397747993469, Accuracy = 0.554347813129425
Training iter #28100:   Batch Loss = 0.696362, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.708541750907898, Accuracy = 0.5652173757553101
Training iter #28200:   Batch Loss = 0.697722, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7068585157394409, Accuracy = 0.554347813129425
Training iter #28300:   Batch Loss = 0.665452, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7180527448654175, Accuracy = 0.54347825050354
Training iter #28400:   Batch Loss = 0.750852, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7051815390586853, Accuracy = 0.5652173757553101
Training iter #28500:   Batch Loss = 0.691640, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7088860273361206, Accuracy = 0.554347813129425
Training iter #28600:   Batch Loss = 0.613398, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.7025771141052246, Accuracy = 0.5652173757553101
Training iter #28700:   Batch Loss = 0.706386, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7023735642433167, Accuracy = 0.5652173757553101
Training iter #28800:   Batch Loss = 0.661640, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.717069149017334, Accuracy = 0.532608687877655
Training iter #28900:   Batch Loss = 0.705629, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7065503001213074, Accuracy = 0.5652173757553101
Training iter #29000:   Batch Loss = 0.704320, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7020660638809204, Accuracy = 0.6086956262588501
Training iter #29100:   Batch Loss = 0.662246, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7102998495101929, Accuracy = 0.532608687877655
Training iter #29200:   Batch Loss = 0.732699, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7091372609138489, Accuracy = 0.5760869383811951
Training iter #29300:   Batch Loss = 0.673901, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7193791270256042, Accuracy = 0.532608687877655
Training iter #29400:   Batch Loss = 0.619994, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7061746716499329, Accuracy = 0.5760869383811951
Training iter #29500:   Batch Loss = 0.695273, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6998263597488403, Accuracy = 0.6086956262588501
Training iter #29600:   Batch Loss = 0.640517, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7066147327423096, Accuracy = 0.5760869383811951
Training iter #29700:   Batch Loss = 0.693858, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7056898474693298, Accuracy = 0.5869565010070801
Training iter #29800:   Batch Loss = 0.694702, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7048137784004211, Accuracy = 0.5869565010070801
Training iter #29900:   Batch Loss = 0.639570, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7145512104034424, Accuracy = 0.532608687877655
Training iter #30000:   Batch Loss = 0.740860, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7003275752067566, Accuracy = 0.5978260636329651
Training iter #30100:   Batch Loss = 0.657927, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7055723071098328, Accuracy = 0.5760869383811951
Training iter #30200:   Batch Loss = 0.603673, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 0.6983988881111145, Accuracy = 0.6195651888847351
Training iter #30300:   Batch Loss = 0.692735, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7237171530723572, Accuracy = 0.510869562625885
Training iter #30400:   Batch Loss = 0.658386, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7198183536529541, Accuracy = 0.5652173757553101
Training iter #30500:   Batch Loss = 0.677301, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6941515803337097, Accuracy = 0.6413043737411499
Training iter #30600:   Batch Loss = 0.689237, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7140220999717712, Accuracy = 0.5760869383811951
Training iter #30700:   Batch Loss = 0.658122, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7459471821784973, Accuracy = 0.45652174949645996
Training iter #30800:   Batch Loss = 0.715127, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.694416344165802, Accuracy = 0.6304348111152649
Training iter #30900:   Batch Loss = 0.658032, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.708810567855835, Accuracy = 0.5869565010070801
Training iter #31000:   Batch Loss = 0.620652, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 0.7030580639839172, Accuracy = 0.6195651888847351
Training iter #31100:   Batch Loss = 0.718890, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7024639844894409, Accuracy = 0.6195651888847351
Training iter #31200:   Batch Loss = 0.645171, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7152330875396729, Accuracy = 0.554347813129425
Training iter #31300:   Batch Loss = 0.692912, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7027037739753723, Accuracy = 0.6304348111152649
Training iter #31400:   Batch Loss = 0.692239, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6978611350059509, Accuracy = 0.6195651888847351
Training iter #31500:   Batch Loss = 0.644074, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7054689526557922, Accuracy = 0.6086956262588501
Training iter #31600:   Batch Loss = 0.692339, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.692211389541626, Accuracy = 0.6195651888847351
Training iter #31700:   Batch Loss = 0.668641, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7068521976470947, Accuracy = 0.5760869383811951
Training iter #31800:   Batch Loss = 0.685486, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7263303995132446, Accuracy = 0.5652173757553101
Training iter #31900:   Batch Loss = 0.744971, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7038915157318115, Accuracy = 0.6413043737411499
Training iter #32000:   Batch Loss = 0.662834, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7213276624679565, Accuracy = 0.554347813129425
Training iter #32100:   Batch Loss = 0.713150, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7124265432357788, Accuracy = 0.5978260636329651
Training iter #32200:   Batch Loss = 0.702876, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7142856121063232, Accuracy = 0.5652173757553101
Training iter #32300:   Batch Loss = 0.636012, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.725782036781311, Accuracy = 0.54347825050354
Training iter #32400:   Batch Loss = 0.700977, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7205975651741028, Accuracy = 0.54347825050354
Training iter #32500:   Batch Loss = 0.650169, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7187860012054443, Accuracy = 0.54347825050354
Training iter #32600:   Batch Loss = 0.618925, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7004053592681885, Accuracy = 0.6521739363670349
Training iter #32700:   Batch Loss = 0.734497, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.6943376064300537, Accuracy = 0.6413043737411499
Training iter #32800:   Batch Loss = 0.644683, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7002190947532654, Accuracy = 0.6413043737411499
Training iter #32900:   Batch Loss = 0.709780, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6962571144104004, Accuracy = 0.6195651888847351
Training iter #33000:   Batch Loss = 0.691656, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.705835223197937, Accuracy = 0.6195651888847351
Training iter #33100:   Batch Loss = 0.607951, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7146421074867249, Accuracy = 0.554347813129425
Training iter #33200:   Batch Loss = 0.711769, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6913800835609436, Accuracy = 0.6413043737411499
Training iter #33300:   Batch Loss = 0.665475, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6969388723373413, Accuracy = 0.6521739363670349
Training iter #33400:   Batch Loss = 0.603092, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 0.7004761099815369, Accuracy = 0.6413043737411499
Training iter #33500:   Batch Loss = 0.695649, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7119401693344116, Accuracy = 0.54347825050354
Training iter #33600:   Batch Loss = 0.655272, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7129864692687988, Accuracy = 0.54347825050354
Training iter #33700:   Batch Loss = 0.706611, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6944714784622192, Accuracy = 0.6413043737411499
Training iter #33800:   Batch Loss = 0.694488, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.69608074426651, Accuracy = 0.6304348111152649
Training iter #33900:   Batch Loss = 0.636933, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7005453705787659, Accuracy = 0.6304348111152649
Training iter #34000:   Batch Loss = 0.719359, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6970296502113342, Accuracy = 0.6304348111152649
Training iter #34100:   Batch Loss = 0.661661, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7114925980567932, Accuracy = 0.5652173757553101
Training iter #34200:   Batch Loss = 0.635862, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7027736902236938, Accuracy = 0.6304348111152649
Training iter #34300:   Batch Loss = 0.715352, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6976104974746704, Accuracy = 0.6195651888847351
Training iter #34400:   Batch Loss = 0.653903, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6988821029663086, Accuracy = 0.6086956262588501
Training iter #34500:   Batch Loss = 0.720656, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6934075355529785, Accuracy = 0.6304348111152649
Training iter #34600:   Batch Loss = 0.686064, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7035581469535828, Accuracy = 0.6304348111152649
Training iter #34700:   Batch Loss = 0.633191, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7125532627105713, Accuracy = 0.554347813129425
Training iter #34800:   Batch Loss = 0.721768, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6942931413650513, Accuracy = 0.6413043737411499
Training iter #34900:   Batch Loss = 0.682022, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6999264359474182, Accuracy = 0.6195651888847351
Training iter #35000:   Batch Loss = 0.628083, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7018068432807922, Accuracy = 0.6413043737411499
Training iter #35100:   Batch Loss = 0.694811, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7057502269744873, Accuracy = 0.5978260636329651
Training iter #35200:   Batch Loss = 0.657524, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7026744484901428, Accuracy = 0.6304348111152649
Training iter #35300:   Batch Loss = 0.720144, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.688225269317627, Accuracy = 0.6413043737411499
Training iter #35400:   Batch Loss = 0.703328, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6962931752204895, Accuracy = 0.6521739363670349
Training iter #35500:   Batch Loss = 0.638607, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7052246928215027, Accuracy = 0.5869565010070801
Training iter #35600:   Batch Loss = 0.706796, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6939342617988586, Accuracy = 0.6304348111152649
Training iter #35700:   Batch Loss = 0.680565, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7016338109970093, Accuracy = 0.6521739363670349
Training iter #35800:   Batch Loss = 0.631778, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7026556134223938, Accuracy = 0.6304348111152649
Training iter #35900:   Batch Loss = 0.721302, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.702928900718689, Accuracy = 0.6304348111152649
Training iter #36000:   Batch Loss = 0.660653, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.700715959072113, Accuracy = 0.6521739363670349
Training iter #36100:   Batch Loss = 0.732768, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6878646016120911, Accuracy = 0.6195651888847351
Training iter #36200:   Batch Loss = 0.720078, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6989969611167908, Accuracy = 0.6413043737411499
Training iter #36300:   Batch Loss = 0.641474, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7103090286254883, Accuracy = 0.5652173757553101
Training iter #36400:   Batch Loss = 0.699945, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6966603994369507, Accuracy = 0.6086956262588501
Training iter #36500:   Batch Loss = 0.682638, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6984740495681763, Accuracy = 0.6086956262588501
Training iter #36600:   Batch Loss = 0.632283, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6988168954849243, Accuracy = 0.6304348111152649
Training iter #36700:   Batch Loss = 0.726327, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7027350664138794, Accuracy = 0.6304348111152649
Training iter #36800:   Batch Loss = 0.663644, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7048343420028687, Accuracy = 0.5978260636329651
Training iter #36900:   Batch Loss = 0.734103, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6873076558113098, Accuracy = 0.6304348111152649
Training iter #37000:   Batch Loss = 0.715631, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6944016814231873, Accuracy = 0.6304348111152649
Training iter #37100:   Batch Loss = 0.647479, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7008739113807678, Accuracy = 0.6304348111152649
Training iter #37200:   Batch Loss = 0.709936, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7002747058868408, Accuracy = 0.6521739363670349
Training iter #37300:   Batch Loss = 0.676213, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7047232389450073, Accuracy = 0.6195651888847351
Training iter #37400:   Batch Loss = 0.677061, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6973440647125244, Accuracy = 0.6413043737411499
Training iter #37500:   Batch Loss = 0.729090, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6955965161323547, Accuracy = 0.6304348111152649
Training iter #37600:   Batch Loss = 0.670346, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7032082676887512, Accuracy = 0.6195651888847351
Training iter #37700:   Batch Loss = 0.729228, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.696347177028656, Accuracy = 0.6195651888847351
Training iter #37800:   Batch Loss = 0.702729, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7152609825134277, Accuracy = 0.52173912525177
Training iter #37900:   Batch Loss = 0.661316, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7178634405136108, Accuracy = 0.54347825050354
Training iter #38000:   Batch Loss = 0.712733, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7116289138793945, Accuracy = 0.532608687877655
Training iter #38100:   Batch Loss = 0.674290, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7108089923858643, Accuracy = 0.5760869383811951
Training iter #38200:   Batch Loss = 0.677374, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.705409586429596, Accuracy = 0.5652173757553101
Training iter #38300:   Batch Loss = 0.721582, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7070541381835938, Accuracy = 0.5760869383811951
Training iter #38400:   Batch Loss = 0.696153, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7134417295455933, Accuracy = 0.532608687877655
Training iter #38500:   Batch Loss = 0.762265, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.6859583854675293, Accuracy = 0.6521739363670349
Training iter #38600:   Batch Loss = 0.707776, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7139406204223633, Accuracy = 0.5760869383811951
Training iter #38700:   Batch Loss = 0.669168, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.716010332107544, Accuracy = 0.5760869383811951
Training iter #38800:   Batch Loss = 0.691760, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7045559287071228, Accuracy = 0.5760869383811951
Training iter #38900:   Batch Loss = 0.687310, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7025872468948364, Accuracy = 0.5869565010070801
Training iter #39000:   Batch Loss = 0.689313, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7061572670936584, Accuracy = 0.5869565010070801
Training iter #39100:   Batch Loss = 0.726575, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7118558287620544, Accuracy = 0.54347825050354
Training iter #39200:   Batch Loss = 0.693270, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7193302512168884, Accuracy = 0.5652173757553101
Training iter #39300:   Batch Loss = 0.793071, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.6978798508644104, Accuracy = 0.6195651888847351
Training iter #39400:   Batch Loss = 0.695479, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7072543501853943, Accuracy = 0.5652173757553101
Training iter #39500:   Batch Loss = 0.625791, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7084094882011414, Accuracy = 0.554347813129425
Training iter #39600:   Batch Loss = 0.709259, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7081720232963562, Accuracy = 0.5652173757553101
Training iter #39700:   Batch Loss = 0.682369, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.713202178478241, Accuracy = 0.532608687877655
Training iter #39800:   Batch Loss = 0.694770, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.711890459060669, Accuracy = 0.554347813129425
Training iter #39900:   Batch Loss = 0.707743, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7071067690849304, Accuracy = 0.54347825050354
Training iter #40000:   Batch Loss = 0.675310, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7085472345352173, Accuracy = 0.5760869383811951
Training iter #40100:   Batch Loss = 0.771738, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.6988601088523865, Accuracy = 0.6304348111152649
Training iter #40200:   Batch Loss = 0.708928, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7090851664543152, Accuracy = 0.5652173757553101
Training iter #40300:   Batch Loss = 0.602254, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7026400566101074, Accuracy = 0.5760869383811951
Training iter #40400:   Batch Loss = 0.721915, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6993253827095032, Accuracy = 0.6195651888847351
Training iter #40500:   Batch Loss = 0.667608, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7036275863647461, Accuracy = 0.5652173757553101
Training iter #40600:   Batch Loss = 0.679383, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7028199434280396, Accuracy = 0.5760869383811951
Training iter #40700:   Batch Loss = 0.701925, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7004718780517578, Accuracy = 0.6086956262588501
Training iter #40800:   Batch Loss = 0.653996, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7106045484542847, Accuracy = 0.5760869383811951
Training iter #40900:   Batch Loss = 0.772307, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7005584836006165, Accuracy = 0.6086956262588501
Training iter #41000:   Batch Loss = 0.691344, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7080971598625183, Accuracy = 0.5978260636329651
Training iter #41100:   Batch Loss = 0.593294, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6975709199905396, Accuracy = 0.6086956262588501
Training iter #41200:   Batch Loss = 0.724132, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.6911916136741638, Accuracy = 0.6304348111152649
Training iter #41300:   Batch Loss = 0.652599, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6947910785675049, Accuracy = 0.6521739363670349
Training iter #41400:   Batch Loss = 0.695720, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6933202743530273, Accuracy = 0.6630434989929199
Training iter #41500:   Batch Loss = 0.698941, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6902589797973633, Accuracy = 0.6739130616188049
Training iter #41600:   Batch Loss = 0.637181, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7033677697181702, Accuracy = 0.5652173757553101
Training iter #41700:   Batch Loss = 0.744082, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6917874217033386, Accuracy = 0.6739130616188049
Training iter #41800:   Batch Loss = 0.684917, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6961352825164795, Accuracy = 0.6521739363670349
Training iter #41900:   Batch Loss = 0.598696, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6913771629333496, Accuracy = 0.6304348111152649
Training iter #42000:   Batch Loss = 0.705266, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6923535466194153, Accuracy = 0.6304348111152649
Training iter #42100:   Batch Loss = 0.650678, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7029638290405273, Accuracy = 0.5978260636329651
Training iter #42200:   Batch Loss = 0.685623, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6978850960731506, Accuracy = 0.6413043737411499
Training iter #42300:   Batch Loss = 0.702577, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6941216588020325, Accuracy = 0.6304348111152649
Training iter #42400:   Batch Loss = 0.630745, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7028223872184753, Accuracy = 0.6086956262588501
Training iter #42500:   Batch Loss = 0.734565, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6936137676239014, Accuracy = 0.6304348111152649
Training iter #42600:   Batch Loss = 0.662654, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7013659477233887, Accuracy = 0.6195651888847351
Training iter #42700:   Batch Loss = 0.594863, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6912939548492432, Accuracy = 0.6304348111152649
Training iter #42800:   Batch Loss = 0.698824, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6891762018203735, Accuracy = 0.6195651888847351
Training iter #42900:   Batch Loss = 0.625394, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7014836072921753, Accuracy = 0.6195651888847351
Training iter #43000:   Batch Loss = 0.690906, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6997063755989075, Accuracy = 0.6413043737411499
Training iter #43100:   Batch Loss = 0.700204, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6964550018310547, Accuracy = 0.6195651888847351
Training iter #43200:   Batch Loss = 0.624325, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.70217365026474, Accuracy = 0.6195651888847351
Training iter #43300:   Batch Loss = 0.731247, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6943329572677612, Accuracy = 0.6195651888847351
Training iter #43400:   Batch Loss = 0.658502, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7022077441215515, Accuracy = 0.6195651888847351
Training iter #43500:   Batch Loss = 0.593924, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6923221945762634, Accuracy = 0.6304348111152649
Training iter #43600:   Batch Loss = 0.712309, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6891412734985352, Accuracy = 0.6195651888847351
Training iter #43700:   Batch Loss = 0.616186, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6971105933189392, Accuracy = 0.6413043737411499
Training iter #43800:   Batch Loss = 0.682976, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6968414187431335, Accuracy = 0.6413043737411499
Training iter #43900:   Batch Loss = 0.685612, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6979919075965881, Accuracy = 0.6413043737411499
Training iter #44000:   Batch Loss = 0.616924, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7047141194343567, Accuracy = 0.5869565010070801
Training iter #44100:   Batch Loss = 0.704263, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6895174384117126, Accuracy = 0.6304348111152649
Training iter #44200:   Batch Loss = 0.653647, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6987738013267517, Accuracy = 0.6304348111152649
Training iter #44300:   Batch Loss = 0.593930, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6927652359008789, Accuracy = 0.6413043737411499
Training iter #44400:   Batch Loss = 0.730603, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.6918832659721375, Accuracy = 0.6304348111152649
Training iter #44500:   Batch Loss = 0.615105, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7009535431861877, Accuracy = 0.6195651888847351
Training iter #44600:   Batch Loss = 0.685958, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6955483555793762, Accuracy = 0.6195651888847351
Training iter #44700:   Batch Loss = 0.685552, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6955692768096924, Accuracy = 0.6304348111152649
Training iter #44800:   Batch Loss = 0.621014, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7017266750335693, Accuracy = 0.5978260636329651
Training iter #44900:   Batch Loss = 0.683933, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6933671236038208, Accuracy = 0.6413043737411499
Training iter #45000:   Batch Loss = 0.649463, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.700394332408905, Accuracy = 0.6086956262588501
Training iter #45100:   Batch Loss = 0.587766, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.6896644830703735, Accuracy = 0.6413043737411499
Training iter #45200:   Batch Loss = 0.734334, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.6903100609779358, Accuracy = 0.6304348111152649
Training iter #45300:   Batch Loss = 0.623997, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7030189037322998, Accuracy = 0.6086956262588501
Training iter #45400:   Batch Loss = 0.702295, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6965173482894897, Accuracy = 0.6195651888847351
Training iter #45500:   Batch Loss = 0.691580, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6977452039718628, Accuracy = 0.6195651888847351
Training iter #45600:   Batch Loss = 0.596066, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7002834677696228, Accuracy = 0.6304348111152649
Training iter #45700:   Batch Loss = 0.689042, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6963463425636292, Accuracy = 0.6195651888847351
Training iter #45800:   Batch Loss = 0.641588, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7019573450088501, Accuracy = 0.6086956262588501
Training iter #45900:   Batch Loss = 0.588808, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6929237842559814, Accuracy = 0.6521739363670349
Training iter #46000:   Batch Loss = 0.724738, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.6925081014633179, Accuracy = 0.6304348111152649
Training iter #46100:   Batch Loss = 0.619414, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6985090374946594, Accuracy = 0.6195651888847351
Training iter #46200:   Batch Loss = 0.707512, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.690540611743927, Accuracy = 0.6304348111152649
Training iter #46300:   Batch Loss = 0.693434, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6949629783630371, Accuracy = 0.6521739363670349
Training iter #46400:   Batch Loss = 0.586741, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7010459899902344, Accuracy = 0.5869565010070801
Training iter #46500:   Batch Loss = 0.705368, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6931108832359314, Accuracy = 0.6195651888847351
Training iter #46600:   Batch Loss = 0.649493, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6987504363059998, Accuracy = 0.6086956262588501
Training iter #46700:   Batch Loss = 0.585508, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.693946123123169, Accuracy = 0.6413043737411499
Training iter #46800:   Batch Loss = 0.715441, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6971051692962646, Accuracy = 0.6304348111152649
Training iter #46900:   Batch Loss = 0.623147, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6983486413955688, Accuracy = 0.6195651888847351
Training iter #47000:   Batch Loss = 0.707206, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6900150775909424, Accuracy = 0.6304348111152649
Training iter #47100:   Batch Loss = 0.697982, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6958324313163757, Accuracy = 0.6413043737411499
Training iter #47200:   Batch Loss = 0.607615, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6991371512413025, Accuracy = 0.6086956262588501
Training iter #47300:   Batch Loss = 0.712719, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6910079717636108, Accuracy = 0.6304348111152649
Training iter #47400:   Batch Loss = 0.657106, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6988504528999329, Accuracy = 0.6086956262588501
Training iter #47500:   Batch Loss = 0.607125, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6974816918373108, Accuracy = 0.6086956262588501
Training iter #47600:   Batch Loss = 0.712600, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6991469264030457, Accuracy = 0.6086956262588501
Training iter #47700:   Batch Loss = 0.636220, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6984527111053467, Accuracy = 0.6195651888847351
Training iter #47800:   Batch Loss = 0.719337, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6885443925857544, Accuracy = 0.6304348111152649
Training iter #47900:   Batch Loss = 0.695851, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6933162212371826, Accuracy = 0.6195651888847351
Training iter #48000:   Batch Loss = 0.614641, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7002575993537903, Accuracy = 0.6086956262588501
Training iter #48100:   Batch Loss = 0.713166, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6927675008773804, Accuracy = 0.6304348111152649
Training iter #48200:   Batch Loss = 0.668357, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7020648121833801, Accuracy = 0.6086956262588501
Training iter #48300:   Batch Loss = 0.623250, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.699185311794281, Accuracy = 0.6086956262588501
Training iter #48400:   Batch Loss = 0.705678, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6974895596504211, Accuracy = 0.6304348111152649
Training iter #48500:   Batch Loss = 0.636102, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6944181323051453, Accuracy = 0.6413043737411499
Training iter #48600:   Batch Loss = 0.727094, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6871638298034668, Accuracy = 0.6195651888847351
Training iter #48700:   Batch Loss = 0.710904, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6964612603187561, Accuracy = 0.6195651888847351
Training iter #48800:   Batch Loss = 0.624323, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7036523818969727, Accuracy = 0.5869565010070801
Training iter #48900:   Batch Loss = 0.708133, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6917968392372131, Accuracy = 0.6304348111152649
Training iter #49000:   Batch Loss = 0.674768, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6970696449279785, Accuracy = 0.6304348111152649
Training iter #49100:   Batch Loss = 0.619068, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6969162821769714, Accuracy = 0.6195651888847351
Training iter #49200:   Batch Loss = 0.735871, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6995750069618225, Accuracy = 0.6086956262588501
Training iter #49300:   Batch Loss = 0.643112, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6984225511550903, Accuracy = 0.6086956262588501
Training iter #49400:   Batch Loss = 0.739610, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6863178014755249, Accuracy = 0.6195651888847351
Training iter #49500:   Batch Loss = 0.734591, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6943987011909485, Accuracy = 0.6413043737411499
Training iter #49600:   Batch Loss = 0.628828, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7025622129440308, Accuracy = 0.5978260636329651
Training iter #49700:   Batch Loss = 0.701607, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6946641206741333, Accuracy = 0.6304348111152649
Training iter #49800:   Batch Loss = 0.673994, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6987332105636597, Accuracy = 0.6195651888847351
Training iter #49900:   Batch Loss = 0.630619, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6969537734985352, Accuracy = 0.6086956262588501
Training iter #50000:   Batch Loss = 0.739171, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6975954174995422, Accuracy = 0.6086956262588501
Training iter #50100:   Batch Loss = 0.644083, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6983006000518799, Accuracy = 0.6086956262588501
Training iter #50200:   Batch Loss = 0.739518, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6856546998023987, Accuracy = 0.6195651888847351
Training iter #50300:   Batch Loss = 0.715731, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.693656861782074, Accuracy = 0.6195651888847351
Training iter #50400:   Batch Loss = 0.636933, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6995196342468262, Accuracy = 0.6086956262588501
Training iter #50500:   Batch Loss = 0.714337, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6980192065238953, Accuracy = 0.6086956262588501
Training iter #50600:   Batch Loss = 0.671469, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.701574444770813, Accuracy = 0.5869565010070801
Training iter #50700:   Batch Loss = 0.672958, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6950904130935669, Accuracy = 0.6304348111152649
Training iter #50800:   Batch Loss = 0.736431, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6940807700157166, Accuracy = 0.6413043737411499
Training iter #50900:   Batch Loss = 0.654386, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7005231380462646, Accuracy = 0.6086956262588501
Training iter #51000:   Batch Loss = 0.734090, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6873128414154053, Accuracy = 0.6195651888847351
Training iter #51100:   Batch Loss = 0.696402, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6980692148208618, Accuracy = 0.6086956262588501
Training iter #51200:   Batch Loss = 0.637482, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6980860829353333, Accuracy = 0.6086956262588501
Training iter #51300:   Batch Loss = 0.719555, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6931939125061035, Accuracy = 0.6195651888847351
Training iter #51400:   Batch Loss = 0.664270, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.697089672088623, Accuracy = 0.6195651888847351
Training iter #51500:   Batch Loss = 0.680920, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6967565417289734, Accuracy = 0.6086956262588501
Training iter #51600:   Batch Loss = 0.731045, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6967799067497253, Accuracy = 0.6086956262588501
Training iter #51700:   Batch Loss = 0.661866, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6982250809669495, Accuracy = 0.6086956262588501
Training iter #51800:   Batch Loss = 0.750875, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6836434602737427, Accuracy = 0.6304348111152649
Training iter #51900:   Batch Loss = 0.691684, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6987442374229431, Accuracy = 0.6086956262588501
Training iter #52000:   Batch Loss = 0.622132, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.701361358165741, Accuracy = 0.6086956262588501
Training iter #52100:   Batch Loss = 0.708538, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6962664127349854, Accuracy = 0.6086956262588501
Training iter #52200:   Batch Loss = 0.681728, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6976727843284607, Accuracy = 0.6195651888847351
Training iter #52300:   Batch Loss = 0.687482, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6977200508117676, Accuracy = 0.6195651888847351
Training iter #52400:   Batch Loss = 0.730189, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6974710822105408, Accuracy = 0.6086956262588501
Training iter #52500:   Batch Loss = 0.658552, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7015276551246643, Accuracy = 0.5978260636329651
Training iter #52600:   Batch Loss = 0.770224, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6861684918403625, Accuracy = 0.6195651888847351
Training iter #52700:   Batch Loss = 0.682520, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6969946026802063, Accuracy = 0.6086956262588501
Training iter #52800:   Batch Loss = 0.608118, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6949999332427979, Accuracy = 0.6195651888847351
Training iter #52900:   Batch Loss = 0.709595, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6913415193557739, Accuracy = 0.6195651888847351
Training iter #53000:   Batch Loss = 0.674891, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6960247755050659, Accuracy = 0.6195651888847351
Training iter #53100:   Batch Loss = 0.699247, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6975942254066467, Accuracy = 0.6086956262588501
Training iter #53200:   Batch Loss = 0.713289, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6954889297485352, Accuracy = 0.6195651888847351
Training iter #53300:   Batch Loss = 0.648761, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7007848024368286, Accuracy = 0.5978260636329651
Training iter #53400:   Batch Loss = 0.768789, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6896570920944214, Accuracy = 0.6304348111152649
Training iter #53500:   Batch Loss = 0.690306, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7004005312919617, Accuracy = 0.5869565010070801
Training iter #53600:   Batch Loss = 0.589865, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6932005882263184, Accuracy = 0.6413043737411499
Training iter #53700:   Batch Loss = 0.719486, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6918624043464661, Accuracy = 0.6304348111152649
Training iter #53800:   Batch Loss = 0.662879, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.696979284286499, Accuracy = 0.6086956262588501
Training iter #53900:   Batch Loss = 0.672508, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6960897445678711, Accuracy = 0.6195651888847351
Training iter #54000:   Batch Loss = 0.710576, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6924442648887634, Accuracy = 0.6195651888847351
Training iter #54100:   Batch Loss = 0.631341, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7031984329223633, Accuracy = 0.5760869383811951
Training iter #54200:   Batch Loss = 0.766598, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6917962431907654, Accuracy = 0.6195651888847351
Training iter #54300:   Batch Loss = 0.674777, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7017451524734497, Accuracy = 0.5869565010070801
Training iter #54400:   Batch Loss = 0.584298, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.6929319500923157, Accuracy = 0.6413043737411499
Training iter #54500:   Batch Loss = 0.718708, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.6897848844528198, Accuracy = 0.6195651888847351
Training iter #54600:   Batch Loss = 0.649603, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6960963010787964, Accuracy = 0.6195651888847351
Training iter #54700:   Batch Loss = 0.677887, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6964544653892517, Accuracy = 0.6086956262588501
Training iter #54800:   Batch Loss = 0.698926, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6955810785293579, Accuracy = 0.6195651888847351
Training iter #54900:   Batch Loss = 0.626154, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7061172127723694, Accuracy = 0.554347813129425
Training iter #55000:   Batch Loss = 0.740070, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6926327347755432, Accuracy = 0.6413043737411499
Training iter #55100:   Batch Loss = 0.679504, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6974307298660278, Accuracy = 0.6086956262588501
Training iter #55200:   Batch Loss = 0.585176, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6899973154067993, Accuracy = 0.6195651888847351
Training iter #55300:   Batch Loss = 0.705212, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6897386312484741, Accuracy = 0.6195651888847351
Training iter #55400:   Batch Loss = 0.638018, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7012260556221008, Accuracy = 0.5978260636329651
Training iter #55500:   Batch Loss = 0.680244, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6966017484664917, Accuracy = 0.6086956262588501
Training iter #55600:   Batch Loss = 0.700565, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6929366588592529, Accuracy = 0.6086956262588501
Training iter #55700:   Batch Loss = 0.616891, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7015706300735474, Accuracy = 0.5869565010070801
Training iter #55800:   Batch Loss = 0.727573, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6925904154777527, Accuracy = 0.6304348111152649
Training iter #55900:   Batch Loss = 0.655456, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7009897232055664, Accuracy = 0.5869565010070801
Training iter #56000:   Batch Loss = 0.585623, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6903119087219238, Accuracy = 0.6195651888847351
Training iter #56100:   Batch Loss = 0.694952, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6881112456321716, Accuracy = 0.6304348111152649
Training iter #56200:   Batch Loss = 0.615367, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6999331712722778, Accuracy = 0.5978260636329651
Training iter #56300:   Batch Loss = 0.678904, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6981454491615295, Accuracy = 0.6086956262588501
Training iter #56400:   Batch Loss = 0.699029, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6956272125244141, Accuracy = 0.6195651888847351
Training iter #56500:   Batch Loss = 0.607436, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7018924355506897, Accuracy = 0.5869565010070801
Training iter #56600:   Batch Loss = 0.728388, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6918142437934875, Accuracy = 0.6195651888847351
Training iter #56700:   Batch Loss = 0.650228, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7007766962051392, Accuracy = 0.5869565010070801
Training iter #56800:   Batch Loss = 0.582837, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6902621984481812, Accuracy = 0.6195651888847351
Training iter #56900:   Batch Loss = 0.714582, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6886186599731445, Accuracy = 0.6304348111152649
Training iter #57000:   Batch Loss = 0.602980, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6980646252632141, Accuracy = 0.6086956262588501
Training iter #57100:   Batch Loss = 0.674355, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6978394985198975, Accuracy = 0.6086956262588501
Training iter #57200:   Batch Loss = 0.682741, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6993603110313416, Accuracy = 0.5760869383811951
Training iter #57300:   Batch Loss = 0.602767, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7048655152320862, Accuracy = 0.5652173757553101
Training iter #57400:   Batch Loss = 0.700413, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6896671652793884, Accuracy = 0.6086956262588501
Training iter #57500:   Batch Loss = 0.647775, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6994213461875916, Accuracy = 0.5869565010070801
Training iter #57600:   Batch Loss = 0.583550, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6927760243415833, Accuracy = 0.6304348111152649
Training iter #57700:   Batch Loss = 0.733362, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.6919195652008057, Accuracy = 0.6195651888847351
Training iter #57800:   Batch Loss = 0.601180, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7008126378059387, Accuracy = 0.5869565010070801
Training iter #57900:   Batch Loss = 0.680547, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6945204138755798, Accuracy = 0.6195651888847351
Training iter #58000:   Batch Loss = 0.667049, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6947131752967834, Accuracy = 0.6086956262588501
Training iter #58100:   Batch Loss = 0.604895, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7011867761611938, Accuracy = 0.5978260636329651
Training iter #58200:   Batch Loss = 0.680650, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6926854848861694, Accuracy = 0.6304348111152649
Training iter #58300:   Batch Loss = 0.641715, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7012348175048828, Accuracy = 0.5869565010070801
Training iter #58400:   Batch Loss = 0.578119, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6891446113586426, Accuracy = 0.6413043737411499
Training iter #58500:   Batch Loss = 0.721217, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6902584433555603, Accuracy = 0.6195651888847351
Training iter #58600:   Batch Loss = 0.612423, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7029842138290405, Accuracy = 0.5760869383811951
Training iter #58700:   Batch Loss = 0.695324, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6968177556991577, Accuracy = 0.5978260636329651
Training iter #58800:   Batch Loss = 0.673505, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6989873647689819, Accuracy = 0.5869565010070801
Training iter #58900:   Batch Loss = 0.578122, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.701484203338623, Accuracy = 0.5869565010070801
Training iter #59000:   Batch Loss = 0.685993, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6958909034729004, Accuracy = 0.6086956262588501
Training iter #59100:   Batch Loss = 0.631525, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.701540470123291, Accuracy = 0.5869565010070801
Training iter #59200:   Batch Loss = 0.577906, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6914384961128235, Accuracy = 0.6304348111152649
Training iter #59300:   Batch Loss = 0.712135, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6926248073577881, Accuracy = 0.6086956262588501
Training iter #59400:   Batch Loss = 0.606599, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.70026034116745, Accuracy = 0.5760869383811951
Training iter #59500:   Batch Loss = 0.703671, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6908396482467651, Accuracy = 0.6195651888847351
Training iter #59600:   Batch Loss = 0.673945, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6951432228088379, Accuracy = 0.6086956262588501
Training iter #59700:   Batch Loss = 0.572502, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7008165717124939, Accuracy = 0.5869565010070801
Training iter #59800:   Batch Loss = 0.705075, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6941316723823547, Accuracy = 0.6086956262588501
Training iter #59900:   Batch Loss = 0.639228, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7009673118591309, Accuracy = 0.5869565010070801
Training iter #60000:   Batch Loss = 0.577745, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6941534876823425, Accuracy = 0.6195651888847351
Training iter #60100:   Batch Loss = 0.701045, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6972031593322754, Accuracy = 0.5869565010070801
Training iter #60200:   Batch Loss = 0.610502, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6980329751968384, Accuracy = 0.5869565010070801
Training iter #60300:   Batch Loss = 0.701971, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6896214485168457, Accuracy = 0.6086956262588501
Training iter #60400:   Batch Loss = 0.676672, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6964649558067322, Accuracy = 0.6086956262588501
Training iter #60500:   Batch Loss = 0.595360, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7001534700393677, Accuracy = 0.5978260636329651
Training iter #60600:   Batch Loss = 0.712389, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6915444731712341, Accuracy = 0.6086956262588501
Training iter #60700:   Batch Loss = 0.646907, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6999489665031433, Accuracy = 0.5760869383811951
Training iter #60800:   Batch Loss = 0.599101, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.697716236114502, Accuracy = 0.5978260636329651
Training iter #60900:   Batch Loss = 0.696785, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7003844976425171, Accuracy = 0.5760869383811951
Training iter #61000:   Batch Loss = 0.625003, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6996197700500488, Accuracy = 0.5760869383811951
Training iter #61100:   Batch Loss = 0.714476, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6881856322288513, Accuracy = 0.6304348111152649
Training iter #61200:   Batch Loss = 0.674809, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6932241320610046, Accuracy = 0.5869565010070801
Training iter #61300:   Batch Loss = 0.604105, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7009769678115845, Accuracy = 0.5869565010070801
Training iter #61400:   Batch Loss = 0.712061, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.693868100643158, Accuracy = 0.5869565010070801
Training iter #61500:   Batch Loss = 0.656068, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7047829031944275, Accuracy = 0.5869565010070801
Training iter #61600:   Batch Loss = 0.614551, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6995754837989807, Accuracy = 0.5760869383811951
Training iter #61700:   Batch Loss = 0.690432, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6978814601898193, Accuracy = 0.5869565010070801
Training iter #61800:   Batch Loss = 0.623705, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6950011253356934, Accuracy = 0.6086956262588501
Training iter #61900:   Batch Loss = 0.722990, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6873441934585571, Accuracy = 0.6086956262588501
Training iter #62000:   Batch Loss = 0.690239, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.697814404964447, Accuracy = 0.5869565010070801
Training iter #62100:   Batch Loss = 0.616005, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7053696513175964, Accuracy = 0.5652173757553101
Training iter #62200:   Batch Loss = 0.706162, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6926600337028503, Accuracy = 0.6086956262588501
Training iter #62300:   Batch Loss = 0.665165, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6988522410392761, Accuracy = 0.5869565010070801
Training iter #62400:   Batch Loss = 0.611251, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.696944534778595, Accuracy = 0.5978260636329651
Training iter #62500:   Batch Loss = 0.721653, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6999698877334595, Accuracy = 0.5760869383811951
Training iter #62600:   Batch Loss = 0.631767, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6987335681915283, Accuracy = 0.5869565010070801
Training iter #62700:   Batch Loss = 0.733283, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.685502290725708, Accuracy = 0.6086956262588501
Training iter #62800:   Batch Loss = 0.716338, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6953405737876892, Accuracy = 0.6086956262588501
Training iter #62900:   Batch Loss = 0.621811, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7047136425971985, Accuracy = 0.5760869383811951
Training iter #63000:   Batch Loss = 0.698994, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6961488723754883, Accuracy = 0.5760869383811951
Training iter #63100:   Batch Loss = 0.664843, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7009892463684082, Accuracy = 0.5869565010070801
Training iter #63200:   Batch Loss = 0.623544, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6976357102394104, Accuracy = 0.5869565010070801
Training iter #63300:   Batch Loss = 0.724051, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6983956694602966, Accuracy = 0.5869565010070801
Training iter #63400:   Batch Loss = 0.632944, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6995630264282227, Accuracy = 0.5760869383811951
Training iter #63500:   Batch Loss = 0.732609, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6855410933494568, Accuracy = 0.6086956262588501
Training iter #63600:   Batch Loss = 0.708709, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6949688792228699, Accuracy = 0.5760869383811951
Training iter #63700:   Batch Loss = 0.630176, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7012506127357483, Accuracy = 0.5760869383811951
Training iter #63800:   Batch Loss = 0.713983, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6997669339179993, Accuracy = 0.5869565010070801
Training iter #63900:   Batch Loss = 0.662743, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7035780549049377, Accuracy = 0.5869565010070801
Training iter #64000:   Batch Loss = 0.664629, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6949731707572937, Accuracy = 0.6195651888847351
Training iter #64100:   Batch Loss = 0.720902, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6940845251083374, Accuracy = 0.6086956262588501
Training iter #64200:   Batch Loss = 0.642518, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7024485468864441, Accuracy = 0.5760869383811951
Training iter #64300:   Batch Loss = 0.727220, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6877752542495728, Accuracy = 0.6086956262588501
Training iter #64400:   Batch Loss = 0.686568, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7003344893455505, Accuracy = 0.5869565010070801
Training iter #64500:   Batch Loss = 0.630642, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6998195052146912, Accuracy = 0.5869565010070801
Training iter #64600:   Batch Loss = 0.719921, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6940476298332214, Accuracy = 0.5869565010070801
Training iter #64700:   Batch Loss = 0.655383, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.698549747467041, Accuracy = 0.5869565010070801
Training iter #64800:   Batch Loss = 0.673749, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6979154348373413, Accuracy = 0.5869565010070801
Training iter #64900:   Batch Loss = 0.713429, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6981258392333984, Accuracy = 0.5869565010070801
Training iter #65000:   Batch Loss = 0.651936, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7003475427627563, Accuracy = 0.5760869383811951
Training iter #65100:   Batch Loss = 0.744918, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6840638518333435, Accuracy = 0.6086956262588501
Training iter #65200:   Batch Loss = 0.683619, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7006967067718506, Accuracy = 0.5869565010070801
Training iter #65300:   Batch Loss = 0.613493, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7033645510673523, Accuracy = 0.5760869383811951
Training iter #65400:   Batch Loss = 0.707529, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.698291540145874, Accuracy = 0.554347813129425
Training iter #65500:   Batch Loss = 0.675142, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.700562059879303, Accuracy = 0.5760869383811951
Training iter #65600:   Batch Loss = 0.681303, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6991567611694336, Accuracy = 0.5869565010070801
Training iter #65700:   Batch Loss = 0.713336, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6977411508560181, Accuracy = 0.5869565010070801
Training iter #65800:   Batch Loss = 0.648366, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7022661566734314, Accuracy = 0.5760869383811951
Training iter #65900:   Batch Loss = 0.767136, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6858360171318054, Accuracy = 0.5978260636329651
Training iter #66000:   Batch Loss = 0.672303, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6995834112167358, Accuracy = 0.5869565010070801
Training iter #66100:   Batch Loss = 0.600772, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6975516080856323, Accuracy = 0.5869565010070801
Training iter #66200:   Batch Loss = 0.708232, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6931008100509644, Accuracy = 0.5869565010070801
Training iter #66300:   Batch Loss = 0.668654, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6983432173728943, Accuracy = 0.5869565010070801
Training iter #66400:   Batch Loss = 0.686600, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6996377110481262, Accuracy = 0.5869565010070801
Training iter #66500:   Batch Loss = 0.695188, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6979080438613892, Accuracy = 0.5869565010070801
Training iter #66600:   Batch Loss = 0.638856, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7037996053695679, Accuracy = 0.5652173757553101
Training iter #66700:   Batch Loss = 0.765072, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6898958086967468, Accuracy = 0.5978260636329651
Training iter #66800:   Batch Loss = 0.679938, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7018980979919434, Accuracy = 0.5760869383811951
Training iter #66900:   Batch Loss = 0.580382, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6931670308113098, Accuracy = 0.5978260636329651
Training iter #67000:   Batch Loss = 0.722506, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6922333836555481, Accuracy = 0.5978260636329651
Training iter #67100:   Batch Loss = 0.654968, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6996022462844849, Accuracy = 0.5869565010070801
Training iter #67200:   Batch Loss = 0.663047, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6985933184623718, Accuracy = 0.5869565010070801
Training iter #67300:   Batch Loss = 0.691073, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6945009231567383, Accuracy = 0.5652173757553101
Training iter #67400:   Batch Loss = 0.622128, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.705942690372467, Accuracy = 0.5652173757553101
Training iter #67500:   Batch Loss = 0.758467, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6940407752990723, Accuracy = 0.5760869383811951
Training iter #67600:   Batch Loss = 0.666986, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7054738402366638, Accuracy = 0.5652173757553101
Training iter #67700:   Batch Loss = 0.574142, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6937144994735718, Accuracy = 0.5978260636329651
Training iter #67800:   Batch Loss = 0.722216, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.6895859837532043, Accuracy = 0.5869565010070801
Training iter #67900:   Batch Loss = 0.638750, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6975469589233398, Accuracy = 0.5869565010070801
Training iter #68000:   Batch Loss = 0.670630, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6990976929664612, Accuracy = 0.5869565010070801
Training iter #68100:   Batch Loss = 0.678811, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6983077526092529, Accuracy = 0.5869565010070801
Training iter #68200:   Batch Loss = 0.618600, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7089398503303528, Accuracy = 0.54347825050354
Training iter #68300:   Batch Loss = 0.737972, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6939390301704407, Accuracy = 0.5978260636329651
Training iter #68400:   Batch Loss = 0.671236, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6999495625495911, Accuracy = 0.5869565010070801
Training iter #68500:   Batch Loss = 0.573640, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6916862726211548, Accuracy = 0.6086956262588501
Training iter #68600:   Batch Loss = 0.706066, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6912612915039062, Accuracy = 0.5869565010070801
Training iter #68700:   Batch Loss = 0.627492, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7049259543418884, Accuracy = 0.5652173757553101
Training iter #68800:   Batch Loss = 0.669187, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6982232928276062, Accuracy = 0.5869565010070801
Training iter #68900:   Batch Loss = 0.682194, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6940677762031555, Accuracy = 0.5760869383811951
Training iter #69000:   Batch Loss = 0.606212, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7037875652313232, Accuracy = 0.5652173757553101
Training iter #69100:   Batch Loss = 0.726093, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.694606363773346, Accuracy = 0.5652173757553101
Training iter #69200:   Batch Loss = 0.644890, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7051796913146973, Accuracy = 0.5652173757553101
Training iter #69300:   Batch Loss = 0.580331, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6910126805305481, Accuracy = 0.5978260636329651
Training iter #69400:   Batch Loss = 0.696692, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6881850361824036, Accuracy = 0.5978260636329651
Training iter #69500:   Batch Loss = 0.599552, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.702633261680603, Accuracy = 0.5869565010070801
Training iter #69600:   Batch Loss = 0.668437, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7010205388069153, Accuracy = 0.5869565010070801
Training iter #69700:   Batch Loss = 0.679331, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6989358067512512, Accuracy = 0.5652173757553101
Training iter #69800:   Batch Loss = 0.596470, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.705646276473999, Accuracy = 0.5652173757553101
Training iter #69900:   Batch Loss = 0.726694, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6943442821502686, Accuracy = 0.5869565010070801
Training iter #70000:   Batch Loss = 0.639197, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7045802474021912, Accuracy = 0.5652173757553101
Training iter #70100:   Batch Loss = 0.576039, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6913110017776489, Accuracy = 0.5978260636329651
Training iter #70200:   Batch Loss = 0.704452, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6893133521080017, Accuracy = 0.5978260636329651
Training iter #70300:   Batch Loss = 0.587434, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7009299993515015, Accuracy = 0.5760869383811951
Training iter #70400:   Batch Loss = 0.665031, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7004844546318054, Accuracy = 0.5869565010070801
Training iter #70500:   Batch Loss = 0.658946, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7019510269165039, Accuracy = 0.5760869383811951
Training iter #70600:   Batch Loss = 0.591495, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7063018679618835, Accuracy = 0.554347813129425
Training iter #70700:   Batch Loss = 0.702452, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6902689933776855, Accuracy = 0.5869565010070801
Training iter #70800:   Batch Loss = 0.637715, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7027717232704163, Accuracy = 0.5652173757553101
Training iter #70900:   Batch Loss = 0.577211, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6955358982086182, Accuracy = 0.5760869383811951
Training iter #71000:   Batch Loss = 0.723433, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6944448351860046, Accuracy = 0.554347813129425
Training iter #71100:   Batch Loss = 0.586753, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7044005393981934, Accuracy = 0.5652173757553101
Training iter #71200:   Batch Loss = 0.671386, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6960853934288025, Accuracy = 0.5652173757553101
Training iter #71300:   Batch Loss = 0.656966, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6963015198707581, Accuracy = 0.5652173757553101
Training iter #71400:   Batch Loss = 0.592518, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7038764357566833, Accuracy = 0.554347813129425
Training iter #71500:   Batch Loss = 0.683043, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6947898864746094, Accuracy = 0.5869565010070801
Training iter #71600:   Batch Loss = 0.630648, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7054699659347534, Accuracy = 0.554347813129425
Training iter #71700:   Batch Loss = 0.570270, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6910697221755981, Accuracy = 0.5869565010070801
Training iter #71800:   Batch Loss = 0.723069, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6923245191574097, Accuracy = 0.5760869383811951
Training iter #71900:   Batch Loss = 0.600044, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7066729664802551, Accuracy = 0.5652173757553101
Training iter #72000:   Batch Loss = 0.684568, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6993036866188049, Accuracy = 0.554347813129425
Training iter #72100:   Batch Loss = 0.660855, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7015589475631714, Accuracy = 0.5652173757553101
Training iter #72200:   Batch Loss = 0.564293, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7042369246482849, Accuracy = 0.5869565010070801
Training iter #72300:   Batch Loss = 0.688938, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6970639228820801, Accuracy = 0.5652173757553101
Training iter #72400:   Batch Loss = 0.617497, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7039737105369568, Accuracy = 0.5652173757553101
Training iter #72500:   Batch Loss = 0.570776, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6927323341369629, Accuracy = 0.6195651888847351
Training iter #72600:   Batch Loss = 0.710045, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6952376961708069, Accuracy = 0.5652173757553101
Training iter #72700:   Batch Loss = 0.596380, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7042399644851685, Accuracy = 0.554347813129425
Training iter #72800:   Batch Loss = 0.692363, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6930819749832153, Accuracy = 0.5760869383811951
Training iter #72900:   Batch Loss = 0.661077, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6975094676017761, Accuracy = 0.5652173757553101
Training iter #73000:   Batch Loss = 0.561344, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7038126587867737, Accuracy = 0.5652173757553101
Training iter #73100:   Batch Loss = 0.709699, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.697170615196228, Accuracy = 0.5652173757553101
Training iter #73200:   Batch Loss = 0.625433, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7053207159042358, Accuracy = 0.5652173757553101
Training iter #73300:   Batch Loss = 0.571043, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6962852478027344, Accuracy = 0.5978260636329651
Training iter #73400:   Batch Loss = 0.697860, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.699914276599884, Accuracy = 0.5869565010070801
Training iter #73500:   Batch Loss = 0.598201, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6999005079269409, Accuracy = 0.5760869383811951
Training iter #73600:   Batch Loss = 0.690930, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6905601620674133, Accuracy = 0.5869565010070801
Training iter #73700:   Batch Loss = 0.662745, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6990411281585693, Accuracy = 0.5760869383811951
Training iter #73800:   Batch Loss = 0.586666, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7043914794921875, Accuracy = 0.554347813129425
Training iter #73900:   Batch Loss = 0.717013, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6950840353965759, Accuracy = 0.554347813129425
Training iter #74000:   Batch Loss = 0.634421, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7040676474571228, Accuracy = 0.5652173757553101
Training iter #74100:   Batch Loss = 0.592183, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7014885544776917, Accuracy = 0.5760869383811951
Training iter #74200:   Batch Loss = 0.690453, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7055579423904419, Accuracy = 0.5652173757553101
Training iter #74300:   Batch Loss = 0.615037, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7022207975387573, Accuracy = 0.5760869383811951
Training iter #74400:   Batch Loss = 0.704178, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6881757378578186, Accuracy = 0.5869565010070801
Training iter #74500:   Batch Loss = 0.660554, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6939361095428467, Accuracy = 0.5869565010070801
Training iter #74600:   Batch Loss = 0.597475, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7059316039085388, Accuracy = 0.554347813129425
Training iter #74700:   Batch Loss = 0.715151, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6983852386474609, Accuracy = 0.5652173757553101
Training iter #74800:   Batch Loss = 0.642646, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.710193395614624, Accuracy = 0.554347813129425
Training iter #74900:   Batch Loss = 0.599240, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7033247351646423, Accuracy = 0.5652173757553101
Training iter #75000:   Batch Loss = 0.682725, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7031504511833191, Accuracy = 0.5760869383811951
Training iter #75100:   Batch Loss = 0.613621, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6987259984016418, Accuracy = 0.5869565010070801
Training iter #75200:   Batch Loss = 0.713767, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.688880980014801, Accuracy = 0.5978260636329651
Training iter #75300:   Batch Loss = 0.677359, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7001309990882874, Accuracy = 0.5869565010070801
Training iter #75400:   Batch Loss = 0.611717, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7083836197853088, Accuracy = 0.532608687877655
Training iter #75500:   Batch Loss = 0.707505, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.694067120552063, Accuracy = 0.554347813129425
Training iter #75600:   Batch Loss = 0.656126, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7023323178291321, Accuracy = 0.5760869383811951
Training iter #75700:   Batch Loss = 0.598675, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7019677758216858, Accuracy = 0.5652173757553101
Training iter #75800:   Batch Loss = 0.712658, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.708264172077179, Accuracy = 0.554347813129425
Training iter #75900:   Batch Loss = 0.625208, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7036753296852112, Accuracy = 0.5652173757553101
Training iter #76000:   Batch Loss = 0.720790, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.68684321641922, Accuracy = 0.5760869383811951
Training iter #76100:   Batch Loss = 0.707350, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6967995762825012, Accuracy = 0.5652173757553101
Training iter #76200:   Batch Loss = 0.620466, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7085406184196472, Accuracy = 0.54347825050354
Training iter #76300:   Batch Loss = 0.699235, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6998633742332458, Accuracy = 0.54347825050354
Training iter #76400:   Batch Loss = 0.655285, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7066019177436829, Accuracy = 0.5652173757553101
Training iter #76500:   Batch Loss = 0.612464, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7036082744598389, Accuracy = 0.5652173757553101
Training iter #76600:   Batch Loss = 0.714418, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7059690356254578, Accuracy = 0.554347813129425
Training iter #76700:   Batch Loss = 0.625950, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7040953040122986, Accuracy = 0.554347813129425
Training iter #76800:   Batch Loss = 0.720241, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6866381168365479, Accuracy = 0.5760869383811951
Training iter #76900:   Batch Loss = 0.696020, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6961520314216614, Accuracy = 0.5652173757553101
Training iter #77000:   Batch Loss = 0.626716, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.704054057598114, Accuracy = 0.5652173757553101
Training iter #77100:   Batch Loss = 0.716461, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7031152248382568, Accuracy = 0.5760869383811951
Training iter #77200:   Batch Loss = 0.653148, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7070778608322144, Accuracy = 0.554347813129425
Training iter #77300:   Batch Loss = 0.644989, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6985589861869812, Accuracy = 0.5760869383811951
Training iter #77400:   Batch Loss = 0.713013, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6988573670387268, Accuracy = 0.554347813129425
Training iter #77500:   Batch Loss = 0.634212, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7077311277389526, Accuracy = 0.554347813129425
Training iter #77600:   Batch Loss = 0.714704, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6901878118515015, Accuracy = 0.5978260636329651
Training iter #77700:   Batch Loss = 0.671827, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7033371329307556, Accuracy = 0.5760869383811951
Training iter #77800:   Batch Loss = 0.623905, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7018155455589294, Accuracy = 0.5760869383811951
Training iter #77900:   Batch Loss = 0.722574, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6960766911506653, Accuracy = 0.554347813129425
Training iter #78000:   Batch Loss = 0.645924, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7019635438919067, Accuracy = 0.5760869383811951
Training iter #78100:   Batch Loss = 0.657874, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7020893692970276, Accuracy = 0.554347813129425
Training iter #78200:   Batch Loss = 0.702518, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7032182812690735, Accuracy = 0.554347813129425
Training iter #78300:   Batch Loss = 0.643341, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7037702202796936, Accuracy = 0.554347813129425
Training iter #78400:   Batch Loss = 0.733074, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6862198710441589, Accuracy = 0.5760869383811951
Training iter #78500:   Batch Loss = 0.673222, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6979048252105713, Accuracy = 0.6195651888847351
Training iter #78600:   Batch Loss = 0.613750, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7224331498146057, Accuracy = 0.52173912525177
Training iter #78700:   Batch Loss = 0.702156, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7116095423698425, Accuracy = 0.5652173757553101
Training iter #78800:   Batch Loss = 0.673513, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7055308818817139, Accuracy = 0.554347813129425
Training iter #78900:   Batch Loss = 0.655422, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6983023881912231, Accuracy = 0.54347825050354
Training iter #79000:   Batch Loss = 0.710998, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6980489492416382, Accuracy = 0.554347813129425
Training iter #79100:   Batch Loss = 0.640145, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7083939909934998, Accuracy = 0.554347813129425
Training iter #79200:   Batch Loss = 0.760259, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6912575960159302, Accuracy = 0.5869565010070801
Training iter #79300:   Batch Loss = 0.663247, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7053757309913635, Accuracy = 0.554347813129425
Training iter #79400:   Batch Loss = 0.592436, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6972192525863647, Accuracy = 0.5869565010070801
Training iter #79500:   Batch Loss = 0.712250, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6926650404930115, Accuracy = 0.5652173757553101
Training iter #79600:   Batch Loss = 0.662401, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.700054943561554, Accuracy = 0.5652173757553101
Training iter #79700:   Batch Loss = 0.668447, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7072856426239014, Accuracy = 0.554347813129425
Training iter #79800:   Batch Loss = 0.681155, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7074028849601746, Accuracy = 0.554347813129425
Training iter #79900:   Batch Loss = 0.631886, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7091856598854065, Accuracy = 0.554347813129425
Training iter #80000:   Batch Loss = 0.756566, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.6905959844589233, Accuracy = 0.5978260636329651
Training iter #80100:   Batch Loss = 0.669560, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.702335774898529, Accuracy = 0.5760869383811951
Training iter #80200:   Batch Loss = 0.571902, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6959866285324097, Accuracy = 0.5652173757553101
Training iter #80300:   Batch Loss = 0.721570, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6982656717300415, Accuracy = 0.554347813129425
Training iter #80400:   Batch Loss = 0.650802, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7062680721282959, Accuracy = 0.554347813129425
Training iter #80500:   Batch Loss = 0.644646, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7018482089042664, Accuracy = 0.5760869383811951
Training iter #80600:   Batch Loss = 0.684299, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6968259215354919, Accuracy = 0.5652173757553101
Training iter #80700:   Batch Loss = 0.613188, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7090126276016235, Accuracy = 0.554347813129425
Training iter #80800:   Batch Loss = 0.746203, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6987109184265137, Accuracy = 0.5652173757553101
Training iter #80900:   Batch Loss = 0.662267, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7121639251708984, Accuracy = 0.532608687877655
Training iter #81000:   Batch Loss = 0.566019, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6942319869995117, Accuracy = 0.5869565010070801
Optimization Finished!
FINAL RESULT: Batch Loss = 0.6942319869995117, Accuracy = 0.5869565010070801
