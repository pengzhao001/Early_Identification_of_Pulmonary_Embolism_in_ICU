Training iter #50:   Batch Loss = 0.780833, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.9425042271614075, Accuracy = 0.3695652186870575
Training iter #100:   Batch Loss = 0.831337, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.9031768441200256, Accuracy = 0.3695652186870575
Training iter #200:   Batch Loss = 0.752236, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.8461189866065979, Accuracy = 0.3695652186870575
Training iter #300:   Batch Loss = 0.754121, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.8062575459480286, Accuracy = 0.3695652186870575
Training iter #400:   Batch Loss = 0.766676, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7794771790504456, Accuracy = 0.3804347813129425
Training iter #500:   Batch Loss = 0.758410, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 0.7579648494720459, Accuracy = 0.3804347813129425
Training iter #600:   Batch Loss = 0.743923, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7439600825309753, Accuracy = 0.46739131212234497
Training iter #700:   Batch Loss = 0.732854, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7351419925689697, Accuracy = 0.554347813129425
Training iter #800:   Batch Loss = 0.730520, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7296944856643677, Accuracy = 0.6304348111152649
Training iter #900:   Batch Loss = 0.736851, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7281697988510132, Accuracy = 0.6304348111152649
Training iter #1000:   Batch Loss = 0.742208, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.727939248085022, Accuracy = 0.6304348111152649
Training iter #1100:   Batch Loss = 0.756397, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7284156680107117, Accuracy = 0.6304348111152649
Training iter #1200:   Batch Loss = 0.714880, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7304307818412781, Accuracy = 0.6304348111152649
Training iter #1300:   Batch Loss = 0.717487, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7329663634300232, Accuracy = 0.532608687877655
Training iter #1400:   Batch Loss = 0.751314, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7364319562911987, Accuracy = 0.489130437374115
Training iter #1500:   Batch Loss = 0.716696, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7400747537612915, Accuracy = 0.45652174949645996
Training iter #1600:   Batch Loss = 0.731979, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7426490783691406, Accuracy = 0.47826087474823
Training iter #1700:   Batch Loss = 0.733074, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7459273338317871, Accuracy = 0.46739131212234497
Training iter #1800:   Batch Loss = 0.716567, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7475551962852478, Accuracy = 0.45652174949645996
Training iter #1900:   Batch Loss = 0.729400, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7484913468360901, Accuracy = 0.45652174949645996
Training iter #2000:   Batch Loss = 0.720688, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7497822642326355, Accuracy = 0.45652174949645996
Training iter #2100:   Batch Loss = 0.721750, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7492976188659668, Accuracy = 0.47826087474823
Training iter #2200:   Batch Loss = 0.737929, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7495504021644592, Accuracy = 0.46739131212234497
Training iter #2300:   Batch Loss = 0.699693, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7497623562812805, Accuracy = 0.47826087474823
Training iter #2400:   Batch Loss = 0.730105, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7478212714195251, Accuracy = 0.45652174949645996
Training iter #2500:   Batch Loss = 0.733945, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7477648258209229, Accuracy = 0.45652174949645996
Training iter #2600:   Batch Loss = 0.707954, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7472965717315674, Accuracy = 0.46739131212234497
Training iter #2700:   Batch Loss = 0.724136, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7481319308280945, Accuracy = 0.46739131212234497
Training iter #2800:   Batch Loss = 0.710713, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7503823041915894, Accuracy = 0.46739131212234497
Training iter #2900:   Batch Loss = 0.716970, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7506683468818665, Accuracy = 0.46739131212234497
Training iter #3000:   Batch Loss = 0.737139, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7521242499351501, Accuracy = 0.46739131212234497
Training iter #3100:   Batch Loss = 0.687932, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7525509595870972, Accuracy = 0.47826087474823
Training iter #3200:   Batch Loss = 0.729912, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7497751116752625, Accuracy = 0.47826087474823
Training iter #3300:   Batch Loss = 0.730029, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7486884593963623, Accuracy = 0.47826087474823
Training iter #3400:   Batch Loss = 0.694072, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.747646152973175, Accuracy = 0.489130437374115
Training iter #3500:   Batch Loss = 0.717235, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7483537197113037, Accuracy = 0.46739131212234497
Training iter #3600:   Batch Loss = 0.698315, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7490419745445251, Accuracy = 0.46739131212234497
Training iter #3700:   Batch Loss = 0.722159, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7477092146873474, Accuracy = 0.489130437374115
Training iter #3800:   Batch Loss = 0.734653, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.748271107673645, Accuracy = 0.489130437374115
Training iter #3900:   Batch Loss = 0.681024, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.746307909488678, Accuracy = 0.510869562625885
Training iter #4000:   Batch Loss = 0.726879, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7422215342521667, Accuracy = 0.510869562625885
Training iter #4100:   Batch Loss = 0.720859, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7365086078643799, Accuracy = 0.532608687877655
Training iter #4200:   Batch Loss = 0.688979, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7404396533966064, Accuracy = 0.510869562625885
Training iter #4300:   Batch Loss = 0.704235, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7447222471237183, Accuracy = 0.489130437374115
Training iter #4400:   Batch Loss = 0.696959, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.739242434501648, Accuracy = 0.5
Training iter #4500:   Batch Loss = 0.722705, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7312764525413513, Accuracy = 0.510869562625885
Training iter #4600:   Batch Loss = 0.718420, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7296709418296814, Accuracy = 0.510869562625885
Training iter #4700:   Batch Loss = 0.685844, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7385543584823608, Accuracy = 0.5
Training iter #4800:   Batch Loss = 0.716271, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.745222806930542, Accuracy = 0.510869562625885
Training iter #4900:   Batch Loss = 0.721078, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7387973666191101, Accuracy = 0.5
Training iter #5000:   Batch Loss = 0.678077, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7421013116836548, Accuracy = 0.510869562625885
Training iter #5100:   Batch Loss = 0.705418, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.742900013923645, Accuracy = 0.510869562625885
Training iter #5200:   Batch Loss = 0.696124, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7521679401397705, Accuracy = 0.5
Training iter #5300:   Batch Loss = 0.737383, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7607017159461975, Accuracy = 0.5
Training iter #5400:   Batch Loss = 0.710878, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7547487616539001, Accuracy = 0.510869562625885
Training iter #5500:   Batch Loss = 0.675376, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7499591708183289, Accuracy = 0.52173912525177
Training iter #5600:   Batch Loss = 0.739617, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7517330646514893, Accuracy = 0.52173912525177
Training iter #5700:   Batch Loss = 0.715551, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7561720609664917, Accuracy = 0.5
Training iter #5800:   Batch Loss = 0.665743, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.76542729139328, Accuracy = 0.5
Training iter #5900:   Batch Loss = 0.708611, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7568618059158325, Accuracy = 0.489130437374115
Training iter #6000:   Batch Loss = 0.688820, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7533459663391113, Accuracy = 0.489130437374115
Training iter #6100:   Batch Loss = 0.718283, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7635067105293274, Accuracy = 0.510869562625885
Training iter #6200:   Batch Loss = 0.697589, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7585799098014832, Accuracy = 0.510869562625885
Training iter #6300:   Batch Loss = 0.660969, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7580043077468872, Accuracy = 0.489130437374115
Training iter #6400:   Batch Loss = 0.727014, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7518031597137451, Accuracy = 0.52173912525177
Training iter #6500:   Batch Loss = 0.709768, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7512659430503845, Accuracy = 0.52173912525177
Training iter #6600:   Batch Loss = 0.654593, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7630627155303955, Accuracy = 0.489130437374115
Training iter #6700:   Batch Loss = 0.710071, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.760015606880188, Accuracy = 0.489130437374115
Training iter #6800:   Batch Loss = 0.661403, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7578637599945068, Accuracy = 0.5
Training iter #6900:   Batch Loss = 0.722349, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7639018893241882, Accuracy = 0.489130437374115
Training iter #7000:   Batch Loss = 0.701408, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7605881094932556, Accuracy = 0.5
Training iter #7100:   Batch Loss = 0.654390, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7779625058174133, Accuracy = 0.47826087474823
Training iter #7200:   Batch Loss = 0.739398, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7591617107391357, Accuracy = 0.52173912525177
Training iter #7300:   Batch Loss = 0.707617, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7528583407402039, Accuracy = 0.5
Training iter #7400:   Batch Loss = 0.652691, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7759559154510498, Accuracy = 0.46739131212234497
Training iter #7500:   Batch Loss = 0.692423, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.774583101272583, Accuracy = 0.47826087474823
Training iter #7600:   Batch Loss = 0.635657, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7647812366485596, Accuracy = 0.532608687877655
Training iter #7700:   Batch Loss = 0.720442, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7730060815811157, Accuracy = 0.5
Training iter #7800:   Batch Loss = 0.709608, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7730206847190857, Accuracy = 0.510869562625885
Training iter #7900:   Batch Loss = 0.658344, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.8005082011222839, Accuracy = 0.44565218687057495
Training iter #8000:   Batch Loss = 0.734418, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7600354552268982, Accuracy = 0.5
Training iter #8100:   Batch Loss = 0.718355, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7460179328918457, Accuracy = 0.52173912525177
Training iter #8200:   Batch Loss = 0.646795, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7859400510787964, Accuracy = 0.46739131212234497
Training iter #8300:   Batch Loss = 0.712628, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7882440090179443, Accuracy = 0.46739131212234497
Training iter #8400:   Batch Loss = 0.657109, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7554016709327698, Accuracy = 0.54347825050354
Training iter #8500:   Batch Loss = 0.723113, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7525889873504639, Accuracy = 0.510869562625885
Training iter #8600:   Batch Loss = 0.692739, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7549813389778137, Accuracy = 0.532608687877655
Training iter #8700:   Batch Loss = 0.640715, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7753098607063293, Accuracy = 0.489130437374115
Training iter #8800:   Batch Loss = 0.728702, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7708436250686646, Accuracy = 0.47826087474823
Training iter #8900:   Batch Loss = 0.687531, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7570156455039978, Accuracy = 0.52173912525177
Training iter #9000:   Batch Loss = 0.645785, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7551144361495972, Accuracy = 0.54347825050354
Training iter #9100:   Batch Loss = 0.693912, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7562767863273621, Accuracy = 0.54347825050354
Training iter #9200:   Batch Loss = 0.655973, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7573033571243286, Accuracy = 0.54347825050354
Training iter #9300:   Batch Loss = 0.714305, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7868268489837646, Accuracy = 0.45652174949645996
Training iter #9400:   Batch Loss = 0.703189, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7744144797325134, Accuracy = 0.47826087474823
Training iter #9500:   Batch Loss = 0.635599, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7630429863929749, Accuracy = 0.52173912525177
Training iter #9600:   Batch Loss = 0.724141, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7498919367790222, Accuracy = 0.5
Training iter #9700:   Batch Loss = 0.692917, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.750693678855896, Accuracy = 0.532608687877655
Training iter #9800:   Batch Loss = 0.655348, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7836580872535706, Accuracy = 0.46739131212234497
Training iter #9900:   Batch Loss = 0.691814, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7888385653495789, Accuracy = 0.46739131212234497
Training iter #10000:   Batch Loss = 0.649934, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7601767182350159, Accuracy = 0.52173912525177
Training iter #10100:   Batch Loss = 0.732759, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.776888906955719, Accuracy = 0.489130437374115
Training iter #10200:   Batch Loss = 0.706543, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7778252363204956, Accuracy = 0.489130437374115
Training iter #10300:   Batch Loss = 0.634577, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7782210111618042, Accuracy = 0.46739131212234497
Training iter #10400:   Batch Loss = 0.692765, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7562259435653687, Accuracy = 0.532608687877655
Training iter #10500:   Batch Loss = 0.667106, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.753372073173523, Accuracy = 0.5652173757553101
Training iter #10600:   Batch Loss = 0.670727, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7786749005317688, Accuracy = 0.46739131212234497
Training iter #10700:   Batch Loss = 0.696071, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7704623937606812, Accuracy = 0.532608687877655
Training iter #10800:   Batch Loss = 0.654615, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.761326253414154, Accuracy = 0.52173912525177
Training iter #10900:   Batch Loss = 0.733814, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7945810556411743, Accuracy = 0.45652174949645996
Training iter #11000:   Batch Loss = 0.707941, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7786513566970825, Accuracy = 0.489130437374115
Training iter #11100:   Batch Loss = 0.641782, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.777727484703064, Accuracy = 0.47826087474823
Training iter #11200:   Batch Loss = 0.700579, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7695659399032593, Accuracy = 0.54347825050354
Training iter #11300:   Batch Loss = 0.635665, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7714812159538269, Accuracy = 0.489130437374115
Training iter #11400:   Batch Loss = 0.676617, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7756232023239136, Accuracy = 0.489130437374115
Training iter #11500:   Batch Loss = 0.695592, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7619172930717468, Accuracy = 0.532608687877655
Training iter #11600:   Batch Loss = 0.639387, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.762398362159729, Accuracy = 0.532608687877655
Training iter #11700:   Batch Loss = 0.724647, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7813648581504822, Accuracy = 0.47826087474823
Training iter #11800:   Batch Loss = 0.701170, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7686304450035095, Accuracy = 0.532608687877655
Training iter #11900:   Batch Loss = 0.639674, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7867581248283386, Accuracy = 0.47826087474823
Training iter #12000:   Batch Loss = 0.680272, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.787409245967865, Accuracy = 0.47826087474823
Training iter #12100:   Batch Loss = 0.591681, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.8385785818099976, Accuracy = 0.3695652186870575
Training iter #12200:   Batch Loss = 0.766917, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7966228723526001, Accuracy = 0.46739131212234497
Training iter #12300:   Batch Loss = 0.684917, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.8017104864120483, Accuracy = 0.44565218687057495
Training iter #12400:   Batch Loss = 0.780178, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7074185609817505, Accuracy = 0.6413043737411499
Training iter #12500:   Batch Loss = 0.781388, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.706602156162262, Accuracy = 0.6304348111152649
Training iter #12600:   Batch Loss = 0.769304, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6970134973526001, Accuracy = 0.6304348111152649
Training iter #12700:   Batch Loss = 0.777382, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.6934065818786621, Accuracy = 0.6304348111152649
Training iter #12800:   Batch Loss = 0.740721, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.704183042049408, Accuracy = 0.6304348111152649
Training iter #12900:   Batch Loss = 0.702195, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7236449718475342, Accuracy = 0.6304348111152649
Training iter #13000:   Batch Loss = 0.725956, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7417821884155273, Accuracy = 0.3804347813129425
Training iter #13100:   Batch Loss = 0.707830, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7562288641929626, Accuracy = 0.3695652186870575
Training iter #13200:   Batch Loss = 0.714395, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.763542890548706, Accuracy = 0.3695652186870575
Training iter #13300:   Batch Loss = 0.744404, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7625898122787476, Accuracy = 0.3695652186870575
Training iter #13400:   Batch Loss = 0.725608, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7590647339820862, Accuracy = 0.3804347813129425
Training iter #13500:   Batch Loss = 0.703242, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7517927289009094, Accuracy = 0.3804347813129425
Training iter #13600:   Batch Loss = 0.727466, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.742032527923584, Accuracy = 0.3586956560611725
Training iter #13700:   Batch Loss = 0.708271, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7357452511787415, Accuracy = 0.42391303181648254
Training iter #13800:   Batch Loss = 0.718768, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7277856469154358, Accuracy = 0.46739131212234497
Training iter #13900:   Batch Loss = 0.713189, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7241995930671692, Accuracy = 0.510869562625885
Training iter #14000:   Batch Loss = 0.710486, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7219284772872925, Accuracy = 0.5652173757553101
Training iter #14100:   Batch Loss = 0.718855, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7195570468902588, Accuracy = 0.5978260636329651
Training iter #14200:   Batch Loss = 0.718601, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7205681204795837, Accuracy = 0.5652173757553101
Training iter #14300:   Batch Loss = 0.714156, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7222174406051636, Accuracy = 0.554347813129425
Training iter #14400:   Batch Loss = 0.722934, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7243574857711792, Accuracy = 0.54347825050354
Training iter #14500:   Batch Loss = 0.678540, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7292177081108093, Accuracy = 0.52173912525177
Training iter #14600:   Batch Loss = 0.691317, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7338294386863708, Accuracy = 0.5
Training iter #14700:   Batch Loss = 0.707577, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7383098006248474, Accuracy = 0.47826087474823
Training iter #14800:   Batch Loss = 0.692819, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7417311668395996, Accuracy = 0.47826087474823
Training iter #14900:   Batch Loss = 0.728116, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7425309419631958, Accuracy = 0.489130437374115
Training iter #15000:   Batch Loss = 0.711494, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7425544857978821, Accuracy = 0.489130437374115
Training iter #15100:   Batch Loss = 0.687698, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7465912103652954, Accuracy = 0.489130437374115
Training iter #15200:   Batch Loss = 0.707010, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7477636933326721, Accuracy = 0.489130437374115
Training iter #15300:   Batch Loss = 0.680250, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7492012977600098, Accuracy = 0.489130437374115
Training iter #15400:   Batch Loss = 0.680972, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7559326887130737, Accuracy = 0.489130437374115
Training iter #15500:   Batch Loss = 0.705384, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7570080161094666, Accuracy = 0.489130437374115
Training iter #15600:   Batch Loss = 0.678789, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7599026560783386, Accuracy = 0.489130437374115
Training iter #15700:   Batch Loss = 0.726384, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7572991251945496, Accuracy = 0.5
Training iter #15800:   Batch Loss = 0.708328, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7516494393348694, Accuracy = 0.489130437374115
Training iter #15900:   Batch Loss = 0.672886, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7540498375892639, Accuracy = 0.489130437374115
Training iter #16000:   Batch Loss = 0.696790, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7557413578033447, Accuracy = 0.5
Training iter #16100:   Batch Loss = 0.681150, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7564002871513367, Accuracy = 0.5
Training iter #16200:   Batch Loss = 0.688434, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7603732347488403, Accuracy = 0.489130437374115
Training iter #16300:   Batch Loss = 0.709423, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7612069845199585, Accuracy = 0.489130437374115
Training iter #16400:   Batch Loss = 0.673267, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7641972899436951, Accuracy = 0.47826087474823
Training iter #16500:   Batch Loss = 0.727004, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7543032169342041, Accuracy = 0.5
Training iter #16600:   Batch Loss = 0.698698, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7436689734458923, Accuracy = 0.532608687877655
Training iter #16700:   Batch Loss = 0.658385, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7491796612739563, Accuracy = 0.5
Training iter #16800:   Batch Loss = 0.696172, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7549346089363098, Accuracy = 0.5
Training iter #16900:   Batch Loss = 0.662199, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7611221075057983, Accuracy = 0.489130437374115
Training iter #17000:   Batch Loss = 0.707874, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7654848098754883, Accuracy = 0.47826087474823
Training iter #17100:   Batch Loss = 0.719532, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7582100033760071, Accuracy = 0.510869562625885
Training iter #17200:   Batch Loss = 0.673406, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7530169486999512, Accuracy = 0.5
Training iter #17300:   Batch Loss = 0.724558, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7499651908874512, Accuracy = 0.5
Training iter #17400:   Batch Loss = 0.699175, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7430028915405273, Accuracy = 0.532608687877655
Training iter #17500:   Batch Loss = 0.664194, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7516965866088867, Accuracy = 0.5
Training iter #17600:   Batch Loss = 0.690636, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7572419047355652, Accuracy = 0.510869562625885
Training iter #17700:   Batch Loss = 0.652741, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7634844779968262, Accuracy = 0.489130437374115
Training iter #17800:   Batch Loss = 0.710161, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7632749080657959, Accuracy = 0.489130437374115
Training iter #17900:   Batch Loss = 0.721383, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7512686848640442, Accuracy = 0.52173912525177
Training iter #18000:   Batch Loss = 0.677128, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7447983622550964, Accuracy = 0.52173912525177
Training iter #18100:   Batch Loss = 0.717010, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7459661364555359, Accuracy = 0.510869562625885
Training iter #18200:   Batch Loss = 0.690589, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7432316541671753, Accuracy = 0.52173912525177
Training iter #18300:   Batch Loss = 0.666615, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7560023665428162, Accuracy = 0.52173912525177
Training iter #18400:   Batch Loss = 0.691271, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7598118782043457, Accuracy = 0.5
Training iter #18500:   Batch Loss = 0.652048, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7566748857498169, Accuracy = 0.510869562625885
Training iter #18600:   Batch Loss = 0.701471, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.751548707485199, Accuracy = 0.52173912525177
Training iter #18700:   Batch Loss = 0.721470, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7430883049964905, Accuracy = 0.52173912525177
Training iter #18800:   Batch Loss = 0.679320, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7420929670333862, Accuracy = 0.532608687877655
Training iter #18900:   Batch Loss = 0.727054, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7468527555465698, Accuracy = 0.52173912525177
Training iter #19000:   Batch Loss = 0.689210, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7456223368644714, Accuracy = 0.52173912525177
Training iter #19100:   Batch Loss = 0.661923, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7531588673591614, Accuracy = 0.52173912525177
Training iter #19200:   Batch Loss = 0.698941, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7455456852912903, Accuracy = 0.52173912525177
Training iter #19300:   Batch Loss = 0.648331, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.741205096244812, Accuracy = 0.52173912525177
Training iter #19400:   Batch Loss = 0.691405, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7531619071960449, Accuracy = 0.52173912525177
Training iter #19500:   Batch Loss = 0.721308, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.74836266040802, Accuracy = 0.52173912525177
Training iter #19600:   Batch Loss = 0.674815, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7441845536231995, Accuracy = 0.52173912525177
Training iter #19700:   Batch Loss = 0.728069, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7396440505981445, Accuracy = 0.554347813129425
Training iter #19800:   Batch Loss = 0.675727, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7347946763038635, Accuracy = 0.5652173757553101
Training iter #19900:   Batch Loss = 0.647129, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7533005475997925, Accuracy = 0.52173912525177
Training iter #20000:   Batch Loss = 0.689470, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7445473074913025, Accuracy = 0.532608687877655
Training iter #20100:   Batch Loss = 0.638278, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7390255928039551, Accuracy = 0.554347813129425
Training iter #20200:   Batch Loss = 0.707850, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.745163083076477, Accuracy = 0.532608687877655
Training iter #20300:   Batch Loss = 0.717292, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.73332679271698, Accuracy = 0.5652173757553101
Training iter #20400:   Batch Loss = 0.686105, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7494937181472778, Accuracy = 0.52173912525177
Training iter #20500:   Batch Loss = 0.751389, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7383456230163574, Accuracy = 0.554347813129425
Training iter #20600:   Batch Loss = 0.670926, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.729955792427063, Accuracy = 0.5652173757553101
Training iter #20700:   Batch Loss = 0.645777, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7524293661117554, Accuracy = 0.510869562625885
Training iter #20800:   Batch Loss = 0.663041, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7396337985992432, Accuracy = 0.554347813129425
Training iter #20900:   Batch Loss = 0.633099, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7296056151390076, Accuracy = 0.5652173757553101
Training iter #21000:   Batch Loss = 0.697816, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7395803332328796, Accuracy = 0.54347825050354
Training iter #21100:   Batch Loss = 0.710773, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7337328791618347, Accuracy = 0.554347813129425
Training iter #21200:   Batch Loss = 0.697006, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.750584602355957, Accuracy = 0.52173912525177
Training iter #21300:   Batch Loss = 0.761863, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7242813110351562, Accuracy = 0.5869565010070801
Training iter #21400:   Batch Loss = 0.663131, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7236031293869019, Accuracy = 0.5869565010070801
Training iter #21500:   Batch Loss = 0.636234, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7543760538101196, Accuracy = 0.52173912525177
Training iter #21600:   Batch Loss = 0.667459, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7485402226448059, Accuracy = 0.510869562625885
Training iter #21700:   Batch Loss = 0.654910, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7255011200904846, Accuracy = 0.5869565010070801
Training iter #21800:   Batch Loss = 0.697280, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7283151149749756, Accuracy = 0.5760869383811951
Training iter #21900:   Batch Loss = 0.694655, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.735480010509491, Accuracy = 0.54347825050354
Training iter #22000:   Batch Loss = 0.668308, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7636349201202393, Accuracy = 0.47826087474823
Training iter #22100:   Batch Loss = 0.756963, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7384017705917358, Accuracy = 0.52173912525177
Training iter #22200:   Batch Loss = 0.664060, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7262871265411377, Accuracy = 0.5760869383811951
Training iter #22300:   Batch Loss = 0.641732, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7279122471809387, Accuracy = 0.5760869383811951
Training iter #22400:   Batch Loss = 0.670388, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7311545610427856, Accuracy = 0.5652173757553101
Training iter #22500:   Batch Loss = 0.671289, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7347610592842102, Accuracy = 0.532608687877655
Training iter #22600:   Batch Loss = 0.710780, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7620695233345032, Accuracy = 0.510869562625885
Training iter #22700:   Batch Loss = 0.713286, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7282522320747375, Accuracy = 0.554347813129425
Training iter #22800:   Batch Loss = 0.667401, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7260849475860596, Accuracy = 0.5760869383811951
Training iter #22900:   Batch Loss = 0.741518, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7268821597099304, Accuracy = 0.5760869383811951
Training iter #23000:   Batch Loss = 0.659501, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7312423586845398, Accuracy = 0.554347813129425
Training iter #23100:   Batch Loss = 0.649625, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7572734355926514, Accuracy = 0.532608687877655
Training iter #23200:   Batch Loss = 0.666795, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.748281717300415, Accuracy = 0.52173912525177
Training iter #23300:   Batch Loss = 0.678862, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7252952456474304, Accuracy = 0.5869565010070801
Training iter #23400:   Batch Loss = 0.715856, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7252820134162903, Accuracy = 0.5869565010070801
Training iter #23500:   Batch Loss = 0.705502, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7284033298492432, Accuracy = 0.5652173757553101
Training iter #23600:   Batch Loss = 0.669060, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7407543659210205, Accuracy = 0.52173912525177
Training iter #23700:   Batch Loss = 0.694490, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7557970285415649, Accuracy = 0.489130437374115
Training iter #23800:   Batch Loss = 0.655923, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7612196207046509, Accuracy = 0.47826087474823
Training iter #23900:   Batch Loss = 0.671739, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7542307376861572, Accuracy = 0.52173912525177
Training iter #24000:   Batch Loss = 0.669007, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.744198739528656, Accuracy = 0.52173912525177
Training iter #24100:   Batch Loss = 0.659792, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7371581792831421, Accuracy = 0.554347813129425
Training iter #24200:   Batch Loss = 0.708041, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7361454367637634, Accuracy = 0.5652173757553101
Training iter #24300:   Batch Loss = 0.689348, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7374356985092163, Accuracy = 0.554347813129425
Training iter #24400:   Batch Loss = 0.660135, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7516281604766846, Accuracy = 0.510869562625885
Training iter #24500:   Batch Loss = 0.693599, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7609244585037231, Accuracy = 0.47826087474823
Training iter #24600:   Batch Loss = 0.625706, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7466603517532349, Accuracy = 0.510869562625885
Training iter #24700:   Batch Loss = 0.649676, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7379688620567322, Accuracy = 0.54347825050354
Training iter #24800:   Batch Loss = 0.683266, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7335934638977051, Accuracy = 0.5652173757553101
Training iter #24900:   Batch Loss = 0.645621, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7334399819374084, Accuracy = 0.554347813129425
Training iter #25000:   Batch Loss = 0.711874, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7392638325691223, Accuracy = 0.532608687877655
Training iter #25100:   Batch Loss = 0.681843, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7387913465499878, Accuracy = 0.510869562625885
Training iter #25200:   Batch Loss = 0.642503, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7451305985450745, Accuracy = 0.52173912525177
Training iter #25300:   Batch Loss = 0.688719, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.737053632736206, Accuracy = 0.532608687877655
Training iter #25400:   Batch Loss = 0.596559, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7274723052978516, Accuracy = 0.532608687877655
Training iter #25500:   Batch Loss = 0.656608, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7285618782043457, Accuracy = 0.5652173757553101
Training iter #25600:   Batch Loss = 0.658990, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7291591763496399, Accuracy = 0.554347813129425
Training iter #25700:   Batch Loss = 0.650165, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7345923781394958, Accuracy = 0.510869562625885
Training iter #25800:   Batch Loss = 0.708018, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7422246336936951, Accuracy = 0.52173912525177
Training iter #25900:   Batch Loss = 0.669911, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7207082509994507, Accuracy = 0.554347813129425
Training iter #26000:   Batch Loss = 0.698799, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7180009484291077, Accuracy = 0.532608687877655
Training iter #26100:   Batch Loss = 0.689325, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7163230776786804, Accuracy = 0.532608687877655
Training iter #26200:   Batch Loss = 0.578107, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.718495786190033, Accuracy = 0.532608687877655
Training iter #26300:   Batch Loss = 0.666362, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7333144545555115, Accuracy = 0.532608687877655
Training iter #26400:   Batch Loss = 0.640397, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7286518812179565, Accuracy = 0.54347825050354
Training iter #26500:   Batch Loss = 0.654654, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.720959484577179, Accuracy = 0.5652173757553101
Training iter #26600:   Batch Loss = 0.697340, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7276656031608582, Accuracy = 0.54347825050354
Training iter #26700:   Batch Loss = 0.689879, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7285741567611694, Accuracy = 0.54347825050354
Training iter #26800:   Batch Loss = 0.677029, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7269021272659302, Accuracy = 0.554347813129425
Training iter #26900:   Batch Loss = 0.667770, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7210251092910767, Accuracy = 0.52173912525177
Training iter #27000:   Batch Loss = 0.573581, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7136569023132324, Accuracy = 0.532608687877655
Training iter #27100:   Batch Loss = 0.650909, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7173476815223694, Accuracy = 0.532608687877655
Training iter #27200:   Batch Loss = 0.622771, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7135252356529236, Accuracy = 0.5869565010070801
Training iter #27300:   Batch Loss = 0.632001, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7224350571632385, Accuracy = 0.52173912525177
Training iter #27400:   Batch Loss = 0.707873, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7211415767669678, Accuracy = 0.532608687877655
Training iter #27500:   Batch Loss = 0.682068, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7027948498725891, Accuracy = 0.5869565010070801
Training iter #27600:   Batch Loss = 0.686895, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.699478030204773, Accuracy = 0.5652173757553101
Training iter #27700:   Batch Loss = 0.667157, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7041489481925964, Accuracy = 0.532608687877655
Training iter #27800:   Batch Loss = 0.568602, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7159683108329773, Accuracy = 0.52173912525177
Training iter #27900:   Batch Loss = 0.643346, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7427743077278137, Accuracy = 0.489130437374115
Training iter #28000:   Batch Loss = 0.639854, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7185283303260803, Accuracy = 0.5760869383811951
Training iter #28100:   Batch Loss = 0.646258, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7097418308258057, Accuracy = 0.5978260636329651
Training iter #28200:   Batch Loss = 0.705000, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7192345261573792, Accuracy = 0.5869565010070801
Training iter #28300:   Batch Loss = 0.685437, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7223023772239685, Accuracy = 0.532608687877655
Training iter #28400:   Batch Loss = 0.640988, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7255973815917969, Accuracy = 0.532608687877655
Training iter #28500:   Batch Loss = 0.657806, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7218412756919861, Accuracy = 0.510869562625885
Training iter #28600:   Batch Loss = 0.587978, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7146918177604675, Accuracy = 0.54347825050354
Training iter #28700:   Batch Loss = 0.635907, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7205641269683838, Accuracy = 0.52173912525177
Training iter #28800:   Batch Loss = 0.628928, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7183690667152405, Accuracy = 0.5760869383811951
Training iter #28900:   Batch Loss = 0.629573, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7154507040977478, Accuracy = 0.554347813129425
Training iter #29000:   Batch Loss = 0.703351, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7187720537185669, Accuracy = 0.554347813129425
Training iter #29100:   Batch Loss = 0.672283, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7000835537910461, Accuracy = 0.5869565010070801
Training iter #29200:   Batch Loss = 0.647204, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6988871693611145, Accuracy = 0.5760869383811951
Training iter #29300:   Batch Loss = 0.670286, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7148545980453491, Accuracy = 0.52173912525177
Training iter #29400:   Batch Loss = 0.561653, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7162163853645325, Accuracy = 0.54347825050354
Training iter #29500:   Batch Loss = 0.651565, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7185198068618774, Accuracy = 0.54347825050354
Training iter #29600:   Batch Loss = 0.616905, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.710263729095459, Accuracy = 0.5978260636329651
Training iter #29700:   Batch Loss = 0.634242, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7104532122612, Accuracy = 0.532608687877655
Training iter #29800:   Batch Loss = 0.712111, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7174088954925537, Accuracy = 0.532608687877655
Training iter #29900:   Batch Loss = 0.666934, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7038965821266174, Accuracy = 0.6195651888847351
Training iter #30000:   Batch Loss = 0.631971, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7070648074150085, Accuracy = 0.54347825050354
Training iter #30100:   Batch Loss = 0.653847, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7120950222015381, Accuracy = 0.52173912525177
Training iter #30200:   Batch Loss = 0.567373, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.7136791348457336, Accuracy = 0.532608687877655
Training iter #30300:   Batch Loss = 0.674679, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7203158736228943, Accuracy = 0.54347825050354
Training iter #30400:   Batch Loss = 0.638906, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7202228307723999, Accuracy = 0.5760869383811951
Training iter #30500:   Batch Loss = 0.639129, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6911159157752991, Accuracy = 0.5869565010070801
Training iter #30600:   Batch Loss = 0.715615, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.691194474697113, Accuracy = 0.5869565010070801
Training iter #30700:   Batch Loss = 0.638267, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6947100162506104, Accuracy = 0.5869565010070801
Training iter #30800:   Batch Loss = 0.634249, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7358209490776062, Accuracy = 0.532608687877655
Training iter #30900:   Batch Loss = 0.643219, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7344980835914612, Accuracy = 0.532608687877655
Training iter #31000:   Batch Loss = 0.588352, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7165988683700562, Accuracy = 0.5869565010070801
Training iter #31100:   Batch Loss = 0.666122, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7216379642486572, Accuracy = 0.54347825050354
Training iter #31200:   Batch Loss = 0.657711, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7221388220787048, Accuracy = 0.5652173757553101
Training iter #31300:   Batch Loss = 0.624329, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7283326983451843, Accuracy = 0.554347813129425
Training iter #31400:   Batch Loss = 0.699534, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7320415377616882, Accuracy = 0.554347813129425
Training iter #31500:   Batch Loss = 0.676287, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7211014032363892, Accuracy = 0.5760869383811951
Training iter #31600:   Batch Loss = 0.632940, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7091007828712463, Accuracy = 0.5869565010070801
Training iter #31700:   Batch Loss = 0.683948, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7068485021591187, Accuracy = 0.5869565010070801
Training iter #31800:   Batch Loss = 0.612013, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7049336433410645, Accuracy = 0.6086956262588501
Training iter #31900:   Batch Loss = 0.647992, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7476316690444946, Accuracy = 0.532608687877655
Training iter #32000:   Batch Loss = 0.716067, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7457235455513, Accuracy = 0.532608687877655
Training iter #32100:   Batch Loss = 0.642269, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7069509029388428, Accuracy = 0.5978260636329651
Training iter #32200:   Batch Loss = 0.718532, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6974431276321411, Accuracy = 0.6086956262588501
Training iter #32300:   Batch Loss = 0.637148, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6975945234298706, Accuracy = 0.6086956262588501
Training iter #32400:   Batch Loss = 0.663974, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7102952599525452, Accuracy = 0.5869565010070801
Training iter #32500:   Batch Loss = 0.667210, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7220733761787415, Accuracy = 0.5652173757553101
Training iter #32600:   Batch Loss = 0.590750, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7276983857154846, Accuracy = 0.532608687877655
Training iter #32700:   Batch Loss = 0.643559, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7373501062393188, Accuracy = 0.54347825050354
Training iter #32800:   Batch Loss = 0.690371, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.730729877948761, Accuracy = 0.52173912525177
Training iter #32900:   Batch Loss = 0.624994, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7235409021377563, Accuracy = 0.5760869383811951
Training iter #33000:   Batch Loss = 0.712654, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7156267762184143, Accuracy = 0.5760869383811951
Training iter #33100:   Batch Loss = 0.641192, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7099065184593201, Accuracy = 0.6195651888847351
Training iter #33200:   Batch Loss = 0.617952, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7370849251747131, Accuracy = 0.52173912525177
Training iter #33300:   Batch Loss = 0.666112, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.748100757598877, Accuracy = 0.510869562625885
Training iter #33400:   Batch Loss = 0.566163, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.7205275893211365, Accuracy = 0.52173912525177
Training iter #33500:   Batch Loss = 0.677572, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7066971063613892, Accuracy = 0.6304348111152649
Training iter #33600:   Batch Loss = 0.666031, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7028026580810547, Accuracy = 0.6195651888847351
Training iter #33700:   Batch Loss = 0.646282, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7216050624847412, Accuracy = 0.532608687877655
Training iter #33800:   Batch Loss = 0.745828, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7494255900382996, Accuracy = 0.5
Training iter #33900:   Batch Loss = 0.657461, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7200533747673035, Accuracy = 0.554347813129425
Training iter #34000:   Batch Loss = 0.630212, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7092879414558411, Accuracy = 0.5652173757553101
Training iter #34100:   Batch Loss = 0.655933, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7057778835296631, Accuracy = 0.6195651888847351
Training iter #34200:   Batch Loss = 0.569591, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7050907611846924, Accuracy = 0.6086956262588501
Training iter #34300:   Batch Loss = 0.653613, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7221150994300842, Accuracy = 0.52173912525177
Training iter #34400:   Batch Loss = 0.683038, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7249295115470886, Accuracy = 0.5
Training iter #34500:   Batch Loss = 0.649006, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7074281573295593, Accuracy = 0.5869565010070801
Training iter #34600:   Batch Loss = 0.732190, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7025143504142761, Accuracy = 0.6086956262588501
Training iter #34700:   Batch Loss = 0.624272, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.698962926864624, Accuracy = 0.6086956262588501
Training iter #34800:   Batch Loss = 0.619771, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.714655876159668, Accuracy = 0.532608687877655
Training iter #34900:   Batch Loss = 0.670640, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7369664311408997, Accuracy = 0.510869562625885
Training iter #35000:   Batch Loss = 0.578002, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7110803723335266, Accuracy = 0.5760869383811951
Training iter #35100:   Batch Loss = 0.663343, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6966745853424072, Accuracy = 0.5869565010070801
Training iter #35200:   Batch Loss = 0.661111, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6939278841018677, Accuracy = 0.6086956262588501
Training iter #35300:   Batch Loss = 0.636934, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7038747072219849, Accuracy = 0.5760869383811951
Training iter #35400:   Batch Loss = 0.726259, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7308040261268616, Accuracy = 0.510869562625885
Training iter #35500:   Batch Loss = 0.633115, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6966705322265625, Accuracy = 0.5978260636329651
Training iter #35600:   Batch Loss = 0.631880, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6982300877571106, Accuracy = 0.6086956262588501
Training iter #35700:   Batch Loss = 0.654762, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7132446765899658, Accuracy = 0.6086956262588501
Training iter #35800:   Batch Loss = 0.594867, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.706595242023468, Accuracy = 0.5760869383811951
Training iter #35900:   Batch Loss = 0.660716, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7332246899604797, Accuracy = 0.554347813129425
Training iter #36000:   Batch Loss = 0.669492, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7085812091827393, Accuracy = 0.54347825050354
Training iter #36100:   Batch Loss = 0.656029, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7225359082221985, Accuracy = 0.5652173757553101
Training iter #36200:   Batch Loss = 0.712198, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7239366173744202, Accuracy = 0.5652173757553101
Training iter #36300:   Batch Loss = 0.617973, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6996662020683289, Accuracy = 0.532608687877655
Training iter #36400:   Batch Loss = 0.626313, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7034358382225037, Accuracy = 0.554347813129425
Training iter #36500:   Batch Loss = 0.636671, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7085683345794678, Accuracy = 0.5760869383811951
Training iter #36600:   Batch Loss = 0.613903, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6978185176849365, Accuracy = 0.5869565010070801
Training iter #36700:   Batch Loss = 0.672053, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7074220180511475, Accuracy = 0.6086956262588501
Training iter #36800:   Batch Loss = 0.652792, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7029756307601929, Accuracy = 0.6195651888847351
Training iter #36900:   Batch Loss = 0.616692, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7161887884140015, Accuracy = 0.5652173757553101
Training iter #37000:   Batch Loss = 0.672745, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7265352010726929, Accuracy = 0.5760869383811951
Training iter #37100:   Batch Loss = 0.590650, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7028816938400269, Accuracy = 0.5760869383811951
Training iter #37200:   Batch Loss = 0.631677, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7208777666091919, Accuracy = 0.554347813129425
Training iter #37300:   Batch Loss = 0.629147, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7331859469413757, Accuracy = 0.554347813129425
Training iter #37400:   Batch Loss = 0.589449, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7039469480514526, Accuracy = 0.5869565010070801
Training iter #37500:   Batch Loss = 0.668384, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7034040093421936, Accuracy = 0.6086956262588501
Training iter #37600:   Batch Loss = 0.642958, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6851204633712769, Accuracy = 0.6304348111152649
Training iter #37700:   Batch Loss = 0.645054, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7158066630363464, Accuracy = 0.5652173757553101
Training iter #37800:   Batch Loss = 0.687170, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7491007447242737, Accuracy = 0.52173912525177
Training iter #37900:   Batch Loss = 0.584369, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7142974734306335, Accuracy = 0.5652173757553101
Training iter #38000:   Batch Loss = 0.614229, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.692207932472229, Accuracy = 0.5869565010070801
Training iter #38100:   Batch Loss = 0.624821, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6888437271118164, Accuracy = 0.6630434989929199
Training iter #38200:   Batch Loss = 0.609891, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6837849617004395, Accuracy = 0.6086956262588501
Training iter #38300:   Batch Loss = 0.659767, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7038984894752502, Accuracy = 0.5652173757553101
Training iter #38400:   Batch Loss = 0.647602, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7104111313819885, Accuracy = 0.554347813129425
Training iter #38500:   Batch Loss = 0.620159, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7364307045936584, Accuracy = 0.54347825050354
Training iter #38600:   Batch Loss = 0.679461, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7050904631614685, Accuracy = 0.5760869383811951
Training iter #38700:   Batch Loss = 0.546118, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.6915090680122375, Accuracy = 0.6195651888847351
Training iter #38800:   Batch Loss = 0.624232, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7042312622070312, Accuracy = 0.5869565010070801
Training iter #38900:   Batch Loss = 0.631025, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7013100981712341, Accuracy = 0.5760869383811951
Training iter #39000:   Batch Loss = 0.587719, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6989003419876099, Accuracy = 0.5978260636329651
Training iter #39100:   Batch Loss = 0.668121, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6842695474624634, Accuracy = 0.6086956262588501
Training iter #39200:   Batch Loss = 0.628591, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6561024785041809, Accuracy = 0.6195651888847351
Training iter #39300:   Batch Loss = 0.667914, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6621799468994141, Accuracy = 0.6195651888847351
Training iter #39400:   Batch Loss = 0.645136, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6787666082382202, Accuracy = 0.6086956262588501
Training iter #39500:   Batch Loss = 0.596692, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6712881326675415, Accuracy = 0.6195651888847351
Training iter #39600:   Batch Loss = 0.620022, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6810210347175598, Accuracy = 0.6304348111152649
Training iter #39700:   Batch Loss = 0.607381, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6697149872779846, Accuracy = 0.6086956262588501
Training iter #39800:   Batch Loss = 0.613680, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6801357269287109, Accuracy = 0.6195651888847351
Training iter #39900:   Batch Loss = 0.671773, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7106546759605408, Accuracy = 0.5652173757553101
Training iter #40000:   Batch Loss = 0.659518, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7011609077453613, Accuracy = 0.5652173757553101
Training iter #40100:   Batch Loss = 0.611326, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7207463383674622, Accuracy = 0.5978260636329651
Training iter #40200:   Batch Loss = 0.668953, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6677443385124207, Accuracy = 0.6413043737411499
Training iter #40300:   Batch Loss = 0.551234, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6914405822753906, Accuracy = 0.6521739363670349
Training iter #40400:   Batch Loss = 0.597078, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6794912815093994, Accuracy = 0.5760869383811951
Training iter #40500:   Batch Loss = 0.624614, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6935733556747437, Accuracy = 0.6086956262588501
Training iter #40600:   Batch Loss = 0.583111, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6989538669586182, Accuracy = 0.5869565010070801
Training iter #40700:   Batch Loss = 0.715631, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7125319838523865, Accuracy = 0.532608687877655
Training iter #40800:   Batch Loss = 0.709665, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6757460236549377, Accuracy = 0.6086956262588501
Training iter #40900:   Batch Loss = 0.664293, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6688638925552368, Accuracy = 0.6521739363670349
Training iter #41000:   Batch Loss = 0.626361, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6755756735801697, Accuracy = 0.6086956262588501
Training iter #41100:   Batch Loss = 0.628211, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.675308883190155, Accuracy = 0.5978260636329651
Training iter #41200:   Batch Loss = 0.583167, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6873186230659485, Accuracy = 0.5869565010070801
Training iter #41300:   Batch Loss = 0.629762, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6897310614585876, Accuracy = 0.6195651888847351
Training iter #41400:   Batch Loss = 0.583854, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6902163624763489, Accuracy = 0.5869565010070801
Training iter #41500:   Batch Loss = 0.678714, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6753131151199341, Accuracy = 0.6413043737411499
Training iter #41600:   Batch Loss = 0.674679, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6504693627357483, Accuracy = 0.717391312122345
Training iter #41700:   Batch Loss = 0.613331, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6510950922966003, Accuracy = 0.6630434989929199
Training iter #41800:   Batch Loss = 0.613392, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6643894910812378, Accuracy = 0.6413043737411499
Training iter #41900:   Batch Loss = 0.549260, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6720685362815857, Accuracy = 0.5978260636329651
Training iter #42000:   Batch Loss = 0.592558, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.703508734703064, Accuracy = 0.5760869383811951
Training iter #42100:   Batch Loss = 0.607721, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6613818407058716, Accuracy = 0.6413043737411499
Training iter #42200:   Batch Loss = 0.568770, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6530917882919312, Accuracy = 0.6739130616188049
Training iter #42300:   Batch Loss = 0.684074, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.654801070690155, Accuracy = 0.6630434989929199
Training iter #42400:   Batch Loss = 0.658762, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6645075082778931, Accuracy = 0.6739130616188049
Training iter #42500:   Batch Loss = 0.604614, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.689419686794281, Accuracy = 0.5978260636329651
Training iter #42600:   Batch Loss = 0.602285, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6672307252883911, Accuracy = 0.6413043737411499
Training iter #42700:   Batch Loss = 0.588194, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6417229175567627, Accuracy = 0.6847826242446899
Training iter #42800:   Batch Loss = 0.626606, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6699551343917847, Accuracy = 0.6304348111152649
Training iter #42900:   Batch Loss = 0.571835, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6654359102249146, Accuracy = 0.6413043737411499
Training iter #43000:   Batch Loss = 0.566837, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6851820945739746, Accuracy = 0.5978260636329651
Training iter #43100:   Batch Loss = 0.679995, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6930409669876099, Accuracy = 0.5978260636329651
Training iter #43200:   Batch Loss = 0.696739, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6416162252426147, Accuracy = 0.6739130616188049
Training iter #43300:   Batch Loss = 0.620149, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6532268524169922, Accuracy = 0.6304348111152649
Training iter #43400:   Batch Loss = 0.606859, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6659683585166931, Accuracy = 0.6195651888847351
Training iter #43500:   Batch Loss = 0.568340, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7222417593002319, Accuracy = 0.5978260636329651
Training iter #43600:   Batch Loss = 0.708383, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7057350873947144, Accuracy = 0.5652173757553101
Training iter #43700:   Batch Loss = 0.658485, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7017837166786194, Accuracy = 0.6630434989929199
Training iter #43800:   Batch Loss = 0.602971, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6863030791282654, Accuracy = 0.6195651888847351
Training iter #43900:   Batch Loss = 0.678702, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7309542894363403, Accuracy = 0.5869565010070801
Training iter #44000:   Batch Loss = 0.703832, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.6737895011901855, Accuracy = 0.5760869383811951
Training iter #44100:   Batch Loss = 0.603404, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6860019564628601, Accuracy = 0.6413043737411499
Training iter #44200:   Batch Loss = 0.602470, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6308876872062683, Accuracy = 0.6739130616188049
Training iter #44300:   Batch Loss = 0.649050, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6601399183273315, Accuracy = 0.6847826242446899
Training iter #44400:   Batch Loss = 0.659638, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7231426239013672, Accuracy = 0.5760869383811951
Training iter #44500:   Batch Loss = 0.692943, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.731784999370575, Accuracy = 0.554347813129425
Training iter #44600:   Batch Loss = 0.671854, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.712435781955719, Accuracy = 0.5978260636329651
Training iter #44700:   Batch Loss = 0.694989, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6872520446777344, Accuracy = 0.5869565010070801
Training iter #44800:   Batch Loss = 0.667344, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6749345064163208, Accuracy = 0.6195651888847351
Training iter #44900:   Batch Loss = 0.635969, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7289944291114807, Accuracy = 0.6086956262588501
Training iter #45000:   Batch Loss = 0.637344, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6647823452949524, Accuracy = 0.5978260636329651
Training iter #45100:   Batch Loss = 0.609720, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6761364936828613, Accuracy = 0.6195651888847351
Training iter #45200:   Batch Loss = 0.634542, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7043939232826233, Accuracy = 0.532608687877655
Training iter #45300:   Batch Loss = 0.688431, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7278639674186707, Accuracy = 0.554347813129425
Training iter #45400:   Batch Loss = 0.583212, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.720715343952179, Accuracy = 0.52173912525177
Training iter #45500:   Batch Loss = 0.708637, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7044694423675537, Accuracy = 0.5652173757553101
Training iter #45600:   Batch Loss = 0.645439, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6723762154579163, Accuracy = 0.6304348111152649
Training iter #45700:   Batch Loss = 0.618511, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6646859645843506, Accuracy = 0.6413043737411499
Training iter #45800:   Batch Loss = 0.632317, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6635576486587524, Accuracy = 0.6195651888847351
Training iter #45900:   Batch Loss = 0.579749, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6684883832931519, Accuracy = 0.6304348111152649
Training iter #46000:   Batch Loss = 0.654572, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6792917847633362, Accuracy = 0.6195651888847351
Training iter #46100:   Batch Loss = 0.644131, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6502654552459717, Accuracy = 0.6521739363670349
Training iter #46200:   Batch Loss = 0.586330, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6712686419487, Accuracy = 0.6521739363670349
Training iter #46300:   Batch Loss = 0.682610, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6682056188583374, Accuracy = 0.6413043737411499
Training iter #46400:   Batch Loss = 0.656058, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6637057065963745, Accuracy = 0.6521739363670349
Training iter #46500:   Batch Loss = 0.568783, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6800042986869812, Accuracy = 0.5978260636329651
Training iter #46600:   Batch Loss = 0.624423, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6597610712051392, Accuracy = 0.6630434989929199
Training iter #46700:   Batch Loss = 0.557120, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.6528027057647705, Accuracy = 0.6847826242446899
Training iter #46800:   Batch Loss = 0.653189, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6520916819572449, Accuracy = 0.6847826242446899
Training iter #46900:   Batch Loss = 0.649935, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6505164504051208, Accuracy = 0.6739130616188049
Training iter #47000:   Batch Loss = 0.598430, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6803598403930664, Accuracy = 0.5978260636329651
Training iter #47100:   Batch Loss = 0.728856, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.681272566318512, Accuracy = 0.6195651888847351
Training iter #47200:   Batch Loss = 0.633454, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6535769104957581, Accuracy = 0.695652186870575
Training iter #47300:   Batch Loss = 0.653546, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6710132956504822, Accuracy = 0.6739130616188049
Training iter #47400:   Batch Loss = 0.608697, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6718375086784363, Accuracy = 0.6086956262588501
Training iter #47500:   Batch Loss = 0.525468, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6949071288108826, Accuracy = 0.6521739363670349
Training iter #47600:   Batch Loss = 0.719013, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.697371780872345, Accuracy = 0.6304348111152649
Training iter #47700:   Batch Loss = 0.682448, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7194167375564575, Accuracy = 0.6304348111152649
Training iter #47800:   Batch Loss = 0.678236, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7122156023979187, Accuracy = 0.5760869383811951
Training iter #47900:   Batch Loss = 0.754989, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7302377820014954, Accuracy = 0.54347825050354
Training iter #48000:   Batch Loss = 0.632191, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.707486629486084, Accuracy = 0.5652173757553101
Training iter #48100:   Batch Loss = 0.693301, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7331050038337708, Accuracy = 0.54347825050354
Training iter #48200:   Batch Loss = 0.705049, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7108368873596191, Accuracy = 0.5652173757553101
Training iter #48300:   Batch Loss = 0.710163, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7056043744087219, Accuracy = 0.5760869383811951
Training iter #48400:   Batch Loss = 0.694855, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7255531549453735, Accuracy = 0.54347825050354
Training iter #48500:   Batch Loss = 0.660259, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.731967568397522, Accuracy = 0.54347825050354
Training iter #48600:   Batch Loss = 0.606585, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7492343187332153, Accuracy = 0.5
Training iter #48700:   Batch Loss = 0.764850, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7283903956413269, Accuracy = 0.52173912525177
Training iter #48800:   Batch Loss = 0.648412, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7156806588172913, Accuracy = 0.52173912525177
Training iter #48900:   Batch Loss = 0.638833, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7223871946334839, Accuracy = 0.54347825050354
Training iter #49000:   Batch Loss = 0.679169, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7274848222732544, Accuracy = 0.52173912525177
Training iter #49100:   Batch Loss = 0.618033, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7286827564239502, Accuracy = 0.52173912525177
Training iter #49200:   Batch Loss = 0.709522, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.739729642868042, Accuracy = 0.46739131212234497
Training iter #49300:   Batch Loss = 0.680930, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7299036979675293, Accuracy = 0.532608687877655
Training iter #49400:   Batch Loss = 0.604616, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7333146929740906, Accuracy = 0.510869562625885
Training iter #49500:   Batch Loss = 0.717823, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7227156758308411, Accuracy = 0.54347825050354
Training iter #49600:   Batch Loss = 0.640729, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7149743437767029, Accuracy = 0.5652173757553101
Training iter #49700:   Batch Loss = 0.635845, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7277110815048218, Accuracy = 0.554347813129425
Training iter #49800:   Batch Loss = 0.654831, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7278613448143005, Accuracy = 0.54347825050354
Training iter #49900:   Batch Loss = 0.639652, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7191824316978455, Accuracy = 0.54347825050354
Training iter #50000:   Batch Loss = 0.709764, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7249749302864075, Accuracy = 0.54347825050354
Training iter #50100:   Batch Loss = 0.660141, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7154014110565186, Accuracy = 0.5652173757553101
Training iter #50200:   Batch Loss = 0.618224, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7510569095611572, Accuracy = 0.489130437374115
Training iter #50300:   Batch Loss = 0.661518, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7231574058532715, Accuracy = 0.554347813129425
Training iter #50400:   Batch Loss = 0.621688, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7110217213630676, Accuracy = 0.5760869383811951
Training iter #50500:   Batch Loss = 0.634840, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7242136001586914, Accuracy = 0.554347813129425
Training iter #50600:   Batch Loss = 0.656620, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7242758870124817, Accuracy = 0.554347813129425
Training iter #50700:   Batch Loss = 0.630139, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7234368920326233, Accuracy = 0.554347813129425
Training iter #50800:   Batch Loss = 0.704679, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.730019211769104, Accuracy = 0.532608687877655
Training iter #50900:   Batch Loss = 0.650110, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7127066254615784, Accuracy = 0.5869565010070801
Training iter #51000:   Batch Loss = 0.638225, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7421633005142212, Accuracy = 0.54347825050354
Training iter #51100:   Batch Loss = 0.659258, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7237176895141602, Accuracy = 0.554347813129425
Training iter #51200:   Batch Loss = 0.583848, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7116670608520508, Accuracy = 0.5978260636329651
Training iter #51300:   Batch Loss = 0.633357, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7238816022872925, Accuracy = 0.54347825050354
Training iter #51400:   Batch Loss = 0.669658, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7181465029716492, Accuracy = 0.554347813129425
Training iter #51500:   Batch Loss = 0.605259, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7197104692459106, Accuracy = 0.554347813129425
Training iter #51600:   Batch Loss = 0.693544, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7272405624389648, Accuracy = 0.532608687877655
Training iter #51700:   Batch Loss = 0.643078, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.710098385810852, Accuracy = 0.5978260636329651
Training iter #51800:   Batch Loss = 0.623364, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7477908134460449, Accuracy = 0.510869562625885
Training iter #51900:   Batch Loss = 0.658255, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7184324860572815, Accuracy = 0.554347813129425
Training iter #52000:   Batch Loss = 0.557535, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7090978622436523, Accuracy = 0.5978260636329651
Training iter #52100:   Batch Loss = 0.648069, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7236400246620178, Accuracy = 0.532608687877655
Training iter #52200:   Batch Loss = 0.651155, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7217846512794495, Accuracy = 0.532608687877655
Training iter #52300:   Batch Loss = 0.606700, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7304848432540894, Accuracy = 0.54347825050354
Training iter #52400:   Batch Loss = 0.700652, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7316837906837463, Accuracy = 0.532608687877655
Training iter #52500:   Batch Loss = 0.633532, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7064382433891296, Accuracy = 0.6304348111152649
Training iter #52600:   Batch Loss = 0.641877, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7390542030334473, Accuracy = 0.5
Training iter #52700:   Batch Loss = 0.669964, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7200579047203064, Accuracy = 0.554347813129425
Training iter #52800:   Batch Loss = 0.553557, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7177482843399048, Accuracy = 0.5652173757553101
Training iter #52900:   Batch Loss = 0.657006, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7321511507034302, Accuracy = 0.532608687877655
Training iter #53000:   Batch Loss = 0.651426, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7145019173622131, Accuracy = 0.5978260636329651
Training iter #53100:   Batch Loss = 0.615626, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7279622554779053, Accuracy = 0.52173912525177
Training iter #53200:   Batch Loss = 0.698053, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7336857914924622, Accuracy = 0.52173912525177
Training iter #53300:   Batch Loss = 0.655383, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7111286520957947, Accuracy = 0.6304348111152649
Training iter #53400:   Batch Loss = 0.633414, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7485578060150146, Accuracy = 0.5
Training iter #53500:   Batch Loss = 0.653030, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7135177850723267, Accuracy = 0.6086956262588501
Training iter #53600:   Batch Loss = 0.544262, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7207286357879639, Accuracy = 0.5652173757553101
Training iter #53700:   Batch Loss = 0.665038, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7369709610939026, Accuracy = 0.510869562625885
Training iter #53800:   Batch Loss = 0.639522, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7087416648864746, Accuracy = 0.6413043737411499
Training iter #53900:   Batch Loss = 0.594168, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7270644307136536, Accuracy = 0.54347825050354
Training iter #54000:   Batch Loss = 0.685700, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7374151945114136, Accuracy = 0.5
Training iter #54100:   Batch Loss = 0.675704, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7113415002822876, Accuracy = 0.6086956262588501
Training iter #54200:   Batch Loss = 0.645400, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7366192936897278, Accuracy = 0.5
Training iter #54300:   Batch Loss = 0.629181, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7176850438117981, Accuracy = 0.5760869383811951
Training iter #54400:   Batch Loss = 0.546510, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7316798567771912, Accuracy = 0.510869562625885
Training iter #54500:   Batch Loss = 0.662544, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.736906111240387, Accuracy = 0.510869562625885
Training iter #54600:   Batch Loss = 0.651920, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.702904999256134, Accuracy = 0.6195651888847351
Training iter #54700:   Batch Loss = 0.599058, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7164667844772339, Accuracy = 0.5978260636329651
Training iter #54800:   Batch Loss = 0.678828, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7434165477752686, Accuracy = 0.5
Training iter #54900:   Batch Loss = 0.655687, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7153816819190979, Accuracy = 0.5978260636329651
Training iter #55000:   Batch Loss = 0.606413, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7279330492019653, Accuracy = 0.54347825050354
Training iter #55100:   Batch Loss = 0.631326, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7090186476707458, Accuracy = 0.6086956262588501
Training iter #55200:   Batch Loss = 0.576070, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7257000803947449, Accuracy = 0.532608687877655
Training iter #55300:   Batch Loss = 0.654675, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7412851452827454, Accuracy = 0.5
Training iter #55400:   Batch Loss = 0.642671, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7032563090324402, Accuracy = 0.6413043737411499
Training iter #55500:   Batch Loss = 0.596163, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7150312066078186, Accuracy = 0.5760869383811951
Training iter #55600:   Batch Loss = 0.707949, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7264151573181152, Accuracy = 0.554347813129425
Training iter #55700:   Batch Loss = 0.655113, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7094541788101196, Accuracy = 0.5760869383811951
Training iter #55800:   Batch Loss = 0.616731, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7260472774505615, Accuracy = 0.532608687877655
Training iter #55900:   Batch Loss = 0.629262, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7108106017112732, Accuracy = 0.5760869383811951
Training iter #56000:   Batch Loss = 0.570026, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7155373096466064, Accuracy = 0.5652173757553101
Training iter #56100:   Batch Loss = 0.638232, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7216007709503174, Accuracy = 0.5652173757553101
Training iter #56200:   Batch Loss = 0.615460, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6929877400398254, Accuracy = 0.6304348111152649
Training iter #56300:   Batch Loss = 0.605212, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6991192698478699, Accuracy = 0.6086956262588501
Training iter #56400:   Batch Loss = 0.679929, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7177567481994629, Accuracy = 0.554347813129425
Training iter #56500:   Batch Loss = 0.664788, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6880841851234436, Accuracy = 0.6195651888847351
Training iter #56600:   Batch Loss = 0.610180, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6881687045097351, Accuracy = 0.6195651888847351
Training iter #56700:   Batch Loss = 0.617274, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6873783469200134, Accuracy = 0.6086956262588501
Training iter #56800:   Batch Loss = 0.568194, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7039408087730408, Accuracy = 0.554347813129425
Training iter #56900:   Batch Loss = 0.678052, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.699504017829895, Accuracy = 0.5869565010070801
Training iter #57000:   Batch Loss = 0.644725, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6830596923828125, Accuracy = 0.6195651888847351
Training iter #57100:   Batch Loss = 0.619908, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6925823092460632, Accuracy = 0.6086956262588501
Training iter #57200:   Batch Loss = 0.671423, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7157838940620422, Accuracy = 0.54347825050354
Training iter #57300:   Batch Loss = 0.661867, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6693502068519592, Accuracy = 0.6521739363670349
Training iter #57400:   Batch Loss = 0.600320, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6652927994728088, Accuracy = 0.6630434989929199
Training iter #57500:   Batch Loss = 0.615794, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6611447930335999, Accuracy = 0.6413043737411499
Training iter #57600:   Batch Loss = 0.568495, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6888536810874939, Accuracy = 0.5978260636329651
Training iter #57700:   Batch Loss = 0.682452, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7049447298049927, Accuracy = 0.6086956262588501
Training iter #57800:   Batch Loss = 0.666473, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6874215006828308, Accuracy = 0.6304348111152649
Training iter #57900:   Batch Loss = 0.618499, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6898705363273621, Accuracy = 0.6195651888847351
Training iter #58000:   Batch Loss = 0.649453, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7081158757209778, Accuracy = 0.6195651888847351
Training iter #58100:   Batch Loss = 0.686280, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6445752382278442, Accuracy = 0.695652186870575
Training iter #58200:   Batch Loss = 0.606751, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6512046456336975, Accuracy = 0.6630434989929199
Training iter #58300:   Batch Loss = 0.626890, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6497036814689636, Accuracy = 0.6739130616188049
Training iter #58400:   Batch Loss = 0.591217, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6862059831619263, Accuracy = 0.6304348111152649
Training iter #58500:   Batch Loss = 0.682148, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7141761779785156, Accuracy = 0.5869565010070801
Training iter #58600:   Batch Loss = 0.694541, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6950340270996094, Accuracy = 0.6195651888847351
Training iter #58700:   Batch Loss = 0.604755, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6859147548675537, Accuracy = 0.6413043737411499
Training iter #58800:   Batch Loss = 0.653890, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6937150359153748, Accuracy = 0.6195651888847351
Training iter #58900:   Batch Loss = 0.694062, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6347376704216003, Accuracy = 0.6847826242446899
Training iter #59000:   Batch Loss = 0.606847, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6631346940994263, Accuracy = 0.6521739363670349
Training iter #59100:   Batch Loss = 0.634754, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6528638005256653, Accuracy = 0.6739130616188049
Training iter #59200:   Batch Loss = 0.557490, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6924393773078918, Accuracy = 0.5760869383811951
Training iter #59300:   Batch Loss = 0.676559, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6978567242622375, Accuracy = 0.6086956262588501
Training iter #59400:   Batch Loss = 0.681285, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6796267032623291, Accuracy = 0.6413043737411499
Training iter #59500:   Batch Loss = 0.634277, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6844412088394165, Accuracy = 0.6304348111152649
Training iter #59600:   Batch Loss = 0.648761, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6945247650146484, Accuracy = 0.6413043737411499
Training iter #59700:   Batch Loss = 0.698302, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.6513276696205139, Accuracy = 0.6847826242446899
Training iter #59800:   Batch Loss = 0.564446, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6653425097465515, Accuracy = 0.6413043737411499
Training iter #59900:   Batch Loss = 0.637430, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6572189927101135, Accuracy = 0.6413043737411499
Training iter #60000:   Batch Loss = 0.542006, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6861526370048523, Accuracy = 0.6304348111152649
Training iter #60100:   Batch Loss = 0.718757, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7022478580474854, Accuracy = 0.5760869383811951
Training iter #60200:   Batch Loss = 0.670164, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6716930866241455, Accuracy = 0.6413043737411499
Training iter #60300:   Batch Loss = 0.629802, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6752877235412598, Accuracy = 0.6304348111152649
Training iter #60400:   Batch Loss = 0.663177, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6743823289871216, Accuracy = 0.6630434989929199
Training iter #60500:   Batch Loss = 0.705701, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6414729952812195, Accuracy = 0.695652186870575
Training iter #60600:   Batch Loss = 0.568952, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6893770098686218, Accuracy = 0.6413043737411499
Training iter #60700:   Batch Loss = 0.628129, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6845868229866028, Accuracy = 0.6413043737411499
Training iter #60800:   Batch Loss = 0.496390, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 0.6932716965675354, Accuracy = 0.6195651888847351
Training iter #60900:   Batch Loss = 0.728653, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6822606325149536, Accuracy = 0.6521739363670349
Training iter #61000:   Batch Loss = 0.674585, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6717328429222107, Accuracy = 0.6413043737411499
Training iter #61100:   Batch Loss = 0.663122, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.6783276796340942, Accuracy = 0.6630434989929199
Training iter #61200:   Batch Loss = 0.685840, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6807687282562256, Accuracy = 0.6304348111152649
Training iter #61300:   Batch Loss = 0.692530, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6439564824104309, Accuracy = 0.695652186870575
Training iter #61400:   Batch Loss = 0.562887, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6817049980163574, Accuracy = 0.6413043737411499
Training iter #61500:   Batch Loss = 0.670842, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6736302375793457, Accuracy = 0.6413043737411499
Training iter #61600:   Batch Loss = 0.527877, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6931993961334229, Accuracy = 0.6304348111152649
Training iter #61700:   Batch Loss = 0.691031, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7036569118499756, Accuracy = 0.5869565010070801
Training iter #61800:   Batch Loss = 0.642544, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6757901310920715, Accuracy = 0.6413043737411499
Training iter #61900:   Batch Loss = 0.609250, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6791156530380249, Accuracy = 0.6521739363670349
Training iter #62000:   Batch Loss = 0.659927, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6616859436035156, Accuracy = 0.6847826242446899
Training iter #62100:   Batch Loss = 0.683657, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6400550007820129, Accuracy = 0.6847826242446899
Training iter #62200:   Batch Loss = 0.529511, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7015022039413452, Accuracy = 0.6304348111152649
Training iter #62300:   Batch Loss = 0.613303, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.696043074131012, Accuracy = 0.6413043737411499
Training iter #62400:   Batch Loss = 0.565731, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6706952452659607, Accuracy = 0.6521739363670349
Training iter #62500:   Batch Loss = 0.647965, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6535142660140991, Accuracy = 0.6847826242446899
Training iter #62600:   Batch Loss = 0.632146, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6935805082321167, Accuracy = 0.5760869383811951
Training iter #62700:   Batch Loss = 0.626720, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6888923645019531, Accuracy = 0.5652173757553101
Training iter #62800:   Batch Loss = 0.658513, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.6954725384712219, Accuracy = 0.6304348111152649
Training iter #62900:   Batch Loss = 0.612473, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6952008008956909, Accuracy = 0.6304348111152649
Training iter #63000:   Batch Loss = 0.572370, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7068786025047302, Accuracy = 0.5760869383811951
Training iter #63100:   Batch Loss = 0.618778, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7174188494682312, Accuracy = 0.5652173757553101
Training iter #63200:   Batch Loss = 0.571222, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6469337344169617, Accuracy = 0.6413043737411499
Training iter #63300:   Batch Loss = 0.616298, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6362470984458923, Accuracy = 0.6413043737411499
Training iter #63400:   Batch Loss = 0.637616, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6332124471664429, Accuracy = 0.6847826242446899
Training iter #63500:   Batch Loss = 0.581631, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7032879590988159, Accuracy = 0.6195651888847351
Training iter #63600:   Batch Loss = 0.608904, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6977841854095459, Accuracy = 0.5760869383811951
Training iter #63700:   Batch Loss = 0.566863, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6718592047691345, Accuracy = 0.6304348111152649
Training iter #63800:   Batch Loss = 0.563296, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6659253239631653, Accuracy = 0.6304348111152649
Training iter #63900:   Batch Loss = 0.577789, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6611824631690979, Accuracy = 0.6739130616188049
Training iter #64000:   Batch Loss = 0.604264, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6510496735572815, Accuracy = 0.6847826242446899
Training iter #64100:   Batch Loss = 0.626109, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.666100263595581, Accuracy = 0.6304348111152649
Training iter #64200:   Batch Loss = 0.634945, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6693547368049622, Accuracy = 0.6521739363670349
Training iter #64300:   Batch Loss = 0.570473, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7276298403739929, Accuracy = 0.5652173757553101
Training iter #64400:   Batch Loss = 0.623792, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6787781715393066, Accuracy = 0.6304348111152649
Training iter #64500:   Batch Loss = 0.552552, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6374942064285278, Accuracy = 0.695652186870575
Training iter #64600:   Batch Loss = 0.553351, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6543708443641663, Accuracy = 0.6630434989929199
Training iter #64700:   Batch Loss = 0.570249, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6654729247093201, Accuracy = 0.6630434989929199
Training iter #64800:   Batch Loss = 0.569388, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6525576114654541, Accuracy = 0.6630434989929199
Training iter #64900:   Batch Loss = 0.654054, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7191628813743591, Accuracy = 0.6304348111152649
Training iter #65000:   Batch Loss = 0.619240, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6787499785423279, Accuracy = 0.6304348111152649
Training iter #65100:   Batch Loss = 0.514176, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.7367162704467773, Accuracy = 0.5652173757553101
Training iter #65200:   Batch Loss = 0.598721, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6666344404220581, Accuracy = 0.6521739363670349
Training iter #65300:   Batch Loss = 0.522736, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6430753469467163, Accuracy = 0.695652186870575
Training iter #65400:   Batch Loss = 0.555568, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6598139405250549, Accuracy = 0.6847826242446899
Training iter #65500:   Batch Loss = 0.566283, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6982871890068054, Accuracy = 0.6086956262588501
Training iter #65600:   Batch Loss = 0.571457, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6593859195709229, Accuracy = 0.6521739363670349
Training iter #65700:   Batch Loss = 0.644298, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6884362101554871, Accuracy = 0.6304348111152649
Training iter #65800:   Batch Loss = 0.602789, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6677569150924683, Accuracy = 0.6413043737411499
Training iter #65900:   Batch Loss = 0.524039, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7031002044677734, Accuracy = 0.6413043737411499
Training iter #66000:   Batch Loss = 0.641153, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7221671938896179, Accuracy = 0.6086956262588501
Training iter #66100:   Batch Loss = 0.492132, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.6972225308418274, Accuracy = 0.6195651888847351
Training iter #66200:   Batch Loss = 0.575558, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6966052651405334, Accuracy = 0.6086956262588501
Training iter #66300:   Batch Loss = 0.573989, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6929541230201721, Accuracy = 0.6195651888847351
Training iter #66400:   Batch Loss = 0.605806, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6263695955276489, Accuracy = 0.6847826242446899
Training iter #66500:   Batch Loss = 0.620562, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6447315216064453, Accuracy = 0.6521739363670349
Training iter #66600:   Batch Loss = 0.635168, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6558203101158142, Accuracy = 0.6304348111152649
Training iter #66700:   Batch Loss = 0.532469, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7573936581611633, Accuracy = 0.554347813129425
Training iter #66800:   Batch Loss = 0.631886, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7084661722183228, Accuracy = 0.6304348111152649
Training iter #66900:   Batch Loss = 0.471680, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 0.6834317445755005, Accuracy = 0.6413043737411499
Training iter #67000:   Batch Loss = 0.556124, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6567009687423706, Accuracy = 0.6847826242446899
Training iter #67100:   Batch Loss = 0.553133, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6770787239074707, Accuracy = 0.6304348111152649
Training iter #67200:   Batch Loss = 0.565690, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6449931859970093, Accuracy = 0.6630434989929199
Training iter #67300:   Batch Loss = 0.630309, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6598608493804932, Accuracy = 0.6413043737411499
Training iter #67400:   Batch Loss = 0.665090, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6749997735023499, Accuracy = 0.6413043737411499
Training iter #67500:   Batch Loss = 0.587356, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7003957629203796, Accuracy = 0.6086956262588501
Training iter #67600:   Batch Loss = 0.594443, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6801701784133911, Accuracy = 0.6413043737411499
Training iter #67700:   Batch Loss = 0.546045, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6248979568481445, Accuracy = 0.6739130616188049
Training iter #67800:   Batch Loss = 0.535983, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6380813121795654, Accuracy = 0.6739130616188049
Training iter #67900:   Batch Loss = 0.595929, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.629666268825531, Accuracy = 0.6630434989929199
Training iter #68000:   Batch Loss = 0.528641, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6830790042877197, Accuracy = 0.6304348111152649
Training iter #68100:   Batch Loss = 0.657499, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7201961278915405, Accuracy = 0.5760869383811951
Training iter #68200:   Batch Loss = 0.629375, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.6369039416313171, Accuracy = 0.6739130616188049
Training iter #68300:   Batch Loss = 0.515284, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6455293297767639, Accuracy = 0.6630434989929199
Training iter #68400:   Batch Loss = 0.604447, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7089407444000244, Accuracy = 0.6304348111152649
Training iter #68500:   Batch Loss = 0.526529, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7162858843803406, Accuracy = 0.6195651888847351
Training iter #68600:   Batch Loss = 0.556991, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7373446226119995, Accuracy = 0.6086956262588501
Training iter #68700:   Batch Loss = 0.593418, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.669330358505249, Accuracy = 0.6086956262588501
Training iter #68800:   Batch Loss = 0.592543, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.631181538105011, Accuracy = 0.6739130616188049
Training iter #68900:   Batch Loss = 0.666702, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6570845246315002, Accuracy = 0.6521739363670349
Training iter #69000:   Batch Loss = 0.652374, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.665272057056427, Accuracy = 0.6630434989929199
Training iter #69100:   Batch Loss = 0.545344, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7183922529220581, Accuracy = 0.554347813129425
Training iter #69200:   Batch Loss = 0.635960, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.707717776298523, Accuracy = 0.5652173757553101
Training iter #69300:   Batch Loss = 0.533774, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6528485417366028, Accuracy = 0.6413043737411499
Training iter #69400:   Batch Loss = 0.588904, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6446094512939453, Accuracy = 0.6413043737411499
Training iter #69500:   Batch Loss = 0.557858, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.6314201951026917, Accuracy = 0.695652186870575
Training iter #69600:   Batch Loss = 0.538176, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.6632581949234009, Accuracy = 0.6521739363670349
Training iter #69700:   Batch Loss = 0.652924, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7108152508735657, Accuracy = 0.6086956262588501
Training iter #69800:   Batch Loss = 0.657385, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.6459797620773315, Accuracy = 0.6847826242446899
Training iter #69900:   Batch Loss = 0.509415, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 0.6508092880249023, Accuracy = 0.6521739363670349
Training iter #70000:   Batch Loss = 0.601330, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.6368588209152222, Accuracy = 0.6847826242446899
Training iter #70100:   Batch Loss = 0.558833, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.6494870781898499, Accuracy = 0.6847826242446899
Training iter #70200:   Batch Loss = 0.642699, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7209132313728333, Accuracy = 0.6086956262588501
Training iter #70300:   Batch Loss = 0.699149, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7590851187705994, Accuracy = 0.6195651888847351
Training iter #70400:   Batch Loss = 0.734593, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 1.608700156211853, Accuracy = 0.44565218687057495
Training iter #70500:   Batch Loss = 1.427179, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 1.8725910186767578, Accuracy = 0.3804347813129425
Training iter #70600:   Batch Loss = 1.664958, Accuracy = 0.41999998688697815
PERFORMANCE ON TEST SET: Batch Loss = 1.6048623323440552, Accuracy = 0.3804347813129425
Training iter #70700:   Batch Loss = 1.253692, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 1.3387027978897095, Accuracy = 0.3804347813129425
Training iter #70800:   Batch Loss = 0.863013, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 1.129581093788147, Accuracy = 0.3804347813129425
Training iter #70900:   Batch Loss = 0.864859, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.9627131819725037, Accuracy = 0.3913043439388275
Training iter #71000:   Batch Loss = 0.865157, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.8449416160583496, Accuracy = 0.4021739065647125
Training iter #71100:   Batch Loss = 0.879146, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7675058841705322, Accuracy = 0.43478259444236755
Training iter #71200:   Batch Loss = 0.631703, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7371622920036316, Accuracy = 0.510869562625885
Training iter #71300:   Batch Loss = 0.953954, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.9087060689926147, Accuracy = 0.6195651888847351
Training iter #71400:   Batch Loss = 1.015701, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.9598446488380432, Accuracy = 0.6413043737411499
Training iter #71500:   Batch Loss = 0.967778, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.862212598323822, Accuracy = 0.6413043737411499
Training iter #71600:   Batch Loss = 0.948583, Accuracy = 0.4399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7775422930717468, Accuracy = 0.6521739363670349
Training iter #71700:   Batch Loss = 0.706748, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7210785150527954, Accuracy = 0.6086956262588501
Training iter #71800:   Batch Loss = 0.695264, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7009958028793335, Accuracy = 0.6413043737411499
Training iter #71900:   Batch Loss = 0.784862, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7108994126319885, Accuracy = 0.52173912525177
Training iter #72000:   Batch Loss = 0.666232, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7265730500221252, Accuracy = 0.5
Training iter #72100:   Batch Loss = 0.722501, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7331753969192505, Accuracy = 0.489130437374115
Training iter #72200:   Batch Loss = 0.738499, Accuracy = 0.5
PERFORMANCE ON TEST SET: Batch Loss = 0.7355347871780396, Accuracy = 0.46739131212234497
Training iter #72300:   Batch Loss = 0.716795, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7362897396087646, Accuracy = 0.47826087474823
Training iter #72400:   Batch Loss = 0.720587, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7349796295166016, Accuracy = 0.46739131212234497
Training iter #72500:   Batch Loss = 0.675669, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7323741912841797, Accuracy = 0.45652174949645996
Training iter #72600:   Batch Loss = 0.698983, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7316509485244751, Accuracy = 0.47826087474823
Training iter #72700:   Batch Loss = 0.736593, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7303929328918457, Accuracy = 0.489130437374115
Training iter #72800:   Batch Loss = 0.663339, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.730491042137146, Accuracy = 0.47826087474823
Training iter #72900:   Batch Loss = 0.718146, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7312050461769104, Accuracy = 0.510869562625885
Training iter #73000:   Batch Loss = 0.697009, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7315438389778137, Accuracy = 0.510869562625885
Training iter #73100:   Batch Loss = 0.678872, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7333163022994995, Accuracy = 0.5
Training iter #73200:   Batch Loss = 0.731277, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7338883876800537, Accuracy = 0.47826087474823
Training iter #73300:   Batch Loss = 0.643673, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7347138524055481, Accuracy = 0.5
Training iter #73400:   Batch Loss = 0.693264, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7369701266288757, Accuracy = 0.510869562625885
Training iter #73500:   Batch Loss = 0.745360, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.737621009349823, Accuracy = 0.5
Training iter #73600:   Batch Loss = 0.658351, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7402121424674988, Accuracy = 0.5
Training iter #73700:   Batch Loss = 0.732858, Accuracy = 0.46000000834465027
PERFORMANCE ON TEST SET: Batch Loss = 0.7414031624794006, Accuracy = 0.5
Training iter #73800:   Batch Loss = 0.679895, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.740855872631073, Accuracy = 0.489130437374115
Training iter #73900:   Batch Loss = 0.674666, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7424923777580261, Accuracy = 0.489130437374115
Training iter #74000:   Batch Loss = 0.714955, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 0.7414035797119141, Accuracy = 0.47826087474823
Training iter #74100:   Batch Loss = 0.627334, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7400264143943787, Accuracy = 0.46739131212234497
Training iter #74200:   Batch Loss = 0.695537, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.739432156085968, Accuracy = 0.47826087474823
Training iter #74300:   Batch Loss = 0.731895, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7382844090461731, Accuracy = 0.5
Training iter #74400:   Batch Loss = 0.664981, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7390502095222473, Accuracy = 0.510869562625885
Training iter #74500:   Batch Loss = 0.729803, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7383848428726196, Accuracy = 0.510869562625885
Training iter #74600:   Batch Loss = 0.669720, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7372100949287415, Accuracy = 0.510869562625885
Training iter #74700:   Batch Loss = 0.668492, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7376577258110046, Accuracy = 0.510869562625885
Training iter #74800:   Batch Loss = 0.716808, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7364947199821472, Accuracy = 0.510869562625885
Training iter #74900:   Batch Loss = 0.631140, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7350183725357056, Accuracy = 0.510869562625885
Training iter #75000:   Batch Loss = 0.705436, Accuracy = 0.5199999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7346823215484619, Accuracy = 0.5
Training iter #75100:   Batch Loss = 0.710895, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7336757183074951, Accuracy = 0.5
Training iter #75200:   Batch Loss = 0.642323, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7350842356681824, Accuracy = 0.510869562625885
Training iter #75300:   Batch Loss = 0.730082, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7356392741203308, Accuracy = 0.5
Training iter #75400:   Batch Loss = 0.657978, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.735808253288269, Accuracy = 0.510869562625885
Training iter #75500:   Batch Loss = 0.655838, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7378674149513245, Accuracy = 0.5
Training iter #75600:   Batch Loss = 0.704233, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7376852631568909, Accuracy = 0.510869562625885
Training iter #75700:   Batch Loss = 0.617775, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7367750406265259, Accuracy = 0.510869562625885
Training iter #75800:   Batch Loss = 0.699488, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7382031083106995, Accuracy = 0.510869562625885
Training iter #75900:   Batch Loss = 0.700697, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7366684079170227, Accuracy = 0.510869562625885
Training iter #76000:   Batch Loss = 0.627949, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7385483384132385, Accuracy = 0.510869562625885
Training iter #76100:   Batch Loss = 0.727017, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7363137602806091, Accuracy = 0.510869562625885
Training iter #76200:   Batch Loss = 0.641172, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7326465249061584, Accuracy = 0.510869562625885
Training iter #76300:   Batch Loss = 0.646617, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7290309071540833, Accuracy = 0.532608687877655
Training iter #76400:   Batch Loss = 0.690220, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7267939448356628, Accuracy = 0.54347825050354
Training iter #76500:   Batch Loss = 0.644211, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7239813804626465, Accuracy = 0.5652173757553101
Training iter #76600:   Batch Loss = 0.703745, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7244145274162292, Accuracy = 0.54347825050354
Training iter #76700:   Batch Loss = 0.683860, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7293897271156311, Accuracy = 0.510869562625885
Training iter #76800:   Batch Loss = 0.641397, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7352432012557983, Accuracy = 0.489130437374115
Training iter #76900:   Batch Loss = 0.683216, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.733930766582489, Accuracy = 0.510869562625885
Training iter #77000:   Batch Loss = 0.624631, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7353508472442627, Accuracy = 0.532608687877655
Training iter #77100:   Batch Loss = 0.656378, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7399973273277283, Accuracy = 0.5
Training iter #77200:   Batch Loss = 0.668995, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7342527508735657, Accuracy = 0.532608687877655
Training iter #77300:   Batch Loss = 0.636708, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.7290375232696533, Accuracy = 0.554347813129425
Training iter #77400:   Batch Loss = 0.699324, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7341956496238708, Accuracy = 0.532608687877655
Training iter #77500:   Batch Loss = 0.686377, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7310717105865479, Accuracy = 0.532608687877655
Training iter #77600:   Batch Loss = 0.654413, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7400742769241333, Accuracy = 0.532608687877655
Training iter #77700:   Batch Loss = 0.691668, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7407059669494629, Accuracy = 0.489130437374115
Training iter #77800:   Batch Loss = 0.596601, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.7389214634895325, Accuracy = 0.510869562625885
Training iter #77900:   Batch Loss = 0.665049, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7403028011322021, Accuracy = 0.52173912525177
Training iter #78000:   Batch Loss = 0.686101, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.7354134917259216, Accuracy = 0.52173912525177
Training iter #78100:   Batch Loss = 0.616835, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.7288442850112915, Accuracy = 0.54347825050354
Training iter #78200:   Batch Loss = 0.681574, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7275559902191162, Accuracy = 0.5760869383811951
Training iter #78300:   Batch Loss = 0.665700, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.728409469127655, Accuracy = 0.5652173757553101
Training iter #78400:   Batch Loss = 0.649873, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7265820503234863, Accuracy = 0.5760869383811951
Training iter #78500:   Batch Loss = 0.695935, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.740777313709259, Accuracy = 0.510869562625885
Training iter #78600:   Batch Loss = 0.585011, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7411662936210632, Accuracy = 0.54347825050354
Training iter #78700:   Batch Loss = 0.663757, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7419406175613403, Accuracy = 0.54347825050354
Training iter #78800:   Batch Loss = 0.662593, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.7151605486869812, Accuracy = 0.5978260636329651
Training iter #78900:   Batch Loss = 0.669576, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7144907712936401, Accuracy = 0.5869565010070801
Training iter #79000:   Batch Loss = 0.693977, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7174710035324097, Accuracy = 0.5869565010070801
Training iter #79100:   Batch Loss = 0.704993, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7166286706924438, Accuracy = 0.5869565010070801
Training iter #79200:   Batch Loss = 0.697007, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.738350510597229, Accuracy = 0.54347825050354
Training iter #79300:   Batch Loss = 0.720706, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7568981647491455, Accuracy = 0.54347825050354
Training iter #79400:   Batch Loss = 0.564581, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 0.7671931982040405, Accuracy = 0.54347825050354
Training iter #79500:   Batch Loss = 0.664981, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.7742130756378174, Accuracy = 0.532608687877655
Training iter #79600:   Batch Loss = 0.682981, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7901187539100647, Accuracy = 0.52173912525177
Training iter #79700:   Batch Loss = 0.622028, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.762787938117981, Accuracy = 0.54347825050354
Training iter #79800:   Batch Loss = 0.713851, Accuracy = 0.5600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 0.7394272685050964, Accuracy = 0.554347813129425
Training iter #79900:   Batch Loss = 0.693482, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.7307037711143494, Accuracy = 0.5652173757553101
Training iter #80000:   Batch Loss = 0.657504, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.7168750762939453, Accuracy = 0.5652173757553101
Training iter #80100:   Batch Loss = 0.687609, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.7118810415267944, Accuracy = 0.5978260636329651
Training iter #80200:   Batch Loss = 0.573650, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 0.719417154788971, Accuracy = 0.5869565010070801
Training iter #80300:   Batch Loss = 0.672182, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.7403464317321777, Accuracy = 0.54347825050354
Training iter #80400:   Batch Loss = 0.655772, Accuracy = 0.6399999856948853
PERFORMANCE ON TEST SET: Batch Loss = 0.734706699848175, Accuracy = 0.5652173757553101
Training iter #80500:   Batch Loss = 0.613720, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 0.735713541507721, Accuracy = 0.54347825050354
Training iter #80600:   Batch Loss = 0.684193, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6906483769416809, Accuracy = 0.6086956262588501
Training iter #80700:   Batch Loss = 0.700579, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 0.6875400543212891, Accuracy = 0.6304348111152649
Training iter #80800:   Batch Loss = 0.728782, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 0.6901121139526367, Accuracy = 0.6086956262588501
Training iter #80900:   Batch Loss = 0.710093, Accuracy = 0.6000000238418579
PERFORMANCE ON TEST SET: Batch Loss = 0.6933217644691467, Accuracy = 0.6195651888847351
Training iter #81000:   Batch Loss = 0.640866, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.7016435861587524, Accuracy = 0.5978260636329651
Optimization Finished!
FINAL RESULT: Batch Loss = 0.7016435861587524, Accuracy = 0.5978260636329651
